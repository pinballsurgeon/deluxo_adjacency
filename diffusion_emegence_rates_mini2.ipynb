{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinballsurgeon/deluxo_adjacency/blob/main/diffusion_emegence_rates_mini2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_JWste6_0Spx"
      },
      "outputs": [],
      "source": [
        "# cell 0: global configuration, imports, and seeds\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import yaml\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from pathlib import Path\n",
        "import random\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import skimage.measure\n",
        "import skimage.metrics\n",
        "import time\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "MASTER_SEED = 42\n",
        "torch.manual_seed(MASTER_SEED)\n",
        "np.random.seed(MASTER_SEED)\n",
        "random.seed(MASTER_SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TEST_MODE = True\n",
        "IMAGE_SIZE = 16\n",
        "MARGIN = 2\n",
        "RENDERING_ANTI_ALIASED = False\n",
        "SHARPNESS_TARGET = \"aliased\"\n",
        "\n",
        "DIFFUSERS_IMPORT_SUCCESS = False\n",
        "try:\n",
        "    import diffusers\n",
        "    CURRENT_DIFFUSERS_VERSION_STR = diffusers.__version__.replace('.', '_')\n",
        "    DIFFUSERS_IMPORT_SUCCESS = True\n",
        "except ImportError:\n",
        "    CURRENT_DIFFUSERS_VERSION_STR = \"INSTALL_FAILED\"\n",
        "\n",
        "\n",
        "EXPERIMENT_SUBFOLDER_NAME = f'run_img{IMAGE_SIZE}_{SHARPNESS_TARGET}_diffusers_{CURRENT_DIFFUSERS_VERSION_STR}_{time.strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "\n",
        "PERSISTENT_SAVE_ROOT_ON_DRIVE = Path(\"./drive/MyDrive/diffusion_tests\")\n",
        "if 'PERSISTENT_SAVE_ROOT_ON_DRIVE' not in globals():\n",
        "    print(\"critical error: PERSISTENT_SAVE_ROOT_ON_DRIVE not defined by cell 0.0. defaulting to local path.\")\n",
        "    PERSISTENT_SAVE_ROOT_ON_DRIVE = Path(\"./drive/MyDrive/diffusion_tests\")\n",
        "\n",
        "BASE_SAVE_DIR = PERSISTENT_SAVE_ROOT_ON_DRIVE / EXPERIMENT_SUBFOLDER_NAME\n",
        "\n",
        "\n",
        "DATASET_DIR = BASE_SAVE_DIR / 'dataset/'\n",
        "MODEL_CHECKPOINT_DIR = BASE_SAVE_DIR / 'model_checkpoints/'\n",
        "RESULTS_LOG_DIR = BASE_SAVE_DIR / 'logs/'\n",
        "VISUALIZATION_DIR = BASE_SAVE_DIR / 'visualizations/'\n",
        "VIDEO_OUTPUT_DIR = BASE_SAVE_DIR / 'videos_ultimate/'\n",
        "\n",
        "for dir_path in [BASE_SAVE_DIR, DATASET_DIR, MODEL_CHECKPOINT_DIR, RESULTS_LOG_DIR, VISUALIZATION_DIR, VIDEO_OUTPUT_DIR]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if TEST_MODE:\n",
        "    DATASET_NUM_SAMPLES_PER_COMPLEXITY_CLASS = 10\n",
        "    DATASET_FORCE_REGENERATE = True\n",
        "else:\n",
        "    DATASET_NUM_SAMPLES_PER_COMPLEXITY_CLASS = 70\n",
        "    DATASET_FORCE_REGENERATE = False\n",
        "\n",
        "DATASET_FILE = DATASET_DIR / f'geometric_dataset_img{IMAGE_SIZE}_{SHARPNESS_TARGET}.pt'\n",
        "DATASET_CONFIG_FILE = DATASET_DIR / f'dataset_config_img{IMAGE_SIZE}_{SHARPNESS_TARGET}.yaml'\n",
        "\n",
        "SHAPE_CATALOG = {\n",
        "    'circle': {'parameters': ['radius_ratio'], 'num_vertices': 0, 'base_complexity_score': 0.1},\n",
        "    'square': {'parameters': ['size_ratio', 'rotation_angle'], 'num_vertices': 4, 'base_complexity_score': 0.2},\n",
        "    'equilateral_triangle': {'parameters': ['size_ratio', 'rotation_angle'], 'num_vertices': 3, 'base_complexity_score': 0.3},\n",
        "    'rectangle': {'parameters': ['width_ratio', 'height_ratio', 'rotation_angle'], 'num_vertices': 4, 'base_complexity_score': 0.4},\n",
        "    'pentagon': {'parameters': ['size_ratio', 'rotation_angle'], 'num_vertices': 5, 'base_complexity_score': 0.6},\n",
        "    'hexagon': {'parameters': ['size_ratio', 'rotation_angle'], 'num_vertices': 6, 'base_complexity_score': 0.7},\n",
        "    'star_5_pointed': {'parameters': ['outer_radius_ratio', 'inner_radius_ratio', 'rotation_angle'], 'num_vertices': 10, 'base_complexity_score': 0.9}\n",
        "}\n",
        "SORTED_SHAPE_NAMES = sorted(SHAPE_CATALOG.keys(), key=lambda s: SHAPE_CATALOG[s]['base_complexity_score'])\n",
        "CLASS_ID_TO_SHAPE_NAME = {i: name for i, name in enumerate(SORTED_SHAPE_NAMES)}\n",
        "SHAPE_NAME_TO_CLASS_ID = {name: i for i, name in CLASS_ID_TO_SHAPE_NAME.items()}\n",
        "for i, shape_name in CLASS_ID_TO_SHAPE_NAME.items(): SHAPE_CATALOG[shape_name]['class_id'] = i\n",
        "NUM_SHAPE_CLASSES = len(SHAPE_CATALOG)\n",
        "\n",
        "ENHANCED_METRIC_CONFIG = {\n",
        "    'local_entropy_patch_size': (4, 4),\n",
        "    'tracked_layers_for_weights': [\n",
        "        'conv_in.weight',                                                 # 1. input convolution\n",
        "        'down_blocks.0.resnets.0.conv1.weight',                           # 2. Early ResNet conv\n",
        "        'down_blocks.1.attentions.0.to_q.weight',                         # 3. Downsampling attention\n",
        "        'mid_block.resnets.0.conv1.weight',                               # 4. Mid-block ResNet conv\n",
        "        'mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight',  # 5. Mid-block self-attention\n",
        "        'up_blocks.1.attentions.0.to_k.weight',                           # 6. upsampling attention\n",
        "        'up_blocks.2.resnets.1.conv2.weight',                             # 7. late ResNet conv\n",
        "        'conv_out.weight'                                                 # 8. final output convolution\n",
        "    ],\n",
        "    'num_stability_samples_k': 3 if not TEST_MODE else 2,\n",
        "    'noise_pixels_binary_threshold_area': 2,\n",
        "}\n",
        "\n",
        "EMERGENT_METRIC_KEYS = [\n",
        "    'pixel_variance',\n",
        "    'shannon_entropy_global',\n",
        "    'local_entropy_mean',\n",
        "    'local_entropy_variance',\n",
        "    'num_clusters_binary',\n",
        "    'noise_pixels_binary',\n",
        "    'IoU_with_gt_mean',\n",
        "    'output_stability_mse',\n",
        "    'output_stability_ssim',\n",
        "    'edge_sharpness_sobel',\n",
        "    'circularity_metric',\n",
        "    'solidity_metric',\n",
        "]\n",
        "\n",
        "TRAIN_NUM_EPOCHS_ULOFI_ = 400 if not TEST_MODE else 10\n",
        "TRAIN_BATCH_SIZE_ULOFI_ = 32 if not TEST_MODE else 16\n",
        "TRAIN_LEARNING_RATE_ = 5e-4\n",
        "TRAIN_LR_WARMUP_STEPS_ULOFI_ = 50\n",
        "TRAIN_GRADIENT_CLIP_NORM_ = 1.0\n",
        "\n",
        "# early stoppign patience\n",
        "early_stopping_patience = 50\n",
        "\n",
        "\n",
        "# unet architecture params for cell 2\n",
        "layers_per_block_unet_ = 2\n",
        "block_out_channels_unet_ = (64, 128, 128)\n",
        "cross_attention_dim_unet_ = 128\n",
        "attention_head_dim_unet_ = 8\n",
        "\n",
        "DATASET_BATCH_SIZE_ = TRAIN_BATCH_SIZE_ULOFI_\n",
        "train_split_ratio_ = 0.70\n",
        "\n",
        "\n",
        "# image rate\n",
        "metrics_sample_freq = 1\n",
        "\n",
        "\n",
        "MODEL_CHECKPOINT_FILE = MODEL_CHECKPOINT_DIR / f'unet_checkpoint_img{IMAGE_SIZE}_{SHARPNESS_TARGET}.pth'\n",
        "TRAINING_HISTORY_FILE = RESULTS_LOG_DIR / f'training_history_img{IMAGE_SIZE}_{SHARPNESS_TARGET}.json'\n",
        "_FIGURE_SAVE_NAME_MAIN_PLOT = f\"learning_curves{'_testmode' if TEST_MODE else ''}.png\"\n",
        "_FIGURE_SAVE_NAME_METRICS_PLOT = f\"emergent_metrics{'_testmode' if TEST_MODE else ''}.png\"\n",
        "_EXPERIMENT_TAG_NAME = f\"ultralofi_img{IMAGE_SIZE}_{SHARPNESS_TARGET}_diffusers_{CURRENT_DIFFUSERS_VERSION_STR}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sYSOcN4GIbOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211,
          "referenced_widgets": [
            "084fb525a670469a8e649ee1eafb9fd8",
            "2d3e9d316e214ca395014d5c19e1896c",
            "3f8356e494ee49869906de79f39e2c8e",
            "4bfb09441718433e8c0e0bf5f5ab975a",
            "5c3cd339e0ed48b782aa8229612a07ee",
            "ed19682e67d047878ee60c68e686c3b0",
            "5b02532e3f8940adb3f1c9e77f693a99",
            "85d2f944560240fcac134cb700a9a78c",
            "6c308a1fae554286a35f3bafa36bd879",
            "bf88c0e6db2f45d7bb447326e4af3d50",
            "86587f8f30aa405aa2073d366eba017e"
          ]
        },
        "id": "8BbOaV0D6cPe",
        "outputId": "73b9064a-b6b0-4874-8d73-a0b055366fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- dataset engine processing (test_mode: True) ---\n",
            "generating new dataset (force_regenerate=True or file not found)...\n",
            "generating 10 samples/class for 7 classes...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generating classes:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "084fb525a670469a8e649ee1eafb9fd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving dataset to drive/MyDrive/diffusion_tests/run_img16_aliased_diffusers_0_33_1_20250606_123328/dataset/geometric_dataset_img16_aliased.pt...\n",
            "dataset config saved: /content/drive/MyDrive/diffusion_tests/run_img16_aliased_diffusers_0_33_1_20250606_123328/dataset/dataset_config_img16_aliased.yaml\n",
            "train_size: 49, val_size: 21. batch_size: 16\n",
            "\n",
            "--- visualizing a few dataset samples ---\n"
          ]
        }
      ],
      "source": [
        "# cell 1: dataset engine - shape renderer, synthetic dataset, dataloaders\n",
        "\n",
        "class ShapeRenderer:\n",
        "    def __init__(self, image_size, margin, anti_aliased_default=False):\n",
        "        self.image_size = image_size\n",
        "        self.margin = margin\n",
        "        self.drawable_size = image_size - 2 * margin\n",
        "        self.anti_aliased_default = anti_aliased_default\n",
        "        if self.drawable_size <= 0:\n",
        "            raise ValueError(f\"image_size ({image_size}) too small for margin ({2*margin})\")\n",
        "\n",
        "    def _finalize_image_and_mask(self, rendered_on_drawable_area_np):\n",
        "        final_image_arr = np.zeros((self.image_size, self.image_size), dtype=np.uint8)\n",
        "        paste_x_start, paste_y_start = self.margin, self.margin\n",
        "        paste_x_end = paste_x_start + self.drawable_size\n",
        "        paste_y_end = paste_y_start + self.drawable_size\n",
        "        h, w = rendered_on_drawable_area_np.shape\n",
        "        final_image_arr[paste_y_start:paste_y_end, paste_x_start:paste_x_end] = rendered_on_drawable_area_np[:self.drawable_size, :self.drawable_size]\n",
        "        mask_arr = (final_image_arr > 128).astype(np.uint8)\n",
        "        return final_image_arr, mask_arr\n",
        "\n",
        "    def _calculate_polygon_vertices(self, center_x, center_y, num_vertices, size_pixels, rotation_angle_deg):\n",
        "        vertices = []\n",
        "        angle_step = 2 * np.pi / num_vertices\n",
        "        rotation_rad = np.deg2rad(rotation_angle_deg)\n",
        "        for i in range(num_vertices):\n",
        "            angle = i * angle_step + rotation_rad\n",
        "            x = int(round(center_x + size_pixels * np.cos(angle)))\n",
        "            y = int(round(center_y + size_pixels * np.sin(angle)))\n",
        "            x = max(0, min(x, self.drawable_size - 1))\n",
        "            y = max(0, min(y, self.drawable_size - 1))\n",
        "            vertices.append((x, y))\n",
        "        return vertices\n",
        "\n",
        "    def _calculate_star_vertices(self, center_x, center_y, outer_r_pixels, inner_r_pixels, num_points, rotation_angle_deg):\n",
        "        vertices = []\n",
        "        total_vertices = num_points * 2\n",
        "        angle_step = 2 * np.pi / total_vertices\n",
        "        rotation_rad = np.deg2rad(rotation_angle_deg)\n",
        "        for i in range(total_vertices):\n",
        "            radius = outer_r_pixels if i % 2 == 0 else inner_r_pixels\n",
        "            angle = i * angle_step + rotation_rad\n",
        "            x = int(round(center_x + radius * np.cos(angle)))\n",
        "            y = int(round(center_y + radius * np.sin(angle)))\n",
        "            x = max(0, min(x, self.drawable_size - 1)) # clip\n",
        "            y = max(0, min(y, self.drawable_size - 1))\n",
        "            vertices.append((x,y))\n",
        "        return vertices\n",
        "\n",
        "    def render_shape(self, shape_type, parameters, anti_aliased=None):\n",
        "        if anti_aliased is None: anti_aliased = self.anti_aliased_default\n",
        "        if anti_aliased: print(f\"warning: anti-aliased rendering requested but cv2 path is aliased.\")\n",
        "\n",
        "        cx_drawable, cy_drawable = int(round(parameters['center_x'])), int(round(parameters['center_y']))\n",
        "        cv2_canvas = np.zeros((self.drawable_size, self.drawable_size), dtype=np.uint8)\n",
        "\n",
        "        if shape_type == 'circle':\n",
        "            r_pixels = max(1, int(round(parameters['radius_ratio'] * (self.drawable_size / 2.0))))\n",
        "            cv2.circle(cv2_canvas, (cx_drawable, cy_drawable), r_pixels, 255, -1)\n",
        "        elif shape_type == 'square':\n",
        "            side_len = max(2, parameters['size_ratio'] * self.drawable_size)\n",
        "            angle_deg = parameters['rotation_angle']\n",
        "            s_half = side_len / 2.0; angle_rad = np.deg2rad(angle_deg); cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)\n",
        "            v_rel = np.array([[-s_half,-s_half],[s_half,-s_half],[s_half,s_half],[-s_half,s_half]])\n",
        "            rot_m = np.array([[cos_a,-sin_a],[sin_a,cos_a]])\n",
        "            final_v = np.clip((v_rel @ rot_m.T + np.array([cx_drawable,cy_drawable])),0,self.drawable_size-1).astype(np.int32)\n",
        "            if len(final_v)==4: cv2.fillPoly(cv2_canvas, [final_v], 255)\n",
        "        elif shape_type == 'equilateral_triangle':\n",
        "            circum_r = max(2, parameters['size_ratio'] * (self.drawable_size / 2.0))\n",
        "            verts = self._calculate_polygon_vertices(cx_drawable,cy_drawable,3,circum_r,parameters['rotation_angle'])\n",
        "            cv2.fillPoly(cv2_canvas, [np.array(verts, dtype=np.int32)], 255)\n",
        "        elif shape_type == 'rectangle':\n",
        "            w_px = max(2, parameters['width_ratio'] * self.drawable_size); h_px = max(2, parameters['height_ratio'] * self.drawable_size)\n",
        "            angle_deg = parameters['rotation_angle']\n",
        "            w_h,h_h = w_px/2.0,h_px/2.0; angle_rad=np.deg2rad(angle_deg); cos_a,sin_a=np.cos(angle_rad),np.sin(angle_rad)\n",
        "            v_rel=np.array([[-w_h,-h_h],[w_h,-h_h],[w_h,h_h],[-w_h,h_h]])\n",
        "            rot_m=np.array([[cos_a,-sin_a],[sin_a,cos_a]])\n",
        "            final_v=np.clip((v_rel @ rot_m.T + np.array([cx_drawable,cy_drawable])),0,self.drawable_size-1).astype(np.int32)\n",
        "            if len(final_v)==4: cv2.fillPoly(cv2_canvas, [final_v], 255)\n",
        "        elif shape_type in ['pentagon', 'hexagon']:\n",
        "            num_v = SHAPE_CATALOG[shape_type]['num_vertices']\n",
        "            circum_r = max(2, parameters['size_ratio'] * (self.drawable_size / 2.0))\n",
        "            verts = self._calculate_polygon_vertices(cx_drawable,cy_drawable,num_v,circum_r,parameters['rotation_angle'])\n",
        "            cv2.fillPoly(cv2_canvas, [np.array(verts, dtype=np.int32)], 255)\n",
        "        elif shape_type == 'star_5_pointed':\n",
        "            out_r = max(3, parameters['outer_radius_ratio'] * (self.drawable_size/2.0))\n",
        "            in_r = max(1, parameters['inner_radius_ratio'] * (self.drawable_size/2.0))\n",
        "            if in_r >= out_r: in_r = out_r * 0.4\n",
        "            verts = self._calculate_star_vertices(cx_drawable,cy_drawable,out_r,in_r,5,parameters['rotation_angle'])\n",
        "            cv2.fillPoly(cv2_canvas, [np.array(verts, dtype=np.int32)], 255)\n",
        "        else:\n",
        "            raise ValueError(f\"unsupported shape_type for cv2 rendering: {shape_type}\")\n",
        "        return self._finalize_image_and_mask(cv2_canvas)\n",
        "\n",
        "class SyntheticGeometricDataset(Dataset):\n",
        "    def __init__(self, num_samples_per_class, image_size, margin, shape_catalog_dict, renderer_instance,\n",
        "                 anti_aliased_flag, dataset_file_path_obj, force_regenerate_flag):\n",
        "        self.num_samples_per_class = num_samples_per_class\n",
        "        self.image_size = image_size\n",
        "        self.margin = margin\n",
        "        self.drawable_size = image_size - 2 * margin\n",
        "        self.shape_catalog = shape_catalog_dict\n",
        "        self.renderer = renderer_instance\n",
        "        self.anti_aliased = anti_aliased_flag\n",
        "        self.dataset_file_path = dataset_file_path_obj\n",
        "        self.samples = []\n",
        "\n",
        "        if self.dataset_file_path and self.dataset_file_path.exists() and not force_regenerate_flag:\n",
        "            print(f\"loading dataset from {self.dataset_file_path}...\")\n",
        "            self._load_dataset()\n",
        "        else:\n",
        "            print(f\"generating new dataset (force_regenerate={force_regenerate_flag} or file not found)...\")\n",
        "            self._generate_samples()\n",
        "            if self.dataset_file_path: self._save_dataset()\n",
        "\n",
        "    def _generate_random_params_for_shape(self, shape_name_str, shape_type_info_dict):\n",
        "        params = {}\n",
        "        center_coord = self.drawable_size / 2.0\n",
        "        max_abs_jitter = 0.5 if self.drawable_size <= 4 else 1.0 if self.drawable_size < 10 else 1.5\n",
        "        params['center_x'] = center_coord + random.uniform(-max_abs_jitter, max_abs_jitter)\n",
        "        params['center_y'] = center_coord + random.uniform(-max_abs_jitter, max_abs_jitter)\n",
        "\n",
        "        large_scale_min, large_scale_max = (0.80, 0.98) if self.drawable_size >= 8 else (0.85, 1.0)\n",
        "\n",
        "        if 'radius_ratio' in shape_type_info_dict['parameters']:\n",
        "            params['radius_ratio'] = random.uniform(large_scale_min, large_scale_max)\n",
        "        if 'size_ratio' in shape_type_info_dict['parameters']:\n",
        "            params['size_ratio'] = random.uniform(large_scale_min * (0.8 if shape_name_str=='square' else 1.0) , large_scale_max)\n",
        "        if 'rotation_angle' in shape_type_info_dict['parameters']:\n",
        "            params['rotation_angle'] = random.uniform(0, 359.9)\n",
        "        if shape_name_str == 'rectangle':\n",
        "            is_w_dom = random.choice([True, False])\n",
        "            min_dim_ratio = 2.0 / self.drawable_size\n",
        "            params['width_ratio'] = random.uniform(0.6,0.9) if is_w_dom else random.uniform(max(min_dim_ratio,0.3*random.uniform(0.6,0.9)),0.5*random.uniform(0.6,0.9))\n",
        "            params['height_ratio'] = random.uniform(max(min_dim_ratio,0.3*params['width_ratio']),0.5*params['width_ratio']) if is_w_dom else random.uniform(0.6,0.9)\n",
        "            if not is_w_dom: params['width_ratio'], params['height_ratio'] = params['height_ratio'], params['width_ratio']\n",
        "        if shape_name_str == 'star_5_pointed':\n",
        "            params['outer_radius_ratio'] = random.uniform(large_scale_min, large_scale_max)\n",
        "            params['inner_radius_ratio'] = random.uniform(params['outer_radius_ratio']*0.35, params['outer_radius_ratio']*0.60)\n",
        "        return params\n",
        "\n",
        "    def _generate_samples(self):\n",
        "        self.samples = []\n",
        "        print(f\"generating {self.num_samples_per_class} samples/class for {len(self.shape_catalog)} classes...\")\n",
        "        for shape_name, shape_info in tqdm(self.shape_catalog.items(), desc=\"generating classes\"):\n",
        "            for _ in range(self.num_samples_per_class):\n",
        "                params = self._generate_random_params_for_shape(shape_name, shape_info)\n",
        "                try:\n",
        "                    img_np, mask_np = self.renderer.render_shape(shape_name, params, self.anti_aliased)\n",
        "                    self.samples.append({\n",
        "                        'image': torch.from_numpy(img_np.astype(np.float32)).unsqueeze(0),\n",
        "                        'mask': torch.from_numpy(mask_np.astype(np.float32)).unsqueeze(0),\n",
        "                        'shape_type': shape_name, 'class_id': shape_info['class_id'],\n",
        "                        'parameters': params, 'base_complexity_score': shape_info['base_complexity_score']\n",
        "                    })\n",
        "                except Exception as e: print(f\"error rendering {shape_name} with {params}: {e}\")\n",
        "        random.shuffle(self.samples)\n",
        "\n",
        "    def _save_dataset(self):\n",
        "        print(f\"saving dataset to {self.dataset_file_path}...\")\n",
        "        self.dataset_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        torch.save(self.samples, self.dataset_file_path)\n",
        "\n",
        "    def _load_dataset(self):\n",
        "        self.samples = torch.load(self.dataset_file_path)\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        return {\n",
        "            'image': sample['image'] / 255.0, 'mask': sample['mask'],\n",
        "            'class_id': torch.tensor(sample['class_id'], dtype=torch.long),\n",
        "            'shape_type': sample['shape_type'],\n",
        "            'base_complexity_score': sample['base_complexity_score']\n",
        "        }\n",
        "\n",
        "try: from tqdm.notebook import tqdm\n",
        "except ImportError: tqdm = lambda x, desc: x\n",
        "\n",
        "print(f\"\\n--- dataset engine processing (test_mode: {TEST_MODE}) ---\")\n",
        "shape_renderer_instance = ShapeRenderer(\n",
        "    image_size=IMAGE_SIZE, margin=MARGIN, anti_aliased_default=RENDERING_ANTI_ALIASED\n",
        ")\n",
        "geometric_dataset = SyntheticGeometricDataset(\n",
        "    num_samples_per_class=DATASET_NUM_SAMPLES_PER_COMPLEXITY_CLASS,\n",
        "    image_size=IMAGE_SIZE, margin=MARGIN, shape_catalog_dict=SHAPE_CATALOG,\n",
        "    renderer_instance=shape_renderer_instance, anti_aliased_flag=RENDERING_ANTI_ALIASED,\n",
        "    dataset_file_path_obj=DATASET_FILE, force_regenerate_flag=DATASET_FORCE_REGENERATE\n",
        ")\n",
        "\n",
        "dataset_config_output = {\n",
        "    'master_seed': MASTER_SEED, 'image_size': IMAGE_SIZE, 'margin': MARGIN,\n",
        "    'rendering_anti_aliased': RENDERING_ANTI_ALIASED,\n",
        "    'num_requested_samples_per_class': DATASET_NUM_SAMPLES_PER_COMPLEXITY_CLASS,\n",
        "    'actual_num_samples_generated': len(geometric_dataset),\n",
        "    'num_shape_classes': NUM_SHAPE_CLASSES,\n",
        "    'dataset_file_used': str(DATASET_FILE.resolve() if DATASET_FILE.exists() else DATASET_FILE),\n",
        "    'dataset_strategy': \"ultra-lofi blocky & bound\",\n",
        "    'generation_timestamp_utc': time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "    'shape_catalog_snapshot': {k: {**v, 'class_id': int(v['class_id'])} for k, v in SHAPE_CATALOG.items()}\n",
        "}\n",
        "try:\n",
        "    with open(DATASET_CONFIG_FILE, 'w') as f: yaml.dump(dataset_config_output, f, indent=2, sort_keys=False)\n",
        "    print(f\"dataset config saved: {DATASET_CONFIG_FILE.resolve()}\")\n",
        "except Exception as e: print(f\"error saving dataset config: {e}\")\n",
        "\n",
        "\n",
        "train_dataloader, val_dataloader = None, None\n",
        "if len(geometric_dataset) > 0:\n",
        "    val_size = max(1, int(len(geometric_dataset) * (1.0 - train_split_ratio_))) if len(geometric_dataset) > 1 else 0\n",
        "    train_size = len(geometric_dataset) - val_size\n",
        "\n",
        "    if train_size > 0 and val_size > 0 :\n",
        "        generator = torch.Generator().manual_seed(MASTER_SEED)\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(geometric_dataset, [train_size, val_size], generator=generator)\n",
        "        num_workers = 0\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=DATASET_BATCH_SIZE_, shuffle=True, num_workers=num_workers, pin_memory=(DEVICE.type == 'cuda'))\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=DATASET_BATCH_SIZE_, shuffle=False, num_workers=num_workers, pin_memory=(DEVICE.type == 'cuda'))\n",
        "        print(f\"train_size: {len(train_dataset)}, val_size: {len(val_dataset)}. batch_size: {DATASET_BATCH_SIZE_}\")\n",
        "    else: print(f\"error: not enough data for train/val split. total: {len(geometric_dataset)}\")\n",
        "else: print(\"error: geometric_dataset is empty.\")\n",
        "\n",
        "\n",
        "if train_dataloader and len(train_dataloader.dataset) > 0 and 'matplotlib' in sys.modules:\n",
        "    print(f\"\\n--- visualizing a few dataset samples ---\")\n",
        "    try:\n",
        "        sample_batch = next(iter(train_dataloader))\n",
        "        num_display = min(sample_batch['image'].size(0), 4)\n",
        "        fig, axes = plt.subplots(2, num_display, figsize=(num_display * 1.8, 4.0))\n",
        "        if num_display == 1: axes_flat = [axes[0], axes[1]]\n",
        "        else: axes_flat = axes.T.flatten()\n",
        "        fig.suptitle(f\"Dataset Samples (Img {IMAGE_SIZE}x{IMAGE_SIZE}, Aliased){' (Test)' if TEST_MODE else ''}\", fontsize=10)\n",
        "        for i in range(num_display):\n",
        "            img, mask, cid = sample_batch['image'][i],sample_batch['mask'][i],sample_batch['class_id'][i].item()\n",
        "            sname = CLASS_ID_TO_SHAPE_NAME.get(cid, \"Unk\")[:10]\n",
        "            ax_img, ax_mask = (axes[0,i], axes[1,i]) if num_display > 1 else (axes_flat[0], axes_flat[1])\n",
        "            ax_img.imshow(img.squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1, interpolation='nearest')\n",
        "            ax_img.set_title(f\"{sname} (ID:{cid})\", fontsize=8); ax_img.axis('off')\n",
        "            ax_mask.imshow(mask.squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1, interpolation='nearest')\n",
        "            ax_mask.set_title(f\"Mask\", fontsize=8); ax_mask.axis('off')\n",
        "        plt.tight_layout(rect=[0,0,1,0.92]); plt.show()\n",
        "    except Exception as e: print(f\"error during visualization: {e}\")\n",
        "else: print(\"train_dataloader not available or matplotlib not imported for visualization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYRB_1ZJmeRr",
        "outputId": "c18c233a-0462-4093-9eb1-4cbb73274b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  input image_size: 16x16, num_classes: 7\n",
            "unet model initialized on cpu. trainable params: 6,768,513\n",
            "\n",
            "--- model layer names for weight tracking configuration ---\n",
            "copy relevant layer names (e.g., specific conv or attention projection weights) to\n",
            "ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] in cell 0.\n",
            "  'conv_in.weight'\n",
            "  'time_embedding.linear_1.weight'\n",
            "  'time_embedding.linear_2.weight'\n",
            "  'class_embedding.weight'\n",
            "  'down_blocks.0.resnets.0.norm1.weight'\n",
            "  'down_blocks.0.resnets.0.conv1.weight'\n",
            "  'down_blocks.0.resnets.0.time_emb_proj.weight'\n",
            "  'down_blocks.0.resnets.0.norm2.weight'\n",
            "  'down_blocks.0.resnets.0.conv2.weight'\n",
            "  'down_blocks.0.resnets.1.norm1.weight'\n",
            "  'down_blocks.0.resnets.1.conv1.weight'\n",
            "  'down_blocks.0.resnets.1.time_emb_proj.weight'\n",
            "  'down_blocks.0.resnets.1.norm2.weight'\n",
            "  'down_blocks.0.resnets.1.conv2.weight'\n",
            "  'down_blocks.0.downsamplers.0.conv.weight'\n",
            "  'down_blocks.1.attentions.0.group_norm.weight'\n",
            "  'down_blocks.1.attentions.0.to_q.weight'\n",
            "  'down_blocks.1.attentions.0.to_k.weight'\n",
            "  'down_blocks.1.attentions.0.to_v.weight'\n",
            "  'down_blocks.1.attentions.0.to_out.0.weight'\n",
            "  'down_blocks.1.attentions.1.group_norm.weight'\n",
            "  'down_blocks.1.attentions.1.to_q.weight'\n",
            "  'down_blocks.1.attentions.1.to_k.weight'\n",
            "  'down_blocks.1.attentions.1.to_v.weight'\n",
            "  'down_blocks.1.attentions.1.to_out.0.weight'\n",
            "  'down_blocks.1.resnets.0.norm1.weight'\n",
            "  'down_blocks.1.resnets.0.conv1.weight'\n",
            "  'down_blocks.1.resnets.0.time_emb_proj.weight'\n",
            "  'down_blocks.1.resnets.0.norm2.weight'\n",
            "  'down_blocks.1.resnets.0.conv2.weight'\n",
            "  'down_blocks.1.resnets.0.conv_shortcut.weight'\n",
            "  'down_blocks.1.resnets.1.norm1.weight'\n",
            "  'down_blocks.1.resnets.1.conv1.weight'\n",
            "  'down_blocks.1.resnets.1.time_emb_proj.weight'\n",
            "  'down_blocks.1.resnets.1.norm2.weight'\n",
            "  'down_blocks.1.resnets.1.conv2.weight'\n",
            "  'down_blocks.1.downsamplers.0.conv.weight'\n",
            "  'down_blocks.2.resnets.0.norm1.weight'\n",
            "  'down_blocks.2.resnets.0.conv1.weight'\n",
            "  'down_blocks.2.resnets.0.time_emb_proj.weight'\n",
            "  'down_blocks.2.resnets.0.norm2.weight'\n",
            "  'down_blocks.2.resnets.0.conv2.weight'\n",
            "  'down_blocks.2.resnets.1.norm1.weight'\n",
            "  'down_blocks.2.resnets.1.conv1.weight'\n",
            "  'down_blocks.2.resnets.1.time_emb_proj.weight'\n",
            "  'down_blocks.2.resnets.1.norm2.weight'\n",
            "  'down_blocks.2.resnets.1.conv2.weight'\n",
            "  'up_blocks.0.resnets.0.norm1.weight'\n",
            "  'up_blocks.0.resnets.0.conv1.weight'\n",
            "  'up_blocks.0.resnets.0.time_emb_proj.weight'\n",
            "  'up_blocks.0.resnets.0.norm2.weight'\n",
            "  'up_blocks.0.resnets.0.conv2.weight'\n",
            "  'up_blocks.0.resnets.0.conv_shortcut.weight'\n",
            "  'up_blocks.0.resnets.1.norm1.weight'\n",
            "  'up_blocks.0.resnets.1.conv1.weight'\n",
            "  'up_blocks.0.resnets.1.time_emb_proj.weight'\n",
            "  'up_blocks.0.resnets.1.norm2.weight'\n",
            "  'up_blocks.0.resnets.1.conv2.weight'\n",
            "  'up_blocks.0.resnets.1.conv_shortcut.weight'\n",
            "  'up_blocks.0.resnets.2.norm1.weight'\n",
            "  'up_blocks.0.resnets.2.conv1.weight'\n",
            "  'up_blocks.0.resnets.2.time_emb_proj.weight'\n",
            "  'up_blocks.0.resnets.2.norm2.weight'\n",
            "  'up_blocks.0.resnets.2.conv2.weight'\n",
            "  'up_blocks.0.resnets.2.conv_shortcut.weight'\n",
            "  'up_blocks.0.upsamplers.0.conv.weight'\n",
            "  'up_blocks.1.attentions.0.group_norm.weight'\n",
            "  'up_blocks.1.attentions.0.to_q.weight'\n",
            "  'up_blocks.1.attentions.0.to_k.weight'\n",
            "  'up_blocks.1.attentions.0.to_v.weight'\n",
            "  'up_blocks.1.attentions.0.to_out.0.weight'\n",
            "  'up_blocks.1.attentions.1.group_norm.weight'\n",
            "  'up_blocks.1.attentions.1.to_q.weight'\n",
            "  'up_blocks.1.attentions.1.to_k.weight'\n",
            "  'up_blocks.1.attentions.1.to_v.weight'\n",
            "  'up_blocks.1.attentions.1.to_out.0.weight'\n",
            "  'up_blocks.1.attentions.2.group_norm.weight'\n",
            "  'up_blocks.1.attentions.2.to_q.weight'\n",
            "  'up_blocks.1.attentions.2.to_k.weight'\n",
            "  'up_blocks.1.attentions.2.to_v.weight'\n",
            "  'up_blocks.1.attentions.2.to_out.0.weight'\n",
            "  'up_blocks.1.resnets.0.norm1.weight'\n",
            "  'up_blocks.1.resnets.0.conv1.weight'\n",
            "  'up_blocks.1.resnets.0.time_emb_proj.weight'\n",
            "  'up_blocks.1.resnets.0.norm2.weight'\n",
            "  'up_blocks.1.resnets.0.conv2.weight'\n",
            "  'up_blocks.1.resnets.0.conv_shortcut.weight'\n",
            "  'up_blocks.1.resnets.1.norm1.weight'\n",
            "  'up_blocks.1.resnets.1.conv1.weight'\n",
            "  'up_blocks.1.resnets.1.time_emb_proj.weight'\n",
            "  'up_blocks.1.resnets.1.norm2.weight'\n",
            "  'up_blocks.1.resnets.1.conv2.weight'\n",
            "  'up_blocks.1.resnets.1.conv_shortcut.weight'\n",
            "  'up_blocks.1.resnets.2.norm1.weight'\n",
            "  'up_blocks.1.resnets.2.conv1.weight'\n",
            "  'up_blocks.1.resnets.2.time_emb_proj.weight'\n",
            "  'up_blocks.1.resnets.2.norm2.weight'\n",
            "  'up_blocks.1.resnets.2.conv2.weight'\n",
            "  'up_blocks.1.resnets.2.conv_shortcut.weight'\n",
            "  'up_blocks.1.upsamplers.0.conv.weight'\n",
            "  'up_blocks.2.resnets.0.norm1.weight'\n",
            "  'up_blocks.2.resnets.0.conv1.weight'\n",
            "  'up_blocks.2.resnets.0.time_emb_proj.weight'\n",
            "  'up_blocks.2.resnets.0.norm2.weight'\n",
            "  'up_blocks.2.resnets.0.conv2.weight'\n",
            "  'up_blocks.2.resnets.0.conv_shortcut.weight'\n",
            "  'up_blocks.2.resnets.1.norm1.weight'\n",
            "  'up_blocks.2.resnets.1.conv1.weight'\n",
            "  'up_blocks.2.resnets.1.time_emb_proj.weight'\n",
            "  'up_blocks.2.resnets.1.norm2.weight'\n",
            "  'up_blocks.2.resnets.1.conv2.weight'\n",
            "  'up_blocks.2.resnets.1.conv_shortcut.weight'\n",
            "  'up_blocks.2.resnets.2.norm1.weight'\n",
            "  'up_blocks.2.resnets.2.conv1.weight'\n",
            "  'up_blocks.2.resnets.2.time_emb_proj.weight'\n",
            "  'up_blocks.2.resnets.2.norm2.weight'\n",
            "  'up_blocks.2.resnets.2.conv2.weight'\n",
            "  'up_blocks.2.resnets.2.conv_shortcut.weight'\n",
            "  'mid_block.attentions.0.norm.weight'\n",
            "  'mid_block.attentions.0.proj_in.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.norm1.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.norm2.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.norm3.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight'\n",
            "  'mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight'\n",
            "  'mid_block.attentions.0.proj_out.weight'\n",
            "  'mid_block.resnets.0.norm1.weight'\n",
            "  'mid_block.resnets.0.conv1.weight'\n",
            "  'mid_block.resnets.0.time_emb_proj.weight'\n",
            "  'mid_block.resnets.0.norm2.weight'\n",
            "  'mid_block.resnets.0.conv2.weight'\n",
            "  'mid_block.resnets.1.norm1.weight'\n",
            "  'mid_block.resnets.1.conv1.weight'\n",
            "  'mid_block.resnets.1.time_emb_proj.weight'\n",
            "  'mid_block.resnets.1.norm2.weight'\n",
            "  'mid_block.resnets.1.conv2.weight'\n",
            "  'conv_norm_out.weight'\n",
            "  'conv_out.weight'\n",
            "\n",
            "ddpm train scheduler (1000 steps) and ddim inference scheduler initialized.\n",
            "\n",
            "--- training hyperparameters (test_mode: True) ---\n",
            "  batch_size: 16, num_epochs: 10, lr: 0.0005\n",
            "  lr_warmup_steps: 50, grad_clip_norm: 1.0\n",
            "warning: lr_warmup_steps (50) too high for max_train_steps (40). adjusted to 4.\n",
            "optimizer (adamw) and lr_scheduler (cosine with warmup) initialized.\n",
            "  max_train_steps for lr_scheduler: 40\n"
          ]
        }
      ],
      "source": [
        "# cell 2: model and training configuration\n",
        "\n",
        "# diffusers and torch.optim\n",
        "from diffusers import UNet2DConditionModel, DDPMScheduler, DDIMScheduler\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "import torch.optim as optim\n",
        "\n",
        "if IMAGE_SIZE != 16: raise ValueError(\"image_size must be 16 for 'ultra-lofi' setup.\")\n",
        "print(f\"  input image_size: {IMAGE_SIZE}x{IMAGE_SIZE}, num_classes: {NUM_SHAPE_CLASSES}\")\n",
        "\n",
        "unet_config = {\n",
        "    'sample_size': IMAGE_SIZE,\n",
        "    'in_channels': 1,\n",
        "    'out_channels': 1,\n",
        "    'layers_per_block': layers_per_block_unet_,\n",
        "    'block_out_channels': block_out_channels_unet_,\n",
        "    'down_block_types': (\"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
        "    'up_block_types': (\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
        "    'cross_attention_dim': cross_attention_dim_unet_,\n",
        "    'num_class_embeds': NUM_SHAPE_CLASSES,\n",
        "    'attention_head_dim': attention_head_dim_unet_,\n",
        "}\n",
        "\n",
        "unet = UNet2DConditionModel(**unet_config).to(DEVICE)\n",
        "model_parameters = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
        "print(f\"unet model initialized on {DEVICE}. trainable params: {model_parameters:,}\")\n",
        "\n",
        "# --- utility to print layer names for weight tracking configuration ---\n",
        "print(\"\\n--- model layer names for weight tracking configuration ---\")\n",
        "print(\"copy relevant layer names (e.g., specific conv or attention projection weights) to\")\n",
        "print(\"ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] in cell 0.\")\n",
        "for name, _ in unet.named_parameters():\n",
        "    if name.endswith(\".weight\"):\n",
        "        print(f\"  '{name}'\")\n",
        "\n",
        "# --- scheduler definitions ---\n",
        "ddpm_train_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=1000,\n",
        "    beta_schedule='linear',\n",
        "    prediction_type='epsilon'\n",
        ")\n",
        "\n",
        "ddim_inference_scheduler = DDIMScheduler.from_config(ddpm_train_scheduler.config)\n",
        "print(f\"\\nddpm train scheduler ({ddpm_train_scheduler.config.num_train_timesteps} steps) and ddim inference scheduler initialized.\")\n",
        "\n",
        "# --- training configuration ---\n",
        "num_epochs = TRAIN_NUM_EPOCHS_ULOFI_\n",
        "batch_size = TRAIN_BATCH_SIZE_ULOFI_\n",
        "learning_rate = TRAIN_LEARNING_RATE_\n",
        "lr_warmup_steps = TRAIN_LR_WARMUP_STEPS_ULOFI_\n",
        "gradient_clip_norm = TRAIN_GRADIENT_CLIP_NORM_\n",
        "\n",
        "print(f\"\\n--- training hyperparameters (test_mode: {TEST_MODE}) ---\")\n",
        "print(f\"  batch_size: {batch_size}, num_epochs: {num_epochs}, lr: {learning_rate}\")\n",
        "print(f\"  lr_warmup_steps: {lr_warmup_steps}, grad_clip_norm: {gradient_clip_norm}\")\n",
        "\n",
        "# --- optimizer & learning rate scheduler ---\n",
        "optimizer = optim.AdamW(unet.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
        "\n",
        "if train_dataloader is not None and len(train_dataloader) > 0:\n",
        "    num_update_steps_per_epoch = len(train_dataloader)\n",
        "    max_train_steps = num_epochs * num_update_steps_per_epoch\n",
        "\n",
        "    if lr_warmup_steps >= max_train_steps // 2 and max_train_steps > 0:\n",
        "        actual_lr_warmup_steps = max(1, int(max_train_steps * 0.1))\n",
        "        print(f\"warning: lr_warmup_steps ({lr_warmup_steps}) too high for max_train_steps ({max_train_steps}). adjusted to {actual_lr_warmup_steps}.\")\n",
        "        lr_warmup_steps = actual_lr_warmup_steps\n",
        "\n",
        "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=lr_warmup_steps,\n",
        "        num_training_steps=max_train_steps,\n",
        "    )\n",
        "    print(f\"optimizer (adamw) and lr_scheduler (cosine with warmup) initialized.\")\n",
        "    print(f\"  max_train_steps for lr_scheduler: {max_train_steps}\")\n",
        "    if max_train_steps == 0: lr_scheduler = None\n",
        "else:\n",
        "    optimizer = None\n",
        "    lr_scheduler = None\n",
        "    max_train_steps = 0\n",
        "    print(f\"error: train_dataloader not available or empty. optimizer/lr_scheduler not fully created.\")\n",
        "    print(f\"  please ensure cell 1 (dataset engine) runs successfully and creates train_dataloader.\")\n",
        "\n",
        "previous_unet_state_dict = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFp1grBkmfX7",
        "outputId": "ecc4add3-1206-4820-bd83-6640850150d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- unet model configuration (test_mode: True) ---\n",
            "  input image_size: 16x16, num_classes for conditioning: 7\n",
            "unet model initialized on cpu. trainable parameters: 6,768,513\n",
            "\n",
            "--- model layer names for weight tracking configuration ---\n",
            "instructions: copy relevant layer names (those ending in '.weight') from the list below.\n",
            "paste them into the 'ENHANCED_METRIC_CONFIG['tracked_layers_for_weights']' list in cell 0.\n",
            "after updating cell 0, re-run cell 0, then re-run this cell (cell 2) before proceeding to cell 3.\n",
            "------------------------------\n",
            "  'conv_in.weight' (shape: [64, 1, 3, 3])\n",
            "  'time_embedding.linear_1.weight' (shape: [256, 64])\n",
            "  'time_embedding.linear_2.weight' (shape: [256, 256])\n",
            "  'class_embedding.weight' (shape: [7, 256])\n",
            "  'down_blocks.0.resnets.0.norm1.weight' (shape: [64])\n",
            "  'down_blocks.0.resnets.0.conv1.weight' (shape: [64, 64, 3, 3])\n",
            "  'down_blocks.0.resnets.0.time_emb_proj.weight' (shape: [64, 256])\n",
            "  'down_blocks.0.resnets.0.norm2.weight' (shape: [64])\n",
            "  'down_blocks.0.resnets.0.conv2.weight' (shape: [64, 64, 3, 3])\n",
            "  'down_blocks.0.resnets.1.norm1.weight' (shape: [64])\n",
            "  'down_blocks.0.resnets.1.conv1.weight' (shape: [64, 64, 3, 3])\n",
            "  'down_blocks.0.resnets.1.time_emb_proj.weight' (shape: [64, 256])\n",
            "  'down_blocks.0.resnets.1.norm2.weight' (shape: [64])\n",
            "  'down_blocks.0.resnets.1.conv2.weight' (shape: [64, 64, 3, 3])\n",
            "  'down_blocks.0.downsamplers.0.conv.weight' (shape: [64, 64, 3, 3])\n",
            "  'down_blocks.1.attentions.0.group_norm.weight' (shape: [128])\n",
            "  'down_blocks.1.attentions.0.to_q.weight' (shape: [128, 128])\n",
            "  'down_blocks.1.attentions.0.to_k.weight' (shape: [128, 128])\n",
            "  'down_blocks.1.attentions.0.to_v.weight' (shape: [128, 128])\n",
            "  'down_blocks.1.attentions.0.to_out.0.weight' (shape: [128, 128])\n",
            "  'down_blocks.1.attentions.1.group_norm.weight' (shape: [128])\n",
            "  'down_blocks.1.attentions.1.to_q.weight' (shape: [128, 128])\n",
            "  'down_blocks.1.attentions.1.to_k.weight' (shape: [128, 128])\n",
            "  'down_blocks.1.attentions.1.to_v.weight' (shape: [128, 128])\n",
            "  'down_blocks.1.attentions.1.to_out.0.weight' (shape: [128, 128])\n",
            "  'down_blocks.1.resnets.0.norm1.weight' (shape: [64])\n",
            "  'down_blocks.1.resnets.0.conv1.weight' (shape: [128, 64, 3, 3])\n",
            "  'down_blocks.1.resnets.0.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'down_blocks.1.resnets.0.norm2.weight' (shape: [128])\n",
            "  'down_blocks.1.resnets.0.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'down_blocks.1.resnets.0.conv_shortcut.weight' (shape: [128, 64, 1, 1])\n",
            "  'down_blocks.1.resnets.1.norm1.weight' (shape: [128])\n",
            "  'down_blocks.1.resnets.1.conv1.weight' (shape: [128, 128, 3, 3])\n",
            "  'down_blocks.1.resnets.1.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'down_blocks.1.resnets.1.norm2.weight' (shape: [128])\n",
            "  'down_blocks.1.resnets.1.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'down_blocks.1.downsamplers.0.conv.weight' (shape: [128, 128, 3, 3])\n",
            "  'down_blocks.2.resnets.0.norm1.weight' (shape: [128])\n",
            "  'down_blocks.2.resnets.0.conv1.weight' (shape: [128, 128, 3, 3])\n",
            "  'down_blocks.2.resnets.0.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'down_blocks.2.resnets.0.norm2.weight' (shape: [128])\n",
            "  'down_blocks.2.resnets.0.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'down_blocks.2.resnets.1.norm1.weight' (shape: [128])\n",
            "  'down_blocks.2.resnets.1.conv1.weight' (shape: [128, 128, 3, 3])\n",
            "  'down_blocks.2.resnets.1.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'down_blocks.2.resnets.1.norm2.weight' (shape: [128])\n",
            "  'down_blocks.2.resnets.1.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'up_blocks.0.resnets.0.norm1.weight' (shape: [256])\n",
            "  'up_blocks.0.resnets.0.conv1.weight' (shape: [128, 256, 3, 3])\n",
            "  'up_blocks.0.resnets.0.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'up_blocks.0.resnets.0.norm2.weight' (shape: [128])\n",
            "  'up_blocks.0.resnets.0.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'up_blocks.0.resnets.0.conv_shortcut.weight' (shape: [128, 256, 1, 1])\n",
            "  'up_blocks.0.resnets.1.norm1.weight' (shape: [256])\n",
            "  'up_blocks.0.resnets.1.conv1.weight' (shape: [128, 256, 3, 3])\n",
            "  'up_blocks.0.resnets.1.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'up_blocks.0.resnets.1.norm2.weight' (shape: [128])\n",
            "  'up_blocks.0.resnets.1.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'up_blocks.0.resnets.1.conv_shortcut.weight' (shape: [128, 256, 1, 1])\n",
            "  'up_blocks.0.resnets.2.norm1.weight' (shape: [256])\n",
            "  'up_blocks.0.resnets.2.conv1.weight' (shape: [128, 256, 3, 3])\n",
            "  'up_blocks.0.resnets.2.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'up_blocks.0.resnets.2.norm2.weight' (shape: [128])\n",
            "  'up_blocks.0.resnets.2.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'up_blocks.0.resnets.2.conv_shortcut.weight' (shape: [128, 256, 1, 1])\n",
            "  'up_blocks.0.upsamplers.0.conv.weight' (shape: [128, 128, 3, 3])\n",
            "  'up_blocks.1.attentions.0.group_norm.weight' (shape: [128])\n",
            "  'up_blocks.1.attentions.0.to_q.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.0.to_k.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.0.to_v.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.0.to_out.0.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.1.group_norm.weight' (shape: [128])\n",
            "  'up_blocks.1.attentions.1.to_q.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.1.to_k.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.1.to_v.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.1.to_out.0.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.2.group_norm.weight' (shape: [128])\n",
            "  'up_blocks.1.attentions.2.to_q.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.2.to_k.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.2.to_v.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.attentions.2.to_out.0.weight' (shape: [128, 128])\n",
            "  'up_blocks.1.resnets.0.norm1.weight' (shape: [256])\n",
            "  'up_blocks.1.resnets.0.conv1.weight' (shape: [128, 256, 3, 3])\n",
            "  'up_blocks.1.resnets.0.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'up_blocks.1.resnets.0.norm2.weight' (shape: [128])\n",
            "  'up_blocks.1.resnets.0.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'up_blocks.1.resnets.0.conv_shortcut.weight' (shape: [128, 256, 1, 1])\n",
            "  'up_blocks.1.resnets.1.norm1.weight' (shape: [256])\n",
            "  'up_blocks.1.resnets.1.conv1.weight' (shape: [128, 256, 3, 3])\n",
            "  'up_blocks.1.resnets.1.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'up_blocks.1.resnets.1.norm2.weight' (shape: [128])\n",
            "  'up_blocks.1.resnets.1.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'up_blocks.1.resnets.1.conv_shortcut.weight' (shape: [128, 256, 1, 1])\n",
            "  'up_blocks.1.resnets.2.norm1.weight' (shape: [192])\n",
            "  'up_blocks.1.resnets.2.conv1.weight' (shape: [128, 192, 3, 3])\n",
            "  'up_blocks.1.resnets.2.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'up_blocks.1.resnets.2.norm2.weight' (shape: [128])\n",
            "  'up_blocks.1.resnets.2.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'up_blocks.1.resnets.2.conv_shortcut.weight' (shape: [128, 192, 1, 1])\n",
            "  'up_blocks.1.upsamplers.0.conv.weight' (shape: [128, 128, 3, 3])\n",
            "  'up_blocks.2.resnets.0.norm1.weight' (shape: [192])\n",
            "  'up_blocks.2.resnets.0.conv1.weight' (shape: [64, 192, 3, 3])\n",
            "  'up_blocks.2.resnets.0.time_emb_proj.weight' (shape: [64, 256])\n",
            "  'up_blocks.2.resnets.0.norm2.weight' (shape: [64])\n",
            "  'up_blocks.2.resnets.0.conv2.weight' (shape: [64, 64, 3, 3])\n",
            "  'up_blocks.2.resnets.0.conv_shortcut.weight' (shape: [64, 192, 1, 1])\n",
            "  'up_blocks.2.resnets.1.norm1.weight' (shape: [128])\n",
            "  'up_blocks.2.resnets.1.conv1.weight' (shape: [64, 128, 3, 3])\n",
            "  'up_blocks.2.resnets.1.time_emb_proj.weight' (shape: [64, 256])\n",
            "  'up_blocks.2.resnets.1.norm2.weight' (shape: [64])\n",
            "  'up_blocks.2.resnets.1.conv2.weight' (shape: [64, 64, 3, 3])\n",
            "  'up_blocks.2.resnets.1.conv_shortcut.weight' (shape: [64, 128, 1, 1])\n",
            "  'up_blocks.2.resnets.2.norm1.weight' (shape: [128])\n",
            "  'up_blocks.2.resnets.2.conv1.weight' (shape: [64, 128, 3, 3])\n",
            "  'up_blocks.2.resnets.2.time_emb_proj.weight' (shape: [64, 256])\n",
            "  'up_blocks.2.resnets.2.norm2.weight' (shape: [64])\n",
            "  'up_blocks.2.resnets.2.conv2.weight' (shape: [64, 64, 3, 3])\n",
            "  'up_blocks.2.resnets.2.conv_shortcut.weight' (shape: [64, 128, 1, 1])\n",
            "  'mid_block.attentions.0.norm.weight' (shape: [128])\n",
            "  'mid_block.attentions.0.proj_in.weight' (shape: [128, 128, 1, 1])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.norm1.weight' (shape: [128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight' (shape: [128, 128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight' (shape: [128, 128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight' (shape: [128, 128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight' (shape: [128, 128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.norm2.weight' (shape: [128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight' (shape: [128, 128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight' (shape: [128, 128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight' (shape: [128, 128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight' (shape: [128, 128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.norm3.weight' (shape: [128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight' (shape: [1024, 128])\n",
            "  'mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight' (shape: [128, 512])\n",
            "  'mid_block.attentions.0.proj_out.weight' (shape: [128, 128, 1, 1])\n",
            "  'mid_block.resnets.0.norm1.weight' (shape: [128])\n",
            "  'mid_block.resnets.0.conv1.weight' (shape: [128, 128, 3, 3])\n",
            "  'mid_block.resnets.0.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'mid_block.resnets.0.norm2.weight' (shape: [128])\n",
            "  'mid_block.resnets.0.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'mid_block.resnets.1.norm1.weight' (shape: [128])\n",
            "  'mid_block.resnets.1.conv1.weight' (shape: [128, 128, 3, 3])\n",
            "  'mid_block.resnets.1.time_emb_proj.weight' (shape: [128, 256])\n",
            "  'mid_block.resnets.1.norm2.weight' (shape: [128])\n",
            "  'mid_block.resnets.1.conv2.weight' (shape: [128, 128, 3, 3])\n",
            "  'conv_norm_out.weight' (shape: [64])\n",
            "  'conv_out.weight' (shape: [1, 64, 3, 3])\n",
            "------------------------------\n",
            "\n",
            "ddpm train scheduler (1000 steps) and ddim inference scheduler initialized.\n",
            "\n",
            "--- training hyperparameters (test_mode: True) ---\n",
            "  batch_size (for training): 16\n",
            "  num_epochs: 10\n",
            "  learning_rate: 0.0005\n",
            "  lr_warmup_steps (config): 50\n",
            "  gradient_clip_norm: 1.0\n",
            "info: lr_warmup_steps (50) was high for max_train_steps (40). adjusted to 4.\n",
            "optimizer (adamw) and lr_scheduler (cosine with 4 warmup steps) initialized.\n",
            "  max_train_steps for lr_scheduler: 40\n",
            "\n",
            "--- cell 2 model & training config complete (test_mode: True) ---\n",
            "optimizer and scheduler setup appears successful.\n"
          ]
        }
      ],
      "source": [
        "# cell 2: model and training configuration\n",
        "\n",
        "from diffusers import UNet2DConditionModel, DDPMScheduler, DDIMScheduler\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "import torch.optim as optim\n",
        "\n",
        "print(f\"--- unet model configuration (test_mode: {TEST_MODE}) ---\")\n",
        "\n",
        "if IMAGE_SIZE != 16:\n",
        "    raise ValueError(\"IMAGE_SIZE from cell 0 must be 16 for 'ultra-lofi' setup.\")\n",
        "if 'NUM_SHAPE_CLASSES' not in globals() or NUM_SHAPE_CLASSES <= 0:\n",
        "    raise ValueError(\"NUM_SHAPE_CLASSES from cell 0 is not defined or invalid.\")\n",
        "print(f\"  input image_size: {IMAGE_SIZE}x{IMAGE_SIZE}, num_classes for conditioning: {NUM_SHAPE_CLASSES}\")\n",
        "\n",
        "# unet configuration dictionary using variables defined in cell 0\n",
        "unet_config = {\n",
        "    'sample_size': IMAGE_SIZE,                      # from cell 0\n",
        "    'in_channels': 1,                               # grayscale input\n",
        "    'out_channels': 1,                              # grayscale output\n",
        "    'layers_per_block': layers_per_block_unet_,     # from cell 0\n",
        "    'block_out_channels': block_out_channels_unet_, # from cell 0\n",
        "    'down_block_types': (\n",
        "        \"DownBlock2D\",      # (image_size / 2)\n",
        "        \"AttnDownBlock2D\",  # (image_size / 4)\n",
        "        \"DownBlock2D\",      # (image_size / 8) -> e.g., 16 -> 8 -> 4 -> 2x2 bottleneck\n",
        "    ),\n",
        "    'up_block_types': (\n",
        "        \"UpBlock2D\",        # (image_size / 4)\n",
        "        \"AttnUpBlock2D\",    # (image_size / 2)\n",
        "        \"UpBlock2D\",        # (image_size)\n",
        "    ),\n",
        "    'cross_attention_dim': cross_attention_dim_unet_, # from cell 0\n",
        "    'num_class_embeds': NUM_SHAPE_CLASSES,          # for class conditioning, from cell 0\n",
        "    'attention_head_dim': attention_head_dim_unet_, # from cell 0\n",
        "}\n",
        "\n",
        "unet = UNet2DConditionModel(**unet_config).to(DEVICE)\n",
        "model_parameters = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
        "print(f\"unet model initialized on {DEVICE}. trainable parameters: {model_parameters:,}\")\n",
        "\n",
        "# --- utility to print layer names for weight tracking configuration ---\n",
        "print(\"\\n--- model layer names for weight tracking configuration ---\")\n",
        "print(\"instructions: copy relevant layer names (those ending in '.weight') from the list below.\")\n",
        "print(\"paste them into the 'ENHANCED_METRIC_CONFIG['tracked_layers_for_weights']' list in cell 0.\")\n",
        "print(\"after updating cell 0, re-run cell 0, then re-run this cell (cell 2) before proceeding to cell 3.\")\n",
        "print(\"-\" * 30)\n",
        "for name, param in unet.named_parameters():\n",
        "    if name.endswith(\".weight\"): # typically track .weight, not .bias\n",
        "        print(f\"  '{name}' (shape: {list(param.shape)})\")\n",
        "print(\"-\" * 30)\n",
        "# important: user action required here based on output.\n",
        "\n",
        "# --- scheduler definitions ---\n",
        "# for training\n",
        "ddpm_train_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=1000,        # standard number of timesteps\n",
        "    beta_schedule='linear',          # common schedule\n",
        "    prediction_type='epsilon'        # model predicts the noise\n",
        ")\n",
        "# for inference/sampling (used in metrics calculation & visualization)\n",
        "ddim_inference_scheduler = DDIMScheduler.from_config(ddpm_train_scheduler.config)\n",
        "print(f\"\\nddpm train scheduler ({ddpm_train_scheduler.config.num_train_timesteps} steps) and ddim inference scheduler initialized.\")\n",
        "\n",
        "# --- training configuration from cell 0 ---\n",
        "# these variables are expected to be defined in cell 0 with the '_' suffix\n",
        "num_epochs = TRAIN_NUM_EPOCHS_ULOFI_\n",
        "# batch_size for training should align with DATASET_BATCH_SIZE_ used for dataloaders\n",
        "# TRAIN_BATCH_SIZE_ULOFI_ is the definitive training batch size from cell 0.\n",
        "# In cell 0, DATASET_BATCH_SIZE_ was set to TRAIN_BATCH_SIZE_ULOFI_.\n",
        "training_batch_size = TRAIN_BATCH_SIZE_ULOFI_\n",
        "learning_rate = TRAIN_LEARNING_RATE_\n",
        "lr_warmup_steps_config = TRAIN_LR_WARMUP_STEPS_ULOFI_ # use a distinct name before potential adjustment\n",
        "gradient_clip_norm = TRAIN_GRADIENT_CLIP_NORM_\n",
        "\n",
        "print(f\"\\n--- training hyperparameters (test_mode: {TEST_MODE}) ---\")\n",
        "print(f\"  batch_size (for training): {training_batch_size}\")\n",
        "print(f\"  num_epochs: {num_epochs}\")\n",
        "print(f\"  learning_rate: {learning_rate}\")\n",
        "print(f\"  lr_warmup_steps (config): {lr_warmup_steps_config}\")\n",
        "print(f\"  gradient_clip_norm: {gradient_clip_norm}\")\n",
        "\n",
        "# --- optimizer & learning rate scheduler ---\n",
        "# ensure train_dataloader (from cell 1) is available and has items\n",
        "if 'train_dataloader' in globals() and train_dataloader is not None and len(train_dataloader) > 0:\n",
        "    optimizer = optim.AdamW(unet.parameters(), lr=learning_rate, weight_decay=1e-2) # typical weight decay\n",
        "\n",
        "    num_update_steps_per_epoch = len(train_dataloader)\n",
        "    max_train_steps = num_epochs * num_update_steps_per_epoch\n",
        "\n",
        "    # adjust warmup steps if it's too large for total steps (esp. in test_mode)\n",
        "    actual_lr_warmup_steps = lr_warmup_steps_config\n",
        "    if lr_warmup_steps_config >= max_train_steps / 2 and max_train_steps > 0: # avoid integer division by zero\n",
        "        actual_lr_warmup_steps = max(1, int(max_train_steps * 0.1)) # 10% of total steps, or 1\n",
        "        print(f\"info: lr_warmup_steps ({lr_warmup_steps_config}) was high for max_train_steps ({max_train_steps}). adjusted to {actual_lr_warmup_steps}.\")\n",
        "\n",
        "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=actual_lr_warmup_steps,\n",
        "        num_training_steps=max_train_steps,\n",
        "    )\n",
        "    print(f\"optimizer (adamw) and lr_scheduler (cosine with {actual_lr_warmup_steps} warmup steps) initialized.\")\n",
        "    print(f\"  max_train_steps for lr_scheduler: {max_train_steps}\")\n",
        "    if max_train_steps == 0:\n",
        "        print(\"warning: max_train_steps is 0. lr_scheduler may not step. check dataloader and epochs.\")\n",
        "        # lr_scheduler will still be created but won't step if num_training_steps is 0.\n",
        "else:\n",
        "    optimizer = None # cannot optimize without data\n",
        "    lr_scheduler = None\n",
        "    max_train_steps = 0 # ensure defined\n",
        "    print(f\"error: train_dataloader from cell 1 not available or empty. optimizer/lr_scheduler cannot be fully created.\")\n",
        "    print(f\"  please re-run cell 1 successfully to create 'train_dataloader'.\")\n",
        "\n",
        "# --- variable for weight update tracking in cell 3 ---\n",
        "previous_unet_state_dict = None # will be populated during the training loop in cell 3\n",
        "\n",
        "print(f\"\\n--- cell 2 model & training config complete (test_mode: {TEST_MODE}) ---\")\n",
        "if optimizer is None:\n",
        "    print(\"critical warning: optimizer is None. training cannot proceed. check train_dataloader in cell 1.\")\n",
        "else:\n",
        "    print(\"optimizer and scheduler setup appears successful.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "c77ffae60c794425b6866181cfad5bad",
            "52349ea4dd9542b98178b99f91558d9d",
            "ad57104d7e954b27bccf4f15934aacdc",
            "d66bdee52f2942dc8bb1756ddb57dbe8",
            "a0b3fe77f4b74c6799545a64a8c064ff",
            "daa512b7aab74d44a161cb3099ca6be7",
            "37516cd89e8744cda9d9d74bccb44dd8",
            "19f8065df85b4db08571b8e492602672",
            "5abc2f316a3c44a3a5ef28adb8b4b9af",
            "f75d8eaf16bb4fa0b1385e7a2f1c742b",
            "db08d5d62d2f45f888a331c03d03841f",
            "46e96862b7a24eba83dff7e041be7c15",
            "43aa06377b83403080db17db1215de9b",
            "f3808c4cc70a4b01b513b9a6338e7dd0",
            "6f3d0a8def68401f96c159ccb2825a4d",
            "0a83fdc9638b4014b41669cdf35266e5",
            "45a85c13511d4d0cb7604440d6f564bd",
            "59e4c30c3d264a1e9e706f499eb3fc65",
            "7d42f7c840a345ca909836b69df216c6",
            "c5cad83a33984d5d9fba73368a02b485",
            "5373f4990a3f4eafb55c25f272a8b13a",
            "3ed317ebebbc40b9960086e4ba7d6917",
            "49dbfe060f3f4a93b505d82dc9837b8b",
            "8a66b3bca6774786aa07b5b86ef8e937",
            "bb724707a82b47ff9fe74703bb72cc30",
            "f42d0e47f3ae4c8dbb8d436e477697c6",
            "b3ffc869431145ca9268b7a63bebe6a1",
            "e4fbe1814f6e4fb388a73f7a7512be4c",
            "8789ae1369ef4e1da51c59280824e129",
            "3dcb13fab38e42f1b1c0341002db7c47",
            "ffd014d8183b4146ab165fa666db5e79",
            "00ff5621e6814cb4a8e92e40d62fe4a0",
            "56fbe536e82f4f24807ebfb56e7311e2",
            "81ac840bec3c4b4995a29ab360d003ba",
            "b6eabec552de4d109094bde0cc9ae6ef",
            "e5e245f42be44328be791a15ea4c1aca",
            "9aaeec6f3a80471088afa6f1181e18b4",
            "28c5253e734c4fb1b0349499203bf19e",
            "fe131076c7bd45fc9742301591d44134",
            "c543ed4b82ad44a388a2cf923664b6c6",
            "c8a0580e9e914382a0132d05adfeffd5",
            "55025b238c0945b98edbbd561f03bdb2",
            "e3854c561b5e46018c0b887b0aa004ff",
            "bb3ac54aa3314001bf929fe52fa41b41"
          ]
        },
        "id": "UM6gdDu1mfk0",
        "outputId": "ad59a36d-9026-4c8c-8d36-80d65a837b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- training engine setup (test_mode: True, enhanced_metrics: enabled) ---\n",
            "  device: cpu, target epochs: 10\n",
            "\n",
            "starting training for 10 epochs on cpu...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "epoch 1/10 [train]:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c77ffae60c794425b6866181cfad5bad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "epoch 1/10 [val]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46e96862b7a24eba83dff7e041be7c15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 001/10 | dur:   6.50s | tr_loss: 0.84050 | vl_loss: 0.32196 | lr: 5.0e-04\n",
            "  -> val_loss improved to 0.32196. model saved to unet_checkpoint_img16_aliased.pth\n",
            "  generating samples & calculating metrics (epoch 1, ddim 20 steps)...\n",
            "  img_metrics: varian:0.02, global:8.00, mean:4.00, binary:3.50, binary:2.00, mean:0.26, mse:0.04, ssim:0.04, sobel:131.14, metric:0.13, metric:0.60\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "epoch 2/10 [train]:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49dbfe060f3f4a93b505d82dc9837b8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "epoch 2/10 [val]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81ac840bec3c4b4995a29ab360d003ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 002/10 | dur:  10.87s | tr_loss: 0.29019 | vl_loss: 0.22747 | lr: 4.8e-04\n"
          ]
        }
      ],
      "source": [
        "# cell 3: training engine - loop, validation, checkpointing & enhanced emergent metrics\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# optim, tqdm, json, time, gc, Path, random, np, cv2, skimage.measure, skimage.metrics\n",
        "# are assumed to be imported in cell 0 or available.\n",
        "from tqdm.notebook import tqdm # ensure tqdm is available for progress bars\n",
        "import copy # for deepcopying state_dicts if necessary\n",
        "\n",
        "print(f\"--- training engine setup (test_mode: {TEST_MODE}, enhanced_metrics: enabled) ---\")\n",
        "# ensure unet and other essentials from cell 2 are loaded\n",
        "if 'unet' not in globals() or 'ddpm_train_scheduler' not in globals() or 'optimizer' not in globals():\n",
        "    raise RuntimeError(\"unet, schedulers, or optimizer not found. ensure cells 0, 1, 2 ran successfully.\")\n",
        "else:\n",
        "    print(f\"  device: {DEVICE}, target epochs: {TRAIN_NUM_EPOCHS_ULOFI_}\") # TRAIN_NUM_EPOCHS_ULOFI_ from cell 0\n",
        "\n",
        "# --- 3.1: emergent metric helper functions (expanded) ---\n",
        "\n",
        "def convert_numpy_types_to_native(obj):\n",
        "    # recursively convert numpy numeric types for json serialization.\n",
        "    if isinstance(obj, dict): return {k: convert_numpy_types_to_native(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, list): return [convert_numpy_types_to_native(elem) for elem in obj]\n",
        "    if isinstance(obj, np.integer): return int(obj)\n",
        "    if isinstance(obj, np.floating): return float(obj)\n",
        "    if isinstance(obj, np.ndarray): return convert_numpy_types_to_native(obj.tolist())\n",
        "    return obj\n",
        "\n",
        "def calculate_pixel_variance(grayscale_image_np):\n",
        "    return np.var(grayscale_image_np) if grayscale_image_np is not None and grayscale_image_np.size > 0 else 0.0\n",
        "\n",
        "def calculate_shannon_entropy_global(grayscale_image_np):\n",
        "    # global shannon entropy.\n",
        "    if grayscale_image_np is None or grayscale_image_np.size == 0: return 0.0\n",
        "    # normalize image to [0,1] for skimage.measure.shannon_entropy\n",
        "    min_val, max_val = np.min(grayscale_image_np), np.max(grayscale_image_np)\n",
        "    if max_val == min_val: return 0.0 # avoid division by zero if image is flat\n",
        "    img_norm = (grayscale_image_np - min_val) / (max_val - min_val)\n",
        "    return skimage.measure.shannon_entropy(img_norm)\n",
        "\n",
        "def calculate_local_shannon_entropy(grayscale_image_np, patch_size_tuple):\n",
        "    # calculates mean and variance of shannon entropy across non-overlapping patches.\n",
        "    if grayscale_image_np is None or grayscale_image_np.size == 0: return 0.0, 0.0\n",
        "    h, w = grayscale_image_np.shape\n",
        "    ph, pw = patch_size_tuple\n",
        "    patch_entropies = []\n",
        "    for i in range(0, h - ph + 1, ph):\n",
        "        for j in range(0, w - pw + 1, pw):\n",
        "            patch = grayscale_image_np[i:i+ph, j:j+pw]\n",
        "            if patch.size > 0:\n",
        "                 # normalize each patch independently for its entropy calculation\n",
        "                min_val, max_val = np.min(patch), np.max(patch)\n",
        "                if max_val == min_val: patch_entropy = 0.0 # flat patch\n",
        "                else:\n",
        "                    patch_norm = (patch - min_val) / (max_val - min_val)\n",
        "                    patch_entropy = skimage.measure.shannon_entropy(patch_norm)\n",
        "                patch_entropies.append(patch_entropy)\n",
        "    if not patch_entropies: return 0.0, 0.0\n",
        "    return np.mean(patch_entropies), np.var(patch_entropies)\n",
        "\n",
        "def calculate_num_clusters_binary(binary_image_np):\n",
        "    if binary_image_np is None or binary_image_np.size == 0: return 0\n",
        "    binary_u8 = (binary_image_np.clip(0,1) * 255).astype(np.uint8)\n",
        "    num_labels, _, _, _ = cv2.connectedComponentsWithStats(binary_u8, connectivity=8)\n",
        "    return num_labels - 1 # subtract background label\n",
        "\n",
        "def calculate_noise_pixels_binary(binary_image_np, noise_threshold_area):\n",
        "    if binary_image_np is None or binary_image_np.size == 0: return 0\n",
        "    binary_u8 = (binary_image_np.clip(0,1) * 255).astype(np.uint8)\n",
        "    num_labels, _, stats, _ = cv2.connectedComponentsWithStats(binary_u8, connectivity=8)\n",
        "    if num_labels <= 1: return 0 # only background or one component\n",
        "    # stats[0] is background, so slice from 1\n",
        "    noise_count = np.sum(stats[1:, cv2.CC_STAT_AREA] <= noise_threshold_area)\n",
        "    return int(noise_count)\n",
        "\n",
        "def calculate_iou_with_gt(binary_pred_np, binary_gt_np):\n",
        "    # calculates intersection over union. inputs are 0/1 numpy arrays.\n",
        "    if binary_pred_np is None or binary_gt_np is None or binary_pred_np.shape != binary_gt_np.shape: return 0.0\n",
        "    intersection = np.logical_and(binary_pred_np, binary_gt_np).sum()\n",
        "    union = np.logical_or(binary_pred_np, binary_gt_np).sum()\n",
        "    if union == 0: return 1.0 if intersection == 0 else 0.0 # both empty or pred empty and gt not, or vice versa\n",
        "    return intersection / union\n",
        "\n",
        "def calculate_output_stability_pair(img1_gray_np, img2_gray_np):\n",
        "    if img1_gray_np is None or img2_gray_np is None or img1_gray_np.shape != img2_gray_np.shape: return 0.0, 0.0\n",
        "    mse = np.mean((img1_gray_np - img2_gray_np)**2)\n",
        "    data_range = np.max(img1_gray_np) - np.min(img1_gray_np)\n",
        "    if data_range < 1e-6: data_range = 1.0 # handle flat images for ssim\n",
        "    min_dim = min(img1_gray_np.shape[0], img1_gray_np.shape[1])\n",
        "    win_size = min(5, min_dim) # ensure win_size is odd and positive\n",
        "    if win_size % 2 == 0: win_size -= 1\n",
        "    if win_size < 1: win_size = 1\n",
        "\n",
        "    ssim = 0.0\n",
        "    if win_size >= 1 and win_size <= min_dim: # ssim requires win_size <= spatial_dims\n",
        "        try:\n",
        "            ssim = skimage.metrics.structural_similarity(\n",
        "                img1_gray_np, img2_gray_np, data_range=data_range,\n",
        "                channel_axis=None, win_size=win_size # ensure channel_axis is None for 2d grayscale\n",
        "            )\n",
        "        except ValueError: ssim = 0.0 # catch if win_size is still problematic\n",
        "    return mse, ssim\n",
        "\n",
        "def calculate_edge_sharpness_sobel(grayscale_image_np):\n",
        "    if grayscale_image_np is None or grayscale_image_np.size == 0: return 0.0\n",
        "    # ensure image is uint8 for sobel\n",
        "    img_u8 = (grayscale_image_np.clip(0,1) * 255).astype(np.uint8)\n",
        "    k_size = 3 if min(img_u8.shape) >=3 else 1 # sobel ksize must be 1,3,5, or 7\n",
        "    sobelx = cv2.Sobel(img_u8, cv2.CV_64F, 1, 0, ksize=k_size)\n",
        "    sobely = cv2.Sobel(img_u8, cv2.CV_64F, 0, 1, ksize=k_size)\n",
        "    magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
        "    return np.mean(magnitude)\n",
        "\n",
        "def calculate_contour_metrics(binary_image_np):\n",
        "    # circularity and solidity from largest contour.\n",
        "    if binary_image_np is None or binary_image_np.size == 0: return 0.0, 0.0 # default if no binary image\n",
        "    binary_u8 = (binary_image_np.clip(0,1) * 255).astype(np.uint8)\n",
        "    contours, _ = cv2.findContours(binary_u8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if not contours: return 0.0, 0.0 # no contours found\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    area = cv2.contourArea(largest_contour)\n",
        "    if area < 1e-6: return 0.0, 0.0 # avoid division by zero for tiny contours\n",
        "    perimeter = cv2.arcLength(largest_contour, True)\n",
        "    circularity = (4 * np.pi * area) / (perimeter**2) if perimeter > 1e-6 else 0.0\n",
        "    hull = cv2.convexHull(largest_contour)\n",
        "    hull_area = cv2.contourArea(hull)\n",
        "    solidity = area / hull_area if hull_area > 1e-6 else 0.0\n",
        "    return circularity, solidity\n",
        "\n",
        "# --- 3.2: helper for qualitative sampling & all metric calculation ---\n",
        "@torch.no_grad()\n",
        "def sample_and_calc_metrics(\n",
        "    model_to_sample, sampling_scheduler, class_ids_list, gt_masks_list_np, # gt_masks_list_np is new\n",
        "    num_inference_steps_sample=30, image_size_val=IMAGE_SIZE, device_val=DEVICE,\n",
        "    seed_offset_base=3000, cfg=ENHANCED_METRIC_CONFIG # Pass enhanced config\n",
        "    ):\n",
        "    model_to_sample.eval() # set model to evaluation mode\n",
        "    num_unique_classes = len(class_ids_list)\n",
        "    if num_unique_classes == 0: return [], [], {} # no classes to sample\n",
        "\n",
        "    # generate initial latents for main samples\n",
        "    latents_main = torch.randn(\n",
        "        (num_unique_classes, model_to_sample.config.in_channels, image_size_val, image_size_val),\n",
        "        device=device_val, generator=torch.Generator(device=device_val).manual_seed(MASTER_SEED + seed_offset_base)\n",
        "    )\n",
        "    class_labels_tensor_main = torch.tensor(class_ids_list, device=device_val, dtype=torch.long)\n",
        "\n",
        "    sampling_scheduler.set_timesteps(num_inference_steps_sample)\n",
        "    current_latents_main = latents_main.clone()\n",
        "    for t_val in sampling_scheduler.timesteps: # ddim sampling loop\n",
        "        model_output = model_to_sample(sample=current_latents_main, timestep=t_val, encoder_hidden_states=None, class_labels=class_labels_tensor_main).sample\n",
        "        current_latents_main = sampling_scheduler.step(model_output, t_val, current_latents_main).prev_sample\n",
        "\n",
        "    # post-process images: scale from [-1,1] to [0,1] and binarize\n",
        "    images_gray_main = (current_latents_main / 2 + 0.5).clamp(0, 1)\n",
        "    images_binary_main = (images_gray_main > 0.5).float() # binarize for metrics like iou, clusters\n",
        "\n",
        "    # convert to numpy for metric calculations\n",
        "    images_gray_main_np = images_gray_main.cpu().permute(0, 2, 3, 1).squeeze(-1).numpy()\n",
        "    images_binary_main_np = images_binary_main.cpu().permute(0, 2, 3, 1).squeeze(-1).numpy()\n",
        "\n",
        "    # accumulator for metrics, initialized for all expected keys\n",
        "    epoch_metrics_accumulator = {key: [] for key in EMERGENT_METRIC_KEYS if not key.startswith(\"w_update_\")} # weight updates handled separately\n",
        "\n",
        "    for i in range(num_unique_classes):\n",
        "        gray_np, binary_np = images_gray_main_np[i], images_binary_main_np[i]\n",
        "        gt_mask_np = gt_masks_list_np[i] if gt_masks_list_np is not None and i < len(gt_masks_list_np) else None\n",
        "        class_id = class_ids_list[i]\n",
        "\n",
        "        if 'pixel_variance' in epoch_metrics_accumulator:\n",
        "            epoch_metrics_accumulator['pixel_variance'].append(calculate_pixel_variance(gray_np))\n",
        "        if 'shannon_entropy_global' in epoch_metrics_accumulator:\n",
        "            epoch_metrics_accumulator['shannon_entropy_global'].append(calculate_shannon_entropy_global(gray_np))\n",
        "        if 'local_entropy_mean' in epoch_metrics_accumulator and 'local_entropy_variance' in epoch_metrics_accumulator:\n",
        "            le_mean, le_var = calculate_local_shannon_entropy(gray_np, cfg['local_entropy_patch_size'])\n",
        "            epoch_metrics_accumulator['local_entropy_mean'].append(le_mean)\n",
        "            epoch_metrics_accumulator['local_entropy_variance'].append(le_var)\n",
        "        if 'num_clusters_binary' in epoch_metrics_accumulator:\n",
        "            epoch_metrics_accumulator['num_clusters_binary'].append(calculate_num_clusters_binary(binary_np))\n",
        "        if 'noise_pixels_binary' in epoch_metrics_accumulator:\n",
        "            epoch_metrics_accumulator['noise_pixels_binary'].append(calculate_noise_pixels_binary(binary_np, cfg['noise_pixels_binary_threshold_area']))\n",
        "        if 'IoU_with_gt_mean' in epoch_metrics_accumulator and gt_mask_np is not None:\n",
        "            epoch_metrics_accumulator['IoU_with_gt_mean'].append(calculate_iou_with_gt(binary_np, gt_mask_np))\n",
        "        if 'edge_sharpness_sobel' in epoch_metrics_accumulator:\n",
        "            epoch_metrics_accumulator['edge_sharpness_sobel'].append(calculate_edge_sharpness_sobel(gray_np))\n",
        "\n",
        "        circ, solid = calculate_contour_metrics(binary_np)\n",
        "        if 'circularity_metric' in epoch_metrics_accumulator and CLASS_ID_TO_SHAPE_NAME.get(class_id) == 'circle':\n",
        "             epoch_metrics_accumulator['circularity_metric'].append(circ) # only for circles\n",
        "        if 'solidity_metric' in epoch_metrics_accumulator:\n",
        "            epoch_metrics_accumulator['solidity_metric'].append(solid)\n",
        "\n",
        "    # output stability metrics (mse, ssim)\n",
        "    num_stability_k = cfg['num_stability_samples_k']\n",
        "    if num_unique_classes > 0 and num_stability_k > 1 and \\\n",
        "       ('output_stability_mse' in epoch_metrics_accumulator or 'output_stability_ssim' in epoch_metrics_accumulator):\n",
        "        # pick one class for stability check, e.g., the first one in the list\n",
        "        stability_class_id = class_ids_list[0]\n",
        "        stability_samples_gray_np_list = []\n",
        "        for k_idx in range(num_stability_k): # generate k samples for this class\n",
        "            latents_stability = torch.randn(\n",
        "                (1, model_to_sample.config.in_channels, image_size_val, image_size_val), device=device_val,\n",
        "                generator=torch.Generator(device=device_val).manual_seed(MASTER_SEED + seed_offset_base + 1000 + stability_class_id + k_idx)\n",
        "            )\n",
        "            class_label_stability = torch.tensor([stability_class_id], device=device_val, dtype=torch.long)\n",
        "            current_latents_stability = latents_stability.clone()\n",
        "            for t_val in sampling_scheduler.timesteps: # ddim sampling\n",
        "                model_output_stab = model_to_sample(sample=current_latents_stability, timestep=t_val, encoder_hidden_states=None, class_labels=class_label_stability).sample\n",
        "                current_latents_stability = sampling_scheduler.step(model_output_stab, t_val, current_latents_stability).prev_sample\n",
        "            stability_samples_gray_np_list.append((current_latents_stability / 2 + 0.5).clamp(0, 1).cpu().squeeze().numpy())\n",
        "\n",
        "        if len(stability_samples_gray_np_list) == num_stability_k:\n",
        "            stability_mses, stability_ssims = [], []\n",
        "            for k1 in range(num_stability_k): # pairwise comparisons\n",
        "                for k2 in range(k1 + 1, num_stability_k):\n",
        "                    mse_pair, ssim_pair = calculate_output_stability_pair(stability_samples_gray_np_list[k1], stability_samples_gray_np_list[k2])\n",
        "                    stability_mses.append(mse_pair); stability_ssims.append(ssim_pair)\n",
        "            if stability_mses and 'output_stability_mse' in epoch_metrics_accumulator:\n",
        "                epoch_metrics_accumulator['output_stability_mse'].append(np.mean(stability_mses))\n",
        "            if stability_ssims and 'output_stability_ssim' in epoch_metrics_accumulator:\n",
        "                epoch_metrics_accumulator['output_stability_ssim'].append(np.mean(stability_ssims))\n",
        "\n",
        "    # average metrics that were collected per-sample (most of them)\n",
        "    averaged_metrics = {}\n",
        "    for key, values in epoch_metrics_accumulator.items():\n",
        "        if values: # if list is not empty\n",
        "            averaged_metrics[key] = np.mean(values)\n",
        "        elif key in EMERGENT_METRIC_KEYS: # ensure key exists with a default if no values collected (e.g. circularity for non-circle batches)\n",
        "             averaged_metrics[key] = 0.0\n",
        "\n",
        "\n",
        "    model_to_sample.train() # set model back to training mode\n",
        "    return images_gray_main_np, images_binary_main_np, averaged_metrics\n",
        "\n",
        "# --- 3.3: Training Loop ---\n",
        "# initialize training history dictionary\n",
        "training_history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'learning_rate': []}\n",
        "# add all metric keys (including placeholders for weight updates that will be filled later)\n",
        "for key in EMERGENT_METRIC_KEYS: training_history[f'metric_{key}'] = []\n",
        "# dynamically add keys for each tracked weight layer\n",
        "for layer_name_tracked in ENHANCED_METRIC_CONFIG.get('tracked_layers_for_weights', []):\n",
        "    metric_key = f\"w_update_{layer_name_tracked.replace('.', '_')}\" # make key filename-friendly\n",
        "    training_history[f'metric_{metric_key}'] = []\n",
        "    if metric_key not in EMERGENT_METRIC_KEYS: EMERGENT_METRIC_KEYS.append(metric_key) # ensure it's in the main list\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs_since_last_improvement = 0\n",
        "\n",
        "# check essential components from cell 1 (dataloaders) and cell 2 (model, optim, sched)\n",
        "essential_components_check = [\n",
        "    'unet', 'ddpm_train_scheduler', 'ddim_inference_scheduler', 'optimizer', 'lr_scheduler',\n",
        "    'train_dataloader', 'val_dataloader', 'previous_unet_state_dict' # previous_unet_state_dict initialized in cell 2\n",
        "]\n",
        "if not all(comp in globals() and globals()[comp] is not None for comp in essential_components_check if comp != 'previous_unet_state_dict'): # previous_unet_state_dict can be None initially\n",
        "    print(\"error: essential components for training missing. ensure cells 0, 1, 2 ran successfully and created all variables.\")\n",
        "else:\n",
        "    print(f\"\\nstarting training for {TRAIN_NUM_EPOCHS_ULOFI_} epochs on {DEVICE}...\")\n",
        "    overall_start_time = time.time()\n",
        "\n",
        "    for epoch in range(TRAIN_NUM_EPOCHS_ULOFI_):\n",
        "        epoch_start_time = time.time()\n",
        "        unet.train() # set model to training mode\n",
        "        total_train_loss_epoch = 0.0\n",
        "\n",
        "        # use global previous_unet_state_dict from cell 2\n",
        "        global previous_unet_state_dict\n",
        "\n",
        "        train_progress_bar = tqdm(train_dataloader, desc=f\"epoch {epoch+1}/{TRAIN_NUM_EPOCHS_ULOFI_} [train]\", leave=False)\n",
        "        for step, batch in enumerate(train_progress_bar):\n",
        "            clean_images = batch['image'].to(DEVICE) # images are [0,1] from dataloader\n",
        "            class_ids_batch = batch['class_id'].to(DEVICE)\n",
        "\n",
        "            images_normalized = clean_images * 2.0 - 1.0 # transform to [-1,1] for diffusion model\n",
        "            noise_target = torch.randn_like(images_normalized) # noise to predict\n",
        "            timesteps_batch = torch.randint(\n",
        "                0, ddpm_train_scheduler.config.num_train_timesteps,\n",
        "                (images_normalized.shape[0],), device=DEVICE\n",
        "            ).long()\n",
        "\n",
        "            noisy_images_input = ddpm_train_scheduler.add_noise(images_normalized, noise_target, timesteps_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predicted_noise = unet(sample=noisy_images_input, timestep=timesteps_batch, encoder_hidden_states=None, class_labels=class_ids_batch).sample\n",
        "            loss = nn.functional.mse_loss(predicted_noise, noise_target)\n",
        "            total_train_loss_epoch += loss.item()\n",
        "            loss.backward()\n",
        "            if TRAIN_GRADIENT_CLIP_NORM_ > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(unet.parameters(), TRAIN_GRADIENT_CLIP_NORM_)\n",
        "            optimizer.step()\n",
        "            if lr_scheduler: lr_scheduler.step()\n",
        "            train_progress_bar.set_postfix(loss=loss.item(), lr=lr_scheduler.get_last_lr()[0] if lr_scheduler else TRAIN_LEARNING_RATE_)\n",
        "\n",
        "        avg_train_loss_epoch = total_train_loss_epoch / len(train_dataloader) if len(train_dataloader) > 0 else 0.0\n",
        "\n",
        "        # --- weight update norm calculation ---\n",
        "        current_epoch_weight_metrics = {}\n",
        "        if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights']:\n",
        "            current_unet_state_dict_for_update = {k: v.cpu().clone() for k, v in unet.state_dict().items()} # work with cpu copy\n",
        "            if previous_unet_state_dict is not None:\n",
        "                for layer_name in ENHANCED_METRIC_CONFIG['tracked_layers_for_weights']:\n",
        "                    if layer_name in current_unet_state_dict_for_update and layer_name in previous_unet_state_dict:\n",
        "                        curr_w = current_unet_state_dict_for_update[layer_name]\n",
        "                        prev_w = previous_unet_state_dict[layer_name]\n",
        "                        if curr_w.shape == prev_w.shape: # ensure shapes match (should always)\n",
        "                            w_update_norm = torch.linalg.norm(curr_w - prev_w).item()\n",
        "                            metric_key = f\"metric_w_update_{layer_name.replace('.', '_')}\"\n",
        "                            current_epoch_weight_metrics[metric_key] = w_update_norm\n",
        "                        else:\n",
        "                            print(f\"warning: shape mismatch for layer {layer_name} between epochs. skipping update norm.\")\n",
        "                    else:\n",
        "                        print(f\"warning: layer {layer_name} not found in one of the state_dicts. skipping update norm.\")\n",
        "            # store current state (cpu copy) for next epoch's comparison\n",
        "            previous_unet_state_dict = current_unet_state_dict_for_update # already a deep copy on cpu\n",
        "            # (no need to unet.to(DEVICE) as we worked on a copy)\n",
        "        # log weight metrics\n",
        "        for key, value in current_epoch_weight_metrics.items():\n",
        "            if key in training_history: training_history[key].append(value)\n",
        "            else: training_history[key] = [value] # if key somehow missed initial setup\n",
        "\n",
        "        # --- validation phase ---\n",
        "        unet.eval() # set model to evaluation mode\n",
        "        total_val_loss_epoch = 0.0\n",
        "        with torch.no_grad():\n",
        "            val_progress_bar = tqdm(val_dataloader, desc=f\"epoch {epoch+1}/{TRAIN_NUM_EPOCHS_ULOFI_} [val]\", leave=False)\n",
        "            for batch_val in val_progress_bar:\n",
        "                images_val, class_ids_val_batch = batch_val['image'].to(DEVICE), batch_val['class_id'].to(DEVICE)\n",
        "                images_val_normalized = images_val * 2.0 - 1.0\n",
        "                noise_val_target = torch.randn_like(images_val_normalized)\n",
        "                timesteps_val_batch = torch.randint(\n",
        "                    0, ddpm_train_scheduler.config.num_train_timesteps,\n",
        "                    (images_val_normalized.shape[0],), device=DEVICE\n",
        "                ).long()\n",
        "                noisy_images_val_input = ddpm_train_scheduler.add_noise(images_val_normalized, noise_val_target, timesteps_val_batch)\n",
        "                predicted_noise_val = unet(sample=noisy_images_val_input, encoder_hidden_states=None, timestep=timesteps_val_batch, class_labels=class_ids_val_batch).sample\n",
        "                total_val_loss_epoch += nn.functional.mse_loss(predicted_noise_val, noise_val_target).item()\n",
        "        avg_val_loss_epoch = total_val_loss_epoch / len(val_dataloader) if len(val_dataloader) > 0 else 0.0\n",
        "        current_lr_epoch = lr_scheduler.get_last_lr()[0] if lr_scheduler else TRAIN_LEARNING_RATE_\n",
        "\n",
        "        print(f\"epoch {epoch+1:03d}/{TRAIN_NUM_EPOCHS_ULOFI_} | dur: {time.time()-epoch_start_time:6.2f}s | tr_loss: {avg_train_loss_epoch:.5f} | vl_loss: {avg_val_loss_epoch:.5f} | lr: {current_lr_epoch:.1e}\")\n",
        "\n",
        "        training_history['epoch'].append(epoch + 1)\n",
        "        training_history['train_loss'].append(avg_train_loss_epoch)\n",
        "        training_history['val_loss'].append(avg_val_loss_epoch)\n",
        "        training_history['learning_rate'].append(current_lr_epoch)\n",
        "\n",
        "        # checkpointing\n",
        "        if avg_val_loss_epoch < best_val_loss:\n",
        "            best_val_loss = avg_val_loss_epoch\n",
        "            torch.save(unet.state_dict(), MODEL_CHECKPOINT_FILE)\n",
        "            print(f\"  -> val_loss improved to {best_val_loss:.5f}. model saved to {MODEL_CHECKPOINT_FILE.name}\")\n",
        "            epochs_since_last_improvement = 0\n",
        "        else:\n",
        "            epochs_since_last_improvement += 1\n",
        "\n",
        "        # --- sample images and calculate emergent metrics for this epoch ---\n",
        "        current_epoch_img_metrics = {key: 0.0 for key in EMERGENT_METRIC_KEYS if not key.startswith(\"w_update_\")} # default values\n",
        "\n",
        "        if (epoch + 1) % metrics_sample_freq == 0 or (epoch + 1) == TRAIN_NUM_EPOCHS_ULOFI_:\n",
        "            # select a subset of classes for visualization and detailed metric calculation\n",
        "            num_ddim_steps_viz = 20 if TEST_MODE else 40\n",
        "            num_classes_to_vis = min(NUM_SHAPE_CLASSES, 2) if TEST_MODE else min(NUM_SHAPE_CLASSES, 4)\n",
        "\n",
        "            all_class_ids_available = list(CLASS_ID_TO_SHAPE_NAME.keys())\n",
        "            if num_classes_to_vis > len(all_class_ids_available): num_classes_to_vis = len(all_class_ids_available)\n",
        "\n",
        "            # create a consistent subset of class_ids for metric sampling across epochs\n",
        "            # uses linspace to pick evenly distributed classes from the sorted list\n",
        "            sample_indices_for_metrics = np.linspace(0, len(all_class_ids_available)-1, num_classes_to_vis, dtype=int).tolist()\n",
        "            sample_class_ids_for_metrics = [all_class_ids_available[i] for i in sample_indices_for_metrics]\n",
        "\n",
        "            # retrieve corresponding ground truth masks for these sampled classes\n",
        "            # this needs a bit of care: sample from the val_dataloader to get corresponding GT masks\n",
        "            # for simplicity, we'll assume val_dataloader can provide these if needed, or we sample from full dataset\n",
        "            # for now, let's just grab some GT masks from the *full* geometric_dataset\n",
        "            # this is not ideal if val_dataset is very different, but okay for this setup\n",
        "            sample_gt_masks_np = []\n",
        "            if 'IoU_with_gt_mean' in EMERGENT_METRIC_KEYS and geometric_dataset and len(geometric_dataset)>0:\n",
        "                temp_gt_masks = {} # store one mask per class_id\n",
        "                for i in range(len(geometric_dataset)):\n",
        "                    sample_item = geometric_dataset[i] # getitem gives normalized image and original mask\n",
        "                    class_id_item = sample_item['class_id'].item()\n",
        "                    if class_id_item in sample_class_ids_for_metrics and class_id_item not in temp_gt_masks:\n",
        "                        temp_gt_masks[class_id_item] = sample_item['mask'].squeeze().cpu().numpy()\n",
        "                    if len(temp_gt_masks) == len(sample_class_ids_for_metrics): break\n",
        "                # order them correctly\n",
        "                for cid in sample_class_ids_for_metrics: sample_gt_masks_np.append(temp_gt_masks.get(cid))\n",
        "\n",
        "\n",
        "            if sample_class_ids_for_metrics:\n",
        "                print(f\"  generating samples & calculating metrics (epoch {epoch+1}, ddim {num_ddim_steps_viz} steps)...\")\n",
        "                # pass the collected GT masks to the sampling function\n",
        "                vis_gray_np, vis_binary_np, calculated_img_metrics = sample_and_calc_metrics(\n",
        "                    unet, ddim_inference_scheduler, sample_class_ids_for_metrics, sample_gt_masks_np,\n",
        "                    num_ddim_steps_viz, cfg=ENHANCED_METRIC_CONFIG\n",
        "                )\n",
        "                current_epoch_img_metrics.update(calculated_img_metrics) # update with actual values\n",
        "                metric_summary_str = \", \".join([f\"{k.split('_')[-1][:6]}:{v:.2f}\" for k,v in current_epoch_img_metrics.items() if v != 0.0 and not k.startswith(\"metric_w_update_\")])\n",
        "                print(f\"  img_metrics: {metric_summary_str}\")\n",
        "\n",
        "                # visualize generated samples\n",
        "                if 'matplotlib' in sys.modules and vis_gray_np is not None and len(vis_gray_np) > 0 :\n",
        "                    fig, axes = plt.subplots(2, num_classes_to_vis, figsize=(num_classes_to_vis * 1.8, 3.8))\n",
        "                    if num_classes_to_vis == 1: axes = np.array(axes).reshape(2,1) # handle single column case\n",
        "                    fig.suptitle(f\"Ep {epoch+1} (DDIM {num_ddim_steps_viz} steps){' (Test)' if TEST_MODE else ''}\", fontsize=9)\n",
        "                    for i in range(num_classes_to_vis):\n",
        "                        s_name = CLASS_ID_TO_SHAPE_NAME.get(sample_class_ids_for_metrics[i],\"Unk\")[:10]\n",
        "                        ax_img = axes[0,i] if num_classes_to_vis > 1 else axes[0]\n",
        "                        ax_bin = axes[1,i] if num_classes_to_vis > 1 else axes[1]\n",
        "                        ax_img.imshow(vis_gray_np[i], cmap='gray',vmin=0,vmax=1, interpolation='nearest'); ax_img.set_title(f\"{s_name} G\", fontsize=7); ax_img.axis('off')\n",
        "                        ax_bin.imshow(vis_binary_np[i], cmap='gray',vmin=0,vmax=1, interpolation='nearest'); ax_bin.set_title(f\"Bin\", fontsize=7); ax_bin.axis('off')\n",
        "                    plt.tight_layout(rect=[0,0,1,0.90]); plt.savefig(VISUALIZATION_DIR / f\"epoch_{(epoch+1):03d}_samples.png\"); plt.show()\n",
        "        # log image-based metrics for the epoch\n",
        "        for key_base in EMERGENT_METRIC_KEYS:\n",
        "            if not key_base.startswith(\"w_update_\"): # weight updates already logged\n",
        "                 metric_key_hist = f'metric_{key_base}'\n",
        "                 if metric_key_hist in training_history:\n",
        "                     training_history[metric_key_hist].append(current_epoch_img_metrics.get(key_base, 0.0)) # default to 0.0 if metric not computed\n",
        "\n",
        "        # early stopping check\n",
        "        if epochs_since_last_improvement >= early_stopping_patience:\n",
        "            print(f\"\\nearly stopping at epoch {epoch+1} due to no val_loss improvement for {early_stopping_patience} epochs.\")\n",
        "            break\n",
        "        gc.collect()\n",
        "        if DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n--- training complete (test_mode: {TEST_MODE}) ---\")\n",
        "    print(f\"total training duration: {(time.time()-overall_start_time)/60:.2f} min\")\n",
        "    print(f\"best validation loss: {best_val_loss:.5f}\")\n",
        "    try:\n",
        "        TRAINING_HISTORY_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "        history_to_save = convert_numpy_types_to_native(training_history) # ensure json serializable\n",
        "        with open(TRAINING_HISTORY_FILE, 'w') as f_hist: json.dump(history_to_save, f_hist, indent=2)\n",
        "        print(f\"training history saved to {TRAINING_HISTORY_FILE.resolve()}\")\n",
        "    except Exception as e_hist_save: print(f\"error saving training history: {e_hist_save}\")\n",
        "\n",
        "print(f\"\\n--- cell 3 training engine complete (test_mode: {TEST_MODE}) ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaFbFp6Xmfuu"
      },
      "outputs": [],
      "source": [
        "# cell 4: plot learning curves & emergent metrics\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt # make sure it's imported, cell 0 should have it\n",
        "from pathlib import Path\n",
        "import numpy as np # for nan handling if any\n",
        "\n",
        "# ensure matplotlib uses a non-interactive backend if not in a typical notebook environment\n",
        "# this might have been set in the ultimate video generator cell if run before, but good to be defensive\n",
        "try:\n",
        "    if 'get_ipython' not in globals() or get_ipython() is None: # if not in ipython/jupyter\n",
        "        plt.switch_backend('Agg')\n",
        "except Exception as e_backend:\n",
        "    print(f\"note: could not switch matplotlib backend: {e_backend}\")\n",
        "\n",
        "\n",
        "print(f\"--- plotting engine (test_mode: {TEST_MODE}) ---\")\n",
        "\n",
        "# configuration from cell 0 (paths, figure names, metric keys)\n",
        "# _TRAINING_HISTORY_FILE, _VISUALIZATION_DIR, _FIGURE_SAVE_NAME_MAIN_PLOT,\n",
        "# _FIGURE_SAVE_NAME_METRICS_PLOT, _EMERGENT_METRIC_KEYS global list (now includes w_updates)\n",
        "# TEST_MODE, CURRENT_DIFFUSERS_VERSION_STR for titles\n",
        "\n",
        "# --- 1. load training history ---\n",
        "loaded_history_for_plotting = None\n",
        "# prioritize in-memory history from cell 3 run\n",
        "if 'training_history' in globals() and isinstance(training_history, dict) and training_history.get('epoch'):\n",
        "    print(\"using 'training_history' from memory for plotting.\")\n",
        "    loaded_history_for_plotting = training_history\n",
        "elif TRAINING_HISTORY_FILE.exists():\n",
        "    print(f\"loading training history from: {TRAINING_HISTORY_FILE}\")\n",
        "    try:\n",
        "        with open(TRAINING_HISTORY_FILE, 'r') as f_hist_plot:\n",
        "            loaded_history_for_plotting = json.load(f_hist_plot)\n",
        "        print(\"training history loaded successfully from file for plotting.\")\n",
        "    except Exception as e_load_plot:\n",
        "        print(f\"error: failed to load/parse training history from file: {e_load_plot}\")\n",
        "else:\n",
        "    print(f\"error: training history file not found at {TRAINING_HISTORY_FILE} and not in memory.\")\n",
        "\n",
        "# --- 2. plotting logic ---\n",
        "if loaded_history_for_plotting and loaded_history_for_plotting.get('epoch') and len(loaded_history_for_plotting['epoch']) > 0:\n",
        "    epochs_recorded_plot = loaded_history_for_plotting['epoch']\n",
        "    num_epochs_recorded_plot = len(epochs_recorded_plot)\n",
        "    print(f\"\\nplotting learning curves & metrics for {num_epochs_recorded_plot} recorded epochs...\")\n",
        "\n",
        "    # helper to safely get data from history dict\n",
        "    def get_plot_data(history_dict, key_name, expected_len, default_val=np.nan):\n",
        "        data = history_dict.get(key_name)\n",
        "        if data is None or len(data) != expected_len:\n",
        "            # pad with default_val if data is shorter (e.g. metric added mid-training, or w_update for epoch 0)\n",
        "            if data is not None and len(data) < expected_len:\n",
        "                 print(f\"warning: data for '{key_name}' (len {len(data)}) shorter than expected ({expected_len}). padding with {default_val}.\")\n",
        "                 return list(data) + [default_val] * (expected_len - len(data))\n",
        "            print(f\"warning: data for '{key_name}' missing or length mismatch. using defaults ({default_val}).\")\n",
        "            return [default_val] * expected_len\n",
        "        # ensure numeric, replace Nones with np.nan\n",
        "        return [x if isinstance(x, (int, float)) else np.nan for x in data]\n",
        "\n",
        "    # --- 2.1: plot main learning curves (loss & lr) ---\n",
        "    fig_main_plot, ax1_main_plot = plt.subplots(figsize=(12, 7)) # slightly wider\n",
        "    title_main_plot = f'Ultra-Lofi 16x16 Training (Diffusers: {CURRENT_DIFFUSERS_VERSION_STR.replace(\"_\",\".\")}, Test: {TEST_MODE})'\n",
        "\n",
        "    color_train_loss = 'tab:blue'\n",
        "    ax1_main_plot.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1_main_plot.set_ylabel('Loss (MSE)', color=color_train_loss, fontsize=12)\n",
        "    train_loss_data = get_plot_data(loaded_history_for_plotting, 'train_loss', num_epochs_recorded_plot)\n",
        "    val_loss_data = get_plot_data(loaded_history_for_plotting, 'val_loss', num_epochs_recorded_plot)\n",
        "    ax1_main_plot.plot(epochs_recorded_plot, train_loss_data, color=color_train_loss, linestyle='-', marker='o', markersize=3, label='Train Loss')\n",
        "    ax1_main_plot.plot(epochs_recorded_plot, val_loss_data, color='tab:orange', linestyle='--', marker='x', markersize=3, label='Validation Loss')\n",
        "    ax1_main_plot.tick_params(axis='y', labelcolor=color_train_loss, labelsize=10)\n",
        "    ax1_main_plot.tick_params(axis='x', labelsize=10)\n",
        "    ax1_main_plot.grid(True, linestyle=':', alpha=0.6)\n",
        "\n",
        "    ax2_main_plot = ax1_main_plot.twinx() # for learning rate\n",
        "    color_lr_plot = 'tab:green'\n",
        "    lr_data = get_plot_data(loaded_history_for_plotting, 'learning_rate', num_epochs_recorded_plot)\n",
        "    ax2_main_plot.set_ylabel('Learning Rate', color=color_lr_plot, fontsize=12)\n",
        "    ax2_main_plot.plot(epochs_recorded_plot, lr_data, color=color_lr_plot, linestyle=':', marker='.', markersize=3, label='Learning Rate')\n",
        "    ax2_main_plot.tick_params(axis='y', labelcolor=color_lr_plot, labelsize=10)\n",
        "    ax2_main_plot.ticklabel_format(style='sci', axis='y', scilimits=(-2,3), useMathText=True)\n",
        "\n",
        "    # combined legend for both axes\n",
        "    lines1, labels1 = ax1_main_plot.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2_main_plot.get_legend_handles_labels()\n",
        "    fig_main_plot.legend(lines1 + lines2, labels1 + labels2, loc='lower center', bbox_to_anchor=(0.5, -0.15), fancybox=True, ncol=3, fontsize=10)\n",
        "    fig_main_plot.suptitle(title_main_plot, fontsize=14, y=0.98)\n",
        "    fig_main_plot.tight_layout(rect=[0, 0.05, 1, 0.95]) # adjust for legend\n",
        "\n",
        "    main_lc_fig_path_plot = VISUALIZATION_DIR / _FIGURE_SAVE_NAME_MAIN_PLOT # from cell 0\n",
        "    try:\n",
        "        plt.savefig(main_lc_fig_path_plot, bbox_inches='tight')\n",
        "        print(f\"main learning curve plot saved to {main_lc_fig_path_plot.resolve()}\")\n",
        "    except Exception as e_lc_save_plot: print(f\"error saving main learning curve plot: {e_lc_save_plot}\")\n",
        "    plt.show()\n",
        "\n",
        "    # --- 2.2: plot emergent metrics ---\n",
        "    # EMERGENT_METRIC_KEYS is the global list, now including weight update keys\n",
        "    metric_keys_to_plot_actual = [key for key in EMERGENT_METRIC_KEYS if f'metric_{key}' in loaded_history_for_plotting]\n",
        "\n",
        "    if metric_keys_to_plot_actual:\n",
        "        num_metrics_plot = len(metric_keys_to_plot_actual)\n",
        "        # adjust grid: try to make it more square-ish or use more columns if many metrics\n",
        "        cols_plot = 3 if num_metrics_plot > 6 else 2 if num_metrics_plot > 1 else 1\n",
        "        if num_metrics_plot > 9: cols_plot = 4 # more columns for many metrics\n",
        "        rows_plot = (num_metrics_plot + cols_plot - 1) // cols_plot\n",
        "\n",
        "        fig_metrics_plot, axes_metrics_plot = plt.subplots(rows_plot, cols_plot, figsize=(cols_plot * 5, rows_plot * 3.5), squeeze=False)\n",
        "        axes_metrics_flat_plot = axes_metrics_plot.flatten()\n",
        "        title_metrics_plot = f'Ultra-Lofi 16x16 Emergent Metrics (Test: {TEST_MODE})'\n",
        "\n",
        "        for i, metric_key_base in enumerate(metric_keys_to_plot_actual):\n",
        "            history_key = f'metric_{metric_key_base}' # key as stored in training_history\n",
        "            ax_curr = axes_metrics_flat_plot[i]\n",
        "            metric_values = get_plot_data(loaded_history_for_plotting, history_key, num_epochs_recorded_plot)\n",
        "\n",
        "            # a bit of specific handling for weight updates for the first epoch (can be NaN/0)\n",
        "            if metric_key_base.startswith(\"w_update_\") and num_epochs_recorded_plot > 0:\n",
        "                # if first value is NaN (no prev epoch), plot from 2nd epoch if available\n",
        "                if len(epochs_recorded_plot) > 1 and np.isnan(metric_values[0]):\n",
        "                    ax_curr.plot(epochs_recorded_plot[1:], metric_values[1:], linestyle='-', marker='.', markersize=4, label=metric_key_base)\n",
        "                else:\n",
        "                    ax_curr.plot(epochs_recorded_plot, metric_values, linestyle='-', marker='.', markersize=4, label=metric_key_base)\n",
        "            else:\n",
        "                ax_curr.plot(epochs_recorded_plot, metric_values, linestyle='-', marker='.', markersize=4, label=metric_key_base)\n",
        "\n",
        "            # make titles more readable\n",
        "            display_title = metric_key_base.replace('metric_', '').replace('_', ' ').title()\n",
        "            if metric_key_base.startswith(\"w_update_\"):\n",
        "                display_title = \"W-Upd: \" + metric_key_base.replace('w_update_', '').replace('_', ' ').title()\n",
        "\n",
        "            ax_curr.set_title(display_title[:30], fontsize=9) # truncate long titles\n",
        "            ax_curr.set_xlabel('Epoch', fontsize=8)\n",
        "            ax_curr.set_ylabel('Value', fontsize=8)\n",
        "            ax_curr.tick_params(axis='both', which='major', labelsize=7)\n",
        "            ax_curr.grid(True, linestyle=':', alpha=0.5)\n",
        "            # ax_curr.legend(fontsize=7) # legend per subplot can be too cluttered\n",
        "\n",
        "        for j in range(num_metrics_plot, len(axes_metrics_flat_plot)): # hide unused subplots\n",
        "            axes_metrics_flat_plot[j].set_visible(False)\n",
        "\n",
        "        fig_metrics_plot.suptitle(title_metrics_plot, fontsize=14, y=0.99)\n",
        "        fig_metrics_plot.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n",
        "        metrics_fig_path_plot = VISUALIZATION_DIR / _FIGURE_SAVE_NAME_METRICS_PLOT # from cell 0\n",
        "        try:\n",
        "            plt.savefig(metrics_fig_path_plot, bbox_inches='tight')\n",
        "            print(f\"emergent metrics plot saved to {metrics_fig_path_plot.resolve()}\")\n",
        "        except Exception as e_metrics_save_plot: print(f\"error saving emergent metrics plot: {e_metrics_save_plot}\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"no emergent metric keys (or their data) found in history for plotting.\")\n",
        "else:\n",
        "    print(\"no valid training history found to plot, or history is empty.\")\n",
        "\n",
        "print(f\"\\n--- cell 4 plotting complete (test_mode: {TEST_MODE}) ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H5rPquomf4b"
      },
      "outputs": [],
      "source": [
        "# cell 5: experiment results archiving & summary\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import shutil # for creating archives\n",
        "from pathlib import Path\n",
        "import time\n",
        "import os # for listing files (though Path.glob is often preferred)\n",
        "\n",
        "print(f\"--- results archiving process (test_mode: {TEST_MODE}) ---\")\n",
        "\n",
        "\n",
        "# determine epochs completed and best val loss\n",
        "if 'best_val_loss' in globals(): # from cell 3\n",
        "    _BEST_VAL_LOSS_ARCHIVE = best_val_loss\n",
        "else: _BEST_VAL_LOSS_ARCHIVE = float('nan')\n",
        "\n",
        "if 'epochs_recorded_plot' in globals(): # from cell 4\n",
        "    _EPOCHS_COMPLETED_ARCHIVE = epochs_recorded_plot[-1] if epochs_recorded_plot else 0\n",
        "elif 'training_history' in globals() and training_history.get('epoch'): # from cell 3\n",
        "    _EPOCHS_COMPLETED_ARCHIVE = training_history['epoch'][-1] if training_history['epoch'] else 0\n",
        "else: _EPOCHS_COMPLETED_ARCHIVE = 0\n",
        "\n",
        "# CORRECTED LINE: Use BASE_SAVE_DIR from Cell 0\n",
        "_ARCHIVE_SUBDIR = BASE_SAVE_DIR / 'archives_experiment' # distinct from video archives\n",
        "_ARCHIVE_SUBDIR.mkdir(parents=True, exist_ok=True)\n",
        "_ARCHIVE_TIMESTAMP_STR = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "# use _EXPERIMENT_TAG_NAME from cell 0 for archive base name\n",
        "_ARCHIVE_NAME_BASE_STR = f\"{_EXPERIMENT_TAG_NAME}{'_testmode' if TEST_MODE else ''}_{_ARCHIVE_TIMESTAMP_STR}\"\n",
        "\n",
        "print(f\"  archive base name: {_ARCHIVE_NAME_BASE_STR}\")\n",
        "print(f\"  archive will be saved in: {_ARCHIVE_SUBDIR.resolve()}\")\n",
        "\n",
        "# --- 1. load training history (if not in memory) ---\n",
        "loaded_history_for_archive = None\n",
        "if 'training_history' in globals() and isinstance(training_history, dict) and training_history.get('epoch'):\n",
        "    print(\"using 'training_history' from memory for archiving.\")\n",
        "    loaded_history_for_archive = training_history\n",
        "elif TRAINING_HISTORY_FILE.exists():\n",
        "    print(f\"loading training history from: {TRAINING_HISTORY_FILE}\")\n",
        "    try:\n",
        "        with open(TRAINING_HISTORY_FILE, 'r') as f_hist_arch:\n",
        "            loaded_history_for_archive = json.load(f_hist_arch)\n",
        "        print(\"training history loaded from file for archiving.\")\n",
        "    except Exception as e_load_arch:\n",
        "        print(f\"error: failed to load training history for archiving: {e_load_arch}\")\n",
        "else:\n",
        "    print(f\"warning: training history file not found at {TRAINING_HISTORY_FILE} and not in memory.\")\n",
        "\n",
        "# --- 2. export training history to csv ---\n",
        "csv_file_path_archive = None\n",
        "if loaded_history_for_archive and loaded_history_for_archive.get('epoch'):\n",
        "    try:\n",
        "        # ensure all lists in history are same length for dataframe creation by padding\n",
        "        max_len = len(loaded_history_for_archive['epoch'])\n",
        "        df_data = {}\n",
        "        for key, values in loaded_history_for_archive.items():\n",
        "            if isinstance(values, list):\n",
        "                if len(values) < max_len: # pad if shorter (e.g. weight updates start from epoch 2)\n",
        "                    df_data[key] = values + [np.nan] * (max_len - len(values))\n",
        "                else:\n",
        "                    df_data[key] = values[:max_len] # truncate if somehow longer (should not happen)\n",
        "            else: # non-list items (should not be common at top level)\n",
        "                df_data[key] = [values] * max_len\n",
        "\n",
        "        history_df_archive = pd.DataFrame(df_data)\n",
        "        csv_file_path_archive = _ARCHIVE_SUBDIR / f\"{_ARCHIVE_NAME_BASE_STR}_training_history.csv\"\n",
        "        history_df_archive.to_csv(csv_file_path_archive, index=False)\n",
        "        print(f\"training history exported to csv: {csv_file_path_archive.resolve()}\")\n",
        "    except Exception as e_csv_archive:\n",
        "        print(f\"error exporting training history to csv: {e_csv_archive}\")\n",
        "else:\n",
        "    print(\"no training history available to export to csv.\")\n",
        "\n",
        "# --- 3. create a summary report file (markdown) ---\n",
        "# use global variables from cell 0 for configuration details\n",
        "summary_content_list = [\n",
        "    f\"# Experiment Run Summary: {_EXPERIMENT_TAG_NAME}\",\n",
        "    f\"\\n**Timestamp:** {_ARCHIVE_TIMESTAMP_STR}\",\n",
        "    f\"**Test Mode:** {TEST_MODE}\",\n",
        "    \"\\n## Configuration Overview\",\n",
        "    f\"- **Base Save Directory:** `{BASE_SAVE_DIR.resolve()}`\",\n",
        "    f\"- **Image Size:** `{IMAGE_SIZE}`\",\n",
        "    f\"- **Rendering:** `{SHARPNESS_TARGET}`\",\n",
        "    f\"- **Diffusers Version (str):** `{CURRENT_DIFFUSERS_VERSION_STR.replace('_','.') if DIFFUSERS_IMPORT_SUCCESS else 'Failed/Unknown'}`\",\n",
        "    f\"- **Dataset Config File:** `{DATASET_CONFIG_FILE.resolve() if DATASET_CONFIG_FILE.exists() else 'Not found'}`\",\n",
        "    \"\\n## Training Outcome\",\n",
        "    f\"- **Epochs Completed:** `{_EPOCHS_COMPLETED_ARCHIVE}`\",\n",
        "    f\"- **Best Validation Loss:** {f'{_BEST_VAL_LOSS_ARCHIVE:.5f}' if isinstance(_BEST_VAL_LOSS_ARCHIVE, float) and not np.isnan(_BEST_VAL_LOSS_ARCHIVE) else 'N/A'}\",\n",
        "    f\"- **Model Checkpoint:** `{MODEL_CHECKPOINT_FILE.resolve() if MODEL_CHECKPOINT_FILE.exists() else 'Not found'}`\",\n",
        "    f\"- **Training History (JSON):** `{TRAINING_HISTORY_FILE.resolve() if TRAINING_HISTORY_FILE.exists() else 'Not found'}`\",\n",
        "    f\"- **Training History (CSV):** `{csv_file_path_archive.resolve() if csv_file_path_archive and csv_file_path_archive.exists() else 'Not generated or not found'}`\",\n",
        "    \"\\n## Key Visualizations\",\n",
        "    f\"- **Learning Curves Plot:** `{ (VISUALIZATION_DIR / _FIGURE_SAVE_NAME_MAIN_PLOT).resolve() if (VISUALIZATION_DIR / _FIGURE_SAVE_NAME_MAIN_PLOT).exists() else 'Not found'}`\",\n",
        "    f\"- **Emergent Metrics Plot:** `{ (VISUALIZATION_DIR / _FIGURE_SAVE_NAME_METRICS_PLOT).resolve() if (VISUALIZATION_DIR / _FIGURE_SAVE_NAME_METRICS_PLOT).exists() else 'Not found'}`\",\n",
        "    f\"- **Epoch Sample Visualizations:** In `{VISUALIZATION_DIR.resolve()}` (e.g., `epoch_XXX_samples.png`).\",\n",
        "    \"\\n## Emergent Metrics Tracked\",\n",
        "    f\"{', '.join(EMERGENT_METRIC_KEYS) if EMERGENT_METRIC_KEYS else 'None'}\", # uses global list\n",
        "    \"\\n## Notes\",\n",
        "    \"(Add any manual notes about this specific run here if needed)\",\n",
        "    \"\\n---\\nGenerated by Experiment Archiver Cell\"\n",
        "]\n",
        "summary_report_content_md = \"\\n\".join(summary_content_list)\n",
        "summary_report_path_md = _ARCHIVE_SUBDIR / f\"{_ARCHIVE_NAME_BASE_STR}_summary.md\"\n",
        "try:\n",
        "    with open(summary_report_path_md, 'w') as f_summary_md: f_summary_md.write(summary_report_content_md)\n",
        "    print(f\"experiment summary report saved to: {summary_report_path_md.resolve()}\")\n",
        "except Exception as e_summary_md: print(f\"error saving summary report: {e_summary_md}\")\n",
        "\n",
        "# --- 4. create a zip archive of key results ---\n",
        "files_to_archive_map = {\n",
        "    \"dataset_config.yaml\": DATASET_CONFIG_FILE, # from cell 0\n",
        "    \"training_history.json\": TRAINING_HISTORY_FILE, # from cell 0\n",
        "    \"training_history.csv\": csv_file_path_archive, # generated above\n",
        "    \"model_checkpoint.pth\": MODEL_CHECKPOINT_FILE, # from cell 0\n",
        "    \"summary_report.md\": summary_report_path_md, # generated above\n",
        "    \"plot_learning_curves.png\": (VISUALIZATION_DIR / _FIGURE_SAVE_NAME_MAIN_PLOT), # cell 0 vars\n",
        "    \"plot_emergent_metrics.png\": (VISUALIZATION_DIR / _FIGURE_SAVE_NAME_METRICS_PLOT), # cell 0 vars\n",
        "}\n",
        "\n",
        "latest_epoch_vis_file_path = None\n",
        "if VISUALIZATION_DIR and VISUALIZATION_DIR.exists():\n",
        "    # find latest epoch_XXX_samples.png\n",
        "    vis_files = sorted([f for f in VISUALIZATION_DIR.glob(f\"epoch_*_samples.png\")], reverse=True)\n",
        "    if vis_files:\n",
        "        latest_epoch_vis_file_path = vis_files[0]\n",
        "        files_to_archive_map[f\"latest_epoch_samples_{latest_epoch_vis_file_path.name}\"] = latest_epoch_vis_file_path\n",
        "\n",
        "archive_zip_target_path = _ARCHIVE_SUBDIR / _ARCHIVE_NAME_BASE_STR # shutil adds .zip\n",
        "temp_archive_content_path = _ARCHIVE_SUBDIR / f\"{_ARCHIVE_NAME_BASE_STR}_temp_contents\"\n",
        "temp_archive_content_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\npreparing files for archive...\")\n",
        "prepared_count = 0\n",
        "for dest_filename_in_zip, source_path_obj in files_to_archive_map.items():\n",
        "    if source_path_obj and Path(source_path_obj).exists():\n",
        "        try:\n",
        "            shutil.copy(Path(source_path_obj), temp_archive_content_path / dest_filename_in_zip)\n",
        "            prepared_count +=1\n",
        "        except Exception as e_copy_zip: print(f\"  error copying {source_path_obj} for archive: {e_copy_zip}\")\n",
        "    elif source_path_obj: print(f\"  warning: source file for '{dest_filename_in_zip}' ({source_path_obj}) not found.\")\n",
        "\n",
        "if prepared_count > 0:\n",
        "    print(f\"\\ncreating zip archive: {archive_zip_target_path.with_suffix('.zip').resolve()}\")\n",
        "    try:\n",
        "        shutil.make_archive(\n",
        "            base_name=str(archive_zip_target_path), # path to output file, minus extension\n",
        "            format='zip',                         # archive format\n",
        "            root_dir=temp_archive_content_path.parent, # dir to make paths relative to\n",
        "            base_dir=temp_archive_content_path.name    # dir to zip\n",
        "        )\n",
        "        print(\"zip archive created successfully.\")\n",
        "        shutil.rmtree(temp_archive_content_path) # clean up temp dir\n",
        "        print(f\"temporary archive content directory '{temp_archive_content_path.name}' removed.\")\n",
        "    except Exception as e_zip_archive: print(f\"error creating zip archive: {e_zip_archive}\")\n",
        "else:\n",
        "    print(\"no files were successfully prepared. zip archive not created.\")\n",
        "    if temp_archive_content_path.exists(): shutil.rmtree(temp_archive_content_path) # clean up if empty\n",
        "\n",
        "print(f\"\\n--- cell 5 archiving process complete (test_mode: {TEST_MODE}) ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69Voq9BbmgDI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_c0UneUmgM7"
      },
      "outputs": [],
      "source": [
        "# cell 6: ultimate epoch progression & inflection story video generator\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd # for rolling mean in metric analysis\n",
        "from pathlib import Path\n",
        "import re\n",
        "# tqdm, time, json should be available from cell 0 or other previous cells\n",
        "try: from tqdm.notebook import tqdm\n",
        "except ImportError: tqdm = lambda x, **kwargs: x # simple fallback\n",
        "\n",
        "# ensure matplotlib is available and set to a non-interactive backend\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # use non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"--- ultimate video generator (test_mode: {TEST_MODE}) ---\")\n",
        "\n",
        "# --- 0. SCRIPT CONFIGURATION & TUNABLE PARAMETERS ---\n",
        "VIDEO_ENABLE_ULTIMATE = True\n",
        "VIDEO_FILENAME_PREFIX = \"UltraLofi_EmergenceStory\" # will be part of final filename\n",
        "VIDEO_WIDTH = 720\n",
        "VIDEO_HEIGHT = 1280 # portrait orientation\n",
        "FPS = 10.0\n",
        "PAUSE_DURATION_SECONDS = 2.5 # how long to pause at inflection events\n",
        "\n",
        "# layout proportions for video sections\n",
        "LAYOUT_EPOCH_IMG_RATIO = 0.38 # top section for epoch sample images\n",
        "LAYOUT_LC_CHART_RATIO = 0.28  # middle section for static learning curve chart\n",
        "LAYOUT_METRICS_CHART_RATIO = 0.34 # bottom section for dynamic emergent metrics chart\n",
        "\n",
        "# custom emergent metrics chart appearance (dynamic matplotlib chart)\n",
        "# metrics to plot: keys from EMERGENT_METRIC_KEYS (Cell 0/3), labels, colors\n",
        "# ensure these keys exist in your training_history file!\n",
        "METRICS_TO_PLOT_CONFIG = {\n",
        "    # image-based metrics\n",
        "    'metric_shannon_entropy_global': {'label': 'Global Entropy', 'color': '#1f77b4'},\n",
        "    'metric_local_entropy_mean':    {'label': 'Local Entropy Mean', 'color': '#aec7e8'},\n",
        "    'metric_local_entropy_variance':{'label': 'Local Entropy Var', 'color': '#ff7f0e'},\n",
        "    'metric_IoU_with_gt_mean':      {'label': 'Mean IoU w/ GT',   'color': '#2ca02c'},\n",
        "    'metric_num_clusters_binary':   {'label': 'Num Clusters',    'color': '#98df8a'},\n",
        "    'metric_noise_pixels_binary':   {'label': 'Noise Pixels',    'color': '#d62728'},\n",
        "    'metric_edge_sharpness_sobel':  {'label': 'Edge Sharpness',  'color': '#ff9896'},\n",
        "    'metric_output_stability_ssim': {'label': 'Stability (SSIM)','color': '#9467bd'},\n",
        "    # add a few example weight update metrics (adjust based on your actual tracked layers)\n",
        "    # the keys here MUST match what's generated in cell 3 for training_history\n",
        "    # (e.g., metric_w_update_conv_in_weight)\n",
        "    # replace these with 1-2 of your *actual* tracked weight keys from ENHANCED_METRIC_CONFIG\n",
        "    f\"metric_w_update_{ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][0].replace('.', '_') if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] else 'dummy_w_update1'}\":\n",
        "                                    {'label': f\"W-Upd: {ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][0].split('.')[0] if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] else 'Layer1'}\", 'color': '#8c564b'},\n",
        "    f\"metric_w_update_{ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][-1].replace('.', '_') if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] and len(ENHANCED_METRIC_CONFIG['tracked_layers_for_weights']) > 1 else 'dummy_w_update_last'}\":\n",
        "                                    {'label': f\"W-Upd: {ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][-1].split('.')[0] if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] and len(ENHANCED_METRIC_CONFIG['tracked_layers_for_weights']) > 1 else 'LayerLast'}\", 'color': '#c49c94'},\n",
        "}\n",
        "METRICS_CHART_DPI = 90\n",
        "METRICS_CHART_BG_COLOR = 'white'\n",
        "METRICS_CHART_GRID_COLOR = 'lightgray'\n",
        "METRICS_CHART_LINE_WIDTH = 1.8\n",
        "METRICS_CHART_X_LABEL = \"Epoch\"\n",
        "METRICS_CHART_Y_LABEL = \"Normalized Value (0-100)\" # since we normalize for plotting\n",
        "METRICS_CHART_TITLE = \"Key Emergent Metrics Progression\"\n",
        "METRICS_CHART_LEGEND_LOC = 'upper center'\n",
        "METRICS_CHART_LEGEND_NCOL = 2\n",
        "METRICS_CHART_PROGRESS_LINE_COLOR = 'red' # sweeping line on this chart\n",
        "METRICS_CHART_PROGRESS_LINE_WIDTH = 2.0\n",
        "\n",
        "# inflection event detection parameters (for metrics in METRICS_TO_PLOT_CONFIG)\n",
        "EVENT_DETECTION_CONFIG = {\n",
        "    'metric_IoU_with_gt_mean':      {'goal': 'maximize', 'threshold_factor': 0.90, 'smoothing': 7, 'stability_win': 5, 'tolerance': 0.03, 'event_label': \"IoU Peak\"},\n",
        "    'metric_noise_pixels_binary':   {'goal': 'minimize', 'target_val_override': 0, 'smoothing': 7, 'stability_win': 7, 'tolerance': 1, 'event_label': \"Noise Cleared\"}, # tolerance is absolute if target_val_override is 0\n",
        "    'metric_num_clusters_binary':   {'goal': 'minimize', 'target_val_override': 1, 'smoothing': 7, 'stability_win': 10, 'tolerance': 0.1, 'event_label': \"Object Coherence\"},\n",
        "    'metric_output_stability_ssim': {'goal': 'maximize', 'threshold_factor': 0.75, 'smoothing': 10, 'stability_win': 7, 'tolerance': 0.02, 'event_label': \"Output Stable\"},\n",
        "    'metric_local_entropy_variance':{'goal': 'stabilize', 'smoothing': 10, 'stability_win': 7, 'tolerance': 0.03, 'event_label': \"Structure Settled\"},\n",
        "    f\"metric_w_update_{ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][-1].replace('.', '_') if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] and len(ENHANCED_METRIC_CONFIG['tracked_layers_for_weights']) > 1 else 'dummy_w_update_last'}\":\n",
        "                                    {'goal': 'minimize', 'threshold_factor': 1.0, 'smoothing': 10, 'stability_win': 6, 'tolerance': 0.1, 'event_label': f\"{ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][-1].split('.')[0] if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] and len(ENHANCED_METRIC_CONFIG['tracked_layers_for_weights']) > 1 else 'LayerLast'} Weights Stable\"},\n",
        "}\n",
        "\n",
        "# pause text label appearance\n",
        "PAUSE_TEXT_FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
        "PAUSE_TEXT_SCALE = 0.8\n",
        "PAUSE_TEXT_COLOR = (255, 255, 255) # white\n",
        "PAUSE_TEXT_THICKNESS = 2\n",
        "PAUSE_TEXT_BG_COLOR = (0, 0, 0) # black\n",
        "PAUSE_TEXT_BG_ALPHA = 0.7\n",
        "PAUSE_TEXT_Y_POS_RATIO = 0.05 # from top of video frame\n",
        "PAUSE_TEXT_PADDING = 15\n",
        "\n",
        "# learning curve chart (pre-rendered) progress line calibration\n",
        "LC_CHART_LINE_COLOR = (0, 0, 255) # blue line on LC chart\n",
        "LC_CHART_LINE_THICKNESS = 2\n",
        "# (x_start, width, y_start, height) relative to LC chart section. MUST CALIBRATE!\n",
        "LC_PLOT_CALIB = {'x_plot_start_rel': 60, 'plot_width_rel': VIDEO_WIDTH - 120,\n",
        "                 'y_line_start_rel': 40, 'line_height_rel': int(VIDEO_HEIGHT * LAYOUT_LC_CHART_RATIO) - 70}\n",
        "\n",
        "# --- 1. Initialization and Path Setup ---\n",
        "# using globals from cell 0: BASE_SAVE_DIR, VISUALIZATION_DIR, TRAINING_HISTORY_FILE,\n",
        "# _FIGURE_SAVE_NAME_MAIN_PLOT, _EXPERIMENT_TAG_NAME, TEST_MODE\n",
        "NUM_PAUSE_FRAMES = int(FPS * PAUSE_DURATION_SECONDS)\n",
        "\n",
        "_VIDEO_TIMESTAMP_ULT = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "_VIDEO_FILENAME_ULT = f\"{VIDEO_FILENAME_PREFIX}_{_EXPERIMENT_TAG_NAME}{'_testmode' if TEST_MODE else ''}_{_VIDEO_TIMESTAMP_ULT}.mp4\"\n",
        "# VIDEO_OUTPUT_DIR is already defined in cell 0\n",
        "_VIDEO_FULL_PATH_ULT = VIDEO_OUTPUT_DIR / _VIDEO_FILENAME_ULT\n",
        "\n",
        "print(f\"  Ultimate Video Output Path: {_VIDEO_FULL_PATH_ULT}\")\n",
        "_H_EPOCH_IMG_SECTION_ULT = int(VIDEO_HEIGHT * LAYOUT_EPOCH_IMG_RATIO)\n",
        "_H_LC_CHART_SECTION_ULT = int(VIDEO_HEIGHT * LAYOUT_LC_CHART_RATIO)\n",
        "_H_METRICS_CHART_SECTION_ULT = VIDEO_HEIGHT - _H_EPOCH_IMG_SECTION_ULT - _H_LC_CHART_SECTION_ULT\n",
        "print(f\"  Layout (H_px): EpochImg:{_H_EPOCH_IMG_SECTION_ULT}, LCChart:{_H_LC_CHART_SECTION_ULT}, MetricsChart:{_H_METRICS_CHART_SECTION_ULT}\")\n",
        "\n",
        "# --- 2. Helper Functions (Normalization, Event Detection - adapted from your V3) ---\n",
        "def normalize_series_0_100_video(series_data):\n",
        "    min_val, max_val = series_data.min(), series_data.max()\n",
        "    return pd.Series(np.full_like(series_data, 50.0), index=series_data.index) if max_val == min_val else (series_data - min_val) / (max_val - min_val) * 100\n",
        "\n",
        "def detect_metric_trigger_events_video(history_dict, full_event_config, selected_metrics_config):\n",
        "    # (this function is complex, assuming it's similar to your v3, adapted slightly)\n",
        "    if not history_dict or not history_dict.get('epoch'): return []\n",
        "    num_epochs = len(history_dict['epoch'])\n",
        "    detected_events = []\n",
        "    for metric_key_hist, config in full_event_config.items(): # metric_key_hist is like 'metric_IoU_with_gt_mean'\n",
        "        if metric_key_hist not in selected_metrics_config: continue # only process metrics we intend to plot\n",
        "\n",
        "        metric_values = history_dict.get(metric_key_hist)\n",
        "        if not metric_values or len(metric_values) != num_epochs: continue\n",
        "\n",
        "        series = pd.Series(metric_values).fillna(method='bfill').fillna(method='ffill') # handle potential NaNs from padding\n",
        "        if series.isna().any() or len(series) < config['smoothing'] or len(series) < config['stability_win']: continue # not enough data\n",
        "\n",
        "        smoothed = series.rolling(window=config['smoothing'], center=True, min_periods=1).mean().fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "        # Pad smoothed series at the beginning if centered rolling creates NaNs there\n",
        "        # This ensures smoothed has same length as original for easier indexing\n",
        "        if smoothed.isna().any():\n",
        "            first_valid_idx = smoothed.first_valid_index()\n",
        "            if first_valid_idx is not None and first_valid_idx > 0:\n",
        "                smoothed.iloc[:first_valid_idx] = smoothed.iloc[first_valid_idx]\n",
        "\n",
        "        trigger_idx = -1\n",
        "        if config['goal'] == 'maximize':\n",
        "            target = smoothed.max() * config.get('threshold_factor', 0.95)\n",
        "            for i in range(num_epochs - config['stability_win']):\n",
        "                if smoothed.iloc[i] >= target:\n",
        "                    if (smoothed.iloc[i : i + config['stability_win']] >= target * (1-config['tolerance'])).all(): trigger_idx=i; break\n",
        "        elif config['goal'] == 'minimize':\n",
        "            override = config.get('target_val_override')\n",
        "            target_abs = override if override is not None else (smoothed.min() * config.get('threshold_factor', 1.05))\n",
        "            tol_abs = abs(target_abs * config['tolerance']) if target_abs != 0 else config['tolerance']\n",
        "            for i in range(num_epochs - config['stability_win']):\n",
        "                if abs(smoothed.iloc[i] - target_abs) <= tol_abs:\n",
        "                    if (abs(smoothed.iloc[i : i + config['stability_win']] - target_abs) <= tol_abs).all(): trigger_idx=i; break\n",
        "        elif config['goal'] == 'stabilize':\n",
        "            # use a very small absolute change threshold if std is zero (flat line)\n",
        "            stabilization_thresh = smoothed.std() * config['tolerance'] if smoothed.std() > 1e-6 else config['tolerance'] * 0.01\n",
        "            for i in range(config['smoothing'], num_epochs - config['stability_win']):\n",
        "                if smoothed.iloc[i : i + config['stability_win']].std() < stabilization_thresh : trigger_idx=i; break\n",
        "\n",
        "        if trigger_idx != -1:\n",
        "            detected_events.append({\n",
        "                'metric_key': metric_key_hist, 'display_name': config.get('event_label', selected_metrics_config[metric_key_hist]['label']),\n",
        "                'trigger_epoch_idx': trigger_idx, 'trigger_epoch_num': history_dict['epoch'][trigger_idx]\n",
        "            })\n",
        "    return sorted(detected_events, key=lambda e: e['trigger_epoch_idx'])\n",
        "\n",
        "\n",
        "# --- 3. Load Data, Pre-process Metrics, Detect Events ---\n",
        "CAN_RUN_ULT_VIDEO = True\n",
        "_HISTORY_DATA_ULT = None\n",
        "if 'training_history' in globals() and training_history.get('epoch'): # from cell 3\n",
        "    _HISTORY_DATA_ULT = training_history\n",
        "elif TRAINING_HISTORY_FILE.exists():\n",
        "    try:\n",
        "        with open(TRAINING_HISTORY_FILE, 'r') as f: _HISTORY_DATA_ULT = json.load(f)\n",
        "        print(f\"loaded training history from: {TRAINING_HISTORY_FILE}\")\n",
        "    except Exception as e: print(f\"error loading history: {e}\"); CAN_RUN_ULT_VIDEO = False\n",
        "else: print(f\"error: training history not found.\"); CAN_RUN_ULT_VIDEO = False\n",
        "\n",
        "normalized_metrics_data_ult = {}\n",
        "if CAN_RUN_ULT_VIDEO and _HISTORY_DATA_ULT:\n",
        "    for key_hist, conf_plot in METRICS_TO_PLOT_CONFIG.items(): # key_hist is like 'metric_shannon_entropy_global'\n",
        "        if key_hist in _HISTORY_DATA_ULT and _HISTORY_DATA_ULT[key_hist]: # ensure data exists and is not empty\n",
        "            # pad with NaNs if necessary, then normalize\n",
        "            num_hist_epochs = len(_HISTORY_DATA_ULT['epoch'])\n",
        "            metric_series_raw = _HISTORY_DATA_ULT[key_hist]\n",
        "            if len(metric_series_raw) < num_hist_epochs:\n",
        "                metric_series_raw.extend([np.nan] * (num_hist_epochs - len(metric_series_raw)))\n",
        "\n",
        "            series_pd = pd.Series(metric_series_raw).fillna(method='bfill').fillna(method='ffill')\n",
        "            if not series_pd.empty and not series_pd.isna().all():\n",
        "                 normalized_metrics_data_ult[key_hist] = normalize_series_0_100_video(series_pd)\n",
        "            else: # if series ends up all NaN or empty after fill\n",
        "                 normalized_metrics_data_ult[key_hist] = pd.Series(np.full(num_hist_epochs, 50.0)) # flat line\n",
        "        else: # metric key not in history or empty list\n",
        "            num_hist_epochs = len(_HISTORY_DATA_ULT.get('epoch', [0]))\n",
        "            normalized_metrics_data_ult[key_hist] = pd.Series(np.full(num_hist_epochs, 50.0))\n",
        "            print(f\"warning: data for '{key_hist}' not found or empty in history. using flat line.\")\n",
        "\n",
        "\n",
        "_TRIGGER_EVENTS_LIST_ULT = []\n",
        "if CAN_RUN_ULT_VIDEO and _HISTORY_DATA_ULT:\n",
        "    _TRIGGER_EVENTS_LIST_ULT = detect_metric_trigger_events_video(_HISTORY_DATA_ULT, EVENT_DETECTION_CONFIG, METRICS_TO_PLOT_CONFIG)\n",
        "    if not _TRIGGER_EVENTS_LIST_ULT: print(\"no metric trigger events detected.\")\n",
        "    else: print(f\"detected {_TRIGGER_EVENTS_LIST_ULT} metric trigger events.\")\n",
        "\n",
        "\n",
        "# --- 4. Load Static Learning Curve Chart ---\n",
        "_LC_CHART_FINAL_SIZE_ULT = None\n",
        "if CAN_RUN_ULT_VIDEO:\n",
        "    lc_path_ult = VISUALIZATION_DIR / _FIGURE_SAVE_NAME_MAIN_PLOT # from cell 0\n",
        "    if lc_path_ult.exists():\n",
        "        lc_orig_ult = cv2.imread(str(lc_path_ult))\n",
        "        if lc_orig_ult is not None:\n",
        "            _LC_CHART_FINAL_SIZE_ULT = cv2.resize(lc_orig_ult, (VIDEO_WIDTH, _H_LC_CHART_SECTION_ULT), interpolation=cv2.INTER_AREA)\n",
        "        else: print(f\"error: failed to read LC chart: {lc_path_ult}\"); CAN_RUN_ULT_VIDEO = False\n",
        "    else: print(f\"error: LC chart not found: {lc_path_ult}\"); CAN_RUN_ULT_VIDEO = False\n",
        "\n",
        "# --- 5. Gather Epoch Sample Images ---\n",
        "_EPOCH_IMAGE_PATHS_ULT, _NUM_VIDEO_FRAMES_ULT = [], 0\n",
        "if CAN_RUN_ULT_VIDEO:\n",
        "    img_pattern_ult = f\"epoch_*_samples.png\" # assumes test/non-test handled by VISUALIZATION_DIR content\n",
        "    def get_epoch_idx_from_path_ult(p_obj): # 0-indexed epoch\n",
        "        match = re.search(r\"epoch_(\\d+)_samples\", p_obj.name); return int(match.group(1)) - 1 if match else -1\n",
        "    raw_paths_ult = [p for p in VISUALIZATION_DIR.glob(img_pattern_ult) if get_epoch_idx_from_path_ult(p) != -1]\n",
        "    _EPOCH_IMAGE_PATHS_ULT = sorted(raw_paths_ult, key=get_epoch_idx_from_path_ult)\n",
        "    _NUM_VIDEO_FRAMES_ULT = len(_EPOCH_IMAGE_PATHS_ULT)\n",
        "    if _NUM_VIDEO_FRAMES_ULT == 0: print(f\"error: no epoch images found.\"); CAN_RUN_ULT_VIDEO = False\n",
        "    elif _HISTORY_DATA_ULT and len(_HISTORY_DATA_ULT.get('epoch',[])) != _NUM_VIDEO_FRAMES_ULT :\n",
        "        print(f\"warning: image count ({_NUM_VIDEO_FRAMES_ULT}) != history epochs ({len(_HISTORY_DATA_ULT.get('epoch',[]))}). video length based on images.\")\n",
        "        # May need to truncate or align history data to image count if this becomes an issue for plotting metric lines.\n",
        "        # For now, we proceed, and plotting will handle shorter history data for lines.\n",
        "\n",
        "# --- 6. Initialize VideoWriter ---\n",
        "_VIDEO_WRITER_ULT = None\n",
        "if CAN_RUN_ULT_VIDEO and _NUM_VIDEO_FRAMES_ULT > 0:\n",
        "    _VIDEO_WRITER_ULT = cv2.VideoWriter(str(_VIDEO_FULL_PATH_ULT), cv2.VideoWriter_fourcc(*'mp4v'), FPS, (VIDEO_WIDTH, VIDEO_HEIGHT))\n",
        "    if not _VIDEO_WRITER_ULT.isOpened(): print(f\"error: could not open VideoWriter.\"); CAN_RUN_ULT_VIDEO = False\n",
        "\n",
        "# --- 7. Main Video Generation Loop ---\n",
        "if CAN_RUN_ULT_VIDEO and _NUM_VIDEO_FRAMES_ULT > 0 and _VIDEO_WRITER_ULT and _VIDEO_WRITER_ULT.isOpened():\n",
        "    print(f\"\\nstarting ultimate video generation for {_NUM_VIDEO_FRAMES_ULT} base frames...\")\n",
        "    _processed_event_indices = set()\n",
        "    fig_w_in, fig_h_in = VIDEO_WIDTH / METRICS_CHART_DPI, _H_METRICS_CHART_SECTION_ULT / METRICS_CHART_DPI\n",
        "\n",
        "    for current_epoch_idx_video in tqdm(range(_NUM_VIDEO_FRAMES_ULT), desc=\"Generating Video Frames\"):\n",
        "        # a. load epoch sample image\n",
        "        epoch_img_path_video = _EPOCH_IMAGE_PATHS_ULT[current_epoch_idx_video]\n",
        "        epoch_raw_img_video = cv2.imread(str(epoch_img_path_video))\n",
        "        epoch_final_img = cv2.resize(epoch_raw_img_video, (VIDEO_WIDTH, _H_EPOCH_IMG_SECTION_ULT), interpolation=cv2.INTER_AREA) if epoch_raw_img_video is not None else np.zeros((_H_EPOCH_IMG_SECTION_ULT, VIDEO_WIDTH, 3), dtype=np.uint8)\n",
        "\n",
        "        # b. generate custom metrics chart frame\n",
        "        fig_met, ax_met = plt.subplots(figsize=(fig_w_in, fig_h_in), dpi=METRICS_CHART_DPI)\n",
        "        fig_met.patch.set_facecolor(METRICS_CHART_BG_COLOR); ax_met.set_facecolor(METRICS_CHART_BG_COLOR)\n",
        "\n",
        "        # x-axis for plotting should correspond to actual epoch numbers from history\n",
        "        history_epochs_axis = np.array(_HISTORY_DATA_ULT.get('epoch', range(1, _NUM_VIDEO_FRAMES_ULT + 1)))\n",
        "        # ensure history_epochs_axis is not longer than current frame count if history is longer\n",
        "        if len(history_epochs_axis) > _NUM_VIDEO_FRAMES_ULT: history_epochs_axis = history_epochs_axis[:_NUM_VIDEO_FRAMES_ULT]\n",
        "\n",
        "        # determine the x-axis limit for plotting dynamically\n",
        "        # this ensures the plot \"grows\" but the x-axis scale remains consistent based on total epochs\n",
        "        plot_xlim_max = history_epochs_axis[-1] if len(history_epochs_axis) > 0 else current_epoch_idx_video + 1\n",
        "\n",
        "        for key_hist_plot, config_plot in METRICS_TO_PLOT_CONFIG.items():\n",
        "            if key_hist_plot in normalized_metrics_data_ult:\n",
        "                norm_data = normalized_metrics_data_ult[key_hist_plot]\n",
        "                # ensure norm_data has same length as history_epochs_axis for plotting\n",
        "                # this might happen if a metric was added late or history/image counts mismatch\n",
        "                if len(norm_data) > len(history_epochs_axis): norm_data = norm_data[:len(history_epochs_axis)]\n",
        "                elif len(norm_data) < len(history_epochs_axis): # should not happen if padded correctly\n",
        "                    norm_data = pd.concat([norm_data, pd.Series([np.nan]*(len(history_epochs_axis) - len(norm_data)))], ignore_index=True)\n",
        "\n",
        "\n",
        "                plot_upto = current_epoch_idx_video\n",
        "                event_for_this_metric = next((e for e_idx, e in enumerate(_TRIGGER_EVENTS_LIST_ULT) if e['metric_key'] == key_hist_plot and e_idx in _processed_event_indices), None)\n",
        "                if event_for_this_metric: plot_upto = event_for_this_metric['trigger_epoch_idx'] # line stops at event\n",
        "\n",
        "                # ensure plot_upto is within bounds of available data\n",
        "                effective_plot_upto = min(plot_upto, len(history_epochs_axis) -1, len(norm_data)-1)\n",
        "\n",
        "                if effective_plot_upto >=0:\n",
        "                    x_data_plot = history_epochs_axis[:effective_plot_upto + 1]\n",
        "                    y_data_plot = norm_data.iloc[:effective_plot_upto + 1]\n",
        "                    if len(x_data_plot) == len(y_data_plot) and len(x_data_plot) > 0:\n",
        "                         ax_met.plot(x_data_plot, y_data_plot, label=config_plot['label'], color=config_plot['color'], linewidth=METRICS_CHART_LINE_WIDTH)\n",
        "\n",
        "        ax_met.set_title(METRICS_CHART_TITLE, fontsize=10); ax_met.set_xlabel(METRICS_CHART_X_LABEL, fontsize=8); ax_met.set_ylabel(METRICS_CHART_Y_LABEL, fontsize=8)\n",
        "        ax_met.set_ylim(0, 105); ax_met.set_xlim(history_epochs_axis[0] if len(history_epochs_axis)>0 else 1, plot_xlim_max)\n",
        "        ax_met.legend(loc=METRICS_CHART_LEGEND_LOC, fontsize=7, ncol=METRICS_CHART_LEGEND_NCOL); ax_met.grid(True, linestyle=':', color=METRICS_CHART_GRID_COLOR, alpha=0.7)\n",
        "        ax_met.tick_params(axis='both', which='major', labelsize=7)\n",
        "\n",
        "        # progress line on metrics chart (using current epoch number)\n",
        "        epoch_num_axvline = history_epochs_axis[min(current_epoch_idx_video, len(history_epochs_axis)-1)] if len(history_epochs_axis) > 0 else current_epoch_idx_video + 1\n",
        "        ax_met.axvline(x=epoch_num_axvline, color=METRICS_CHART_PROGRESS_LINE_COLOR, linestyle='-', linewidth=METRICS_CHART_PROGRESS_LINE_WIDTH, alpha=0.8)\n",
        "\n",
        "        fig_met.canvas.draw()\n",
        "        metrics_chart_img = cv2.cvtColor(np.array(fig_met.canvas.buffer_rgba()), cv2.COLOR_RGBA2BGR)\n",
        "        plt.close(fig_met); gc.collect()\n",
        "\n",
        "        # c. composite base video frame\n",
        "        base_frame = np.zeros((VIDEO_HEIGHT, VIDEO_WIDTH, 3), dtype=np.uint8)\n",
        "        y_off = 0\n",
        "        base_frame[y_off : y_off + _H_EPOCH_IMG_SECTION_ULT, :] = epoch_final_img; y_off += _H_EPOCH_IMG_SECTION_ULT\n",
        "        if _LC_CHART_FINAL_SIZE_ULT is not None: base_frame[y_off : y_off + _H_LC_CHART_SECTION_ULT, :] = _LC_CHART_FINAL_SIZE_ULT\n",
        "        y_off_lc_start = y_off; y_off += _H_LC_CHART_SECTION_ULT\n",
        "        # resize metrics_chart_img if its rendered size is slightly off target section dimensions\n",
        "        if metrics_chart_img.shape[0] != _H_METRICS_CHART_SECTION_ULT or metrics_chart_img.shape[1] != VIDEO_WIDTH:\n",
        "            metrics_chart_img = cv2.resize(metrics_chart_img, (VIDEO_WIDTH, _H_METRICS_CHART_SECTION_ULT), interpolation=cv2.INTER_AREA)\n",
        "        base_frame[y_off : y_off + _H_METRICS_CHART_SECTION_ULT, :] = metrics_chart_img\n",
        "\n",
        "        # progress line on LC chart\n",
        "        if _LC_CHART_FINAL_SIZE_ULT is not None:\n",
        "            lc_prog_ratio = current_epoch_idx_video / (_NUM_VIDEO_FRAMES_ULT - 1) if _NUM_VIDEO_FRAMES_ULT > 1 else 0.0\n",
        "            line_x_lc = LC_PLOT_CALIB['x_plot_start_rel'] + int(lc_prog_ratio * LC_PLOT_CALIB['plot_width_rel'])\n",
        "            line_x_lc = max(LC_PLOT_CALIB['x_plot_start_rel'], min(line_x_lc, LC_PLOT_CALIB['x_plot_start_rel'] + LC_PLOT_CALIB['plot_width_rel']))\n",
        "            y1_lc, y2_lc = y_off_lc_start + LC_PLOT_CALIB['y_line_start_rel'], y_off_lc_start + LC_PLOT_CALIB['y_line_start_rel'] + LC_PLOT_CALIB['line_height_rel']\n",
        "            cv2.line(base_frame, (line_x_lc, y1_lc), (line_x_lc, y2_lc), LC_CHART_LINE_COLOR, LC_CHART_LINE_THICKNESS)\n",
        "\n",
        "        # epoch text\n",
        "        epoch_num_disp = get_epoch_idx_from_path_ult(epoch_img_path_video) + 1 # 1-based\n",
        "        cv2.putText(base_frame, f\"Epoch: {epoch_num_disp}\", (15, _H_EPOCH_IMG_SECTION_ULT - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (220,220,220), 1, cv2.LINE_AA)\n",
        "\n",
        "        # d. handle pause events\n",
        "        pause_this_frame, active_event_names_list = False, []\n",
        "        for event_list_idx, event_dat in enumerate(_TRIGGER_EVENTS_LIST_ULT):\n",
        "            if event_dat['trigger_epoch_idx'] == current_epoch_idx_video and event_list_idx not in _processed_event_indices:\n",
        "                pause_this_frame = True\n",
        "                active_event_names_list.append(event_dat['display_name'])\n",
        "                _processed_event_indices.add(event_list_idx) # mark this *event definition* as processed for pause\n",
        "\n",
        "        if pause_this_frame:\n",
        "            pause_frame_ann = base_frame.copy()\n",
        "            pause_label = f\"EVENT: {', '.join(active_event_names_list)} (Epoch {epoch_num_disp})\"\n",
        "            (txt_w, txt_h), baseline = cv2.getTextSize(pause_label, PAUSE_TEXT_FONT, PAUSE_TEXT_SCALE, PAUSE_TEXT_THICKNESS)\n",
        "            banner_h = txt_h + baseline + 2 * PAUSE_TEXT_PADDING\n",
        "            banner_y = int(VIDEO_HEIGHT * PAUSE_TEXT_Y_POS_RATIO) - int(banner_h/2) # center banner\n",
        "            banner_x = int((VIDEO_WIDTH - (txt_w + 2 * PAUSE_TEXT_PADDING)) / 2) # center banner\n",
        "\n",
        "            text_x_orig = banner_x + PAUSE_TEXT_PADDING\n",
        "            text_y_orig = banner_y + PAUSE_TEXT_PADDING + txt_h # cv2 text origin is bottom-left\n",
        "\n",
        "            banner_overlay = pause_frame_ann.copy()\n",
        "            cv2.rectangle(banner_overlay, (banner_x, banner_y), (banner_x + txt_w + 2*PAUSE_TEXT_PADDING, banner_y + banner_h), PAUSE_TEXT_BG_COLOR, -1)\n",
        "            cv2.addWeighted(banner_overlay, PAUSE_TEXT_BG_ALPHA, pause_frame_ann, 1 - PAUSE_TEXT_BG_ALPHA, 0, pause_frame_ann)\n",
        "            cv2.putText(pause_frame_ann, pause_label, (text_x_orig, text_y_orig), PAUSE_TEXT_FONT, PAUSE_TEXT_SCALE, PAUSE_TEXT_COLOR, PAUSE_TEXT_THICKNESS, cv2.LINE_AA)\n",
        "            for _ in range(NUM_PAUSE_FRAMES): _VIDEO_WRITER_ULT.write(pause_frame_ann)\n",
        "        else:\n",
        "            _VIDEO_WRITER_ULT.write(base_frame)\n",
        "\n",
        "    print(\"ultimate video generation loop complete.\")\n",
        "\n",
        "# --- 8. Finalize ---\n",
        "if _VIDEO_WRITER_ULT is not None and _VIDEO_WRITER_ULT.isOpened():\n",
        "    _VIDEO_WRITER_ULT.release(); print(f\"ultimate video saved successfully to: {_VIDEO_FULL_PATH_ULT.resolve()}\")\n",
        "elif CAN_RUN_ULT_VIDEO and _NUM_VIDEO_FRAMES_ULT > 0 : print(\"video writer was not initialized or not open. video not saved.\")\n",
        "else: print(\"video generation skipped due to earlier errors or no frames.\")\n",
        "\n",
        "print(f\"\\n--- cell 6 ultimate video generator complete (test_mode: {TEST_MODE}) ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtfbJhLFnzOj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmiMPxlFmgYO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 7: emergent metric & weight update correlation heatmap generator\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # for heatmaps\n",
        "from pathlib import Path\n",
        "\n",
        "print(f\"--- correlation heatmap generator (test_mode: {TEST_MODE}) ---\")\n",
        "\n",
        "# configuration from cell 0:\n",
        "# TRAINING_HISTORY_FILE, VISUALIZATION_DIR, _EXPERIMENT_TAG_NAME, TEST_MODE\n",
        "# EMERGENT_METRIC_KEYS (global list, includes weight update keys)\n",
        "\n",
        "# --- 1. load training history ---\n",
        "loaded_history_for_heatmap = None\n",
        "if 'training_history' in globals() and isinstance(training_history, dict) and training_history.get('epoch'):\n",
        "    print(\"using 'training_history' from memory for heatmap.\")\n",
        "    loaded_history_for_heatmap = training_history\n",
        "elif TRAINING_HISTORY_FILE.exists():\n",
        "    print(f\"loading training history from: {TRAINING_HISTORY_FILE}\")\n",
        "    try:\n",
        "        with open(TRAINING_HISTORY_FILE, 'r') as f_hist_hm:\n",
        "            loaded_history_for_heatmap = json.load(f_hist_hm)\n",
        "        print(\"training history loaded successfully for heatmap.\")\n",
        "    except Exception as e_load_hm:\n",
        "        print(f\"error: failed to load training history for heatmap: {e_load_hm}\")\n",
        "else:\n",
        "    print(f\"error: training history not found. cannot generate heatmap.\")\n",
        "    loaded_history_for_heatmap = None # ensure it's None if loading failed\n",
        "\n",
        "# --- 2. prepare data for correlation ---\n",
        "correlation_df = None\n",
        "if loaded_history_for_heatmap and loaded_history_for_heatmap.get('epoch'):\n",
        "    # select metric keys that are present in the history and have list data\n",
        "    # EMERGENT_METRIC_KEYS (global) should contain all keys, including dynamic w_update ones\n",
        "\n",
        "    # construct a dictionary for dataframe creation, ensuring all lists are of same length\n",
        "    # (number of epochs recorded)\n",
        "    num_epochs_recorded_hm = len(loaded_history_for_heatmap['epoch'])\n",
        "    data_for_df = {}\n",
        "\n",
        "    # use the full list of EMERGENT_METRIC_KEYS that were actually populated\n",
        "    # (Cell 3 appends w_update keys to the global EMERGENT_METRIC_KEYS)\n",
        "    keys_to_correlate = [key for key in EMERGENT_METRIC_KEYS if f\"metric_{key}\" in loaded_history_for_heatmap and isinstance(loaded_history_for_heatmap[f\"metric_{key}\"], list)]\n",
        "\n",
        "    if not keys_to_correlate:\n",
        "        print(\"error: no valid metric keys found in history to build correlation dataframe.\")\n",
        "    else:\n",
        "        for key_base in keys_to_correlate:\n",
        "            history_key = f\"metric_{key_base}\"\n",
        "            metric_data = loaded_history_for_heatmap[history_key]\n",
        "\n",
        "            # handle lists that might be shorter (e.g., weight updates missing first epoch)\n",
        "            if len(metric_data) < num_epochs_recorded_hm:\n",
        "                # pad with NaN at the beginning if it's a weight update, else at the end\n",
        "                if key_base.startswith(\"w_update_\"):\n",
        "                    padded_data = [np.nan] * (num_epochs_recorded_hm - len(metric_data)) + metric_data\n",
        "                else: # for other metrics, pad at the end (shouldn't happen if logged correctly)\n",
        "                    padded_data = metric_data + [np.nan] * (num_epochs_recorded_hm - len(metric_data))\n",
        "                data_for_df[key_base] = padded_data[:num_epochs_recorded_hm] # ensure exact length\n",
        "            elif len(metric_data) > num_epochs_recorded_hm: # truncate if somehow longer\n",
        "                data_for_df[key_base] = metric_data[:num_epochs_recorded_hm]\n",
        "            else:\n",
        "                data_for_df[key_base] = metric_data\n",
        "\n",
        "        try:\n",
        "            correlation_df = pd.DataFrame(data_for_df)\n",
        "            # drop columns that are all NaN (if any metric was never recorded)\n",
        "            correlation_df.dropna(axis=1, how='all', inplace=True)\n",
        "            # for correlation, rows with any NaN will be excluded by default by .corr()\n",
        "            # or fill NaNs with mean/median if preferred, but default pairwise exclusion is often fine\n",
        "            # correlation_df.fillna(correlation_df.mean(), inplace=True) # optional fill\n",
        "            print(f\"dataframe for correlation created with shape: {correlation_df.shape}\")\n",
        "            if correlation_df.empty or correlation_df.shape[1] < 2: # need at least 2 columns to correlate\n",
        "                 print(\"error: not enough valid data columns for correlation analysis.\")\n",
        "                 correlation_df = None\n",
        "\n",
        "        except Exception as e_df_hm:\n",
        "            print(f\"error creating dataframe for heatmap: {e_df_hm}\")\n",
        "            correlation_df = None\n",
        "else:\n",
        "    print(\"no training history data available for heatmap.\")\n",
        "\n",
        "# --- 3. calculate and visualize correlation matrix ---\n",
        "if correlation_df is not None and not correlation_df.empty and correlation_df.shape[1] >= 2:\n",
        "    print(\"\\ncalculating and plotting correlation heatmap...\")\n",
        "    corr_matrix = correlation_df.corr(method='pearson') # pearson, spearman, or kendall\n",
        "\n",
        "    plt.figure(figsize=(max(12, correlation_df.shape[1] * 0.6), max(10, correlation_df.shape[1] * 0.5))) # dynamic figsize\n",
        "\n",
        "    # make labels more readable for heatmap\n",
        "    heatmap_labels = [col.replace('metric_', '').replace('w_update_', 'WUpd:').replace('_', ' ').title()[:25] for col in corr_matrix.columns]\n",
        "\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0,\n",
        "                linewidths=.5, cbar_kws={\"shrink\": .8},\n",
        "                xticklabels=heatmap_labels, yticklabels=heatmap_labels)\n",
        "\n",
        "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
        "    plt.yticks(rotation=0, fontsize=8)\n",
        "    plt.title(f\"Metric & Weight Update Correlation Heatmap (Epochs 1-{num_epochs_recorded_hm}, Test: {TEST_MODE})\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    heatmap_filename = f\"correlation_heatmap_{_EXPERIMENT_TAG_NAME}{'_testmode' if TEST_MODE else ''}.png\"\n",
        "    heatmap_save_path = VISUALIZATION_DIR / heatmap_filename\n",
        "    try:\n",
        "        plt.savefig(heatmap_save_path, bbox_inches='tight', dpi=150)\n",
        "        print(f\"correlation heatmap saved to: {heatmap_save_path.resolve()}\")\n",
        "    except Exception as e_save_hm:\n",
        "        print(f\"error saving heatmap: {e_save_hm}\")\n",
        "    plt.show()\n",
        "\n",
        "    # --- 4. (optional) visualize evolution of correlations over time windows ---\n",
        "    # this is more advanced: split epochs into N windows and plot heatmap for each\n",
        "    # e.g., for 100 epochs, windows: 1-33, 34-66, 67-100\n",
        "    num_windows = 3 # example: early, mid, late training\n",
        "    if num_epochs_recorded_hm >= num_windows * 5: # only if enough data for meaningful windows (e.g. min 5 epochs per window)\n",
        "        print(f\"\\ncalculating correlation heatmaps for {num_windows} time windows...\")\n",
        "        window_size = num_epochs_recorded_hm // num_windows\n",
        "        for i in range(num_windows):\n",
        "            start_epoch_idx = i * window_size\n",
        "            # for the last window, include all remaining epochs\n",
        "            end_epoch_idx = (i + 1) * window_size if i < num_windows - 1 else num_epochs_recorded_hm\n",
        "\n",
        "            window_df = correlation_df.iloc[start_epoch_idx:end_epoch_idx]\n",
        "\n",
        "            if window_df.empty or window_df.shape[0] < 2 or window_df.shape[1] < 2: # need at least 2 rows and 2 columns\n",
        "                print(f\"  skipping window {i+1} due to insufficient data ({window_df.shape[0]} rows).\")\n",
        "                continue\n",
        "\n",
        "            window_corr_matrix = window_df.corr(method='pearson')\n",
        "\n",
        "            # check if correlation matrix itself is all NaN (can happen if a column in window_df is constant or all NaN)\n",
        "            if window_corr_matrix.isnull().all().all():\n",
        "                print(f\"  skipping window {i+1} as correlation matrix is all NaN (likely constant data in window).\")\n",
        "                continue\n",
        "\n",
        "            plt.figure(figsize=(max(12, window_df.shape[1] * 0.6), max(10, window_df.shape[1] * 0.5)))\n",
        "            window_heatmap_labels = [col.replace('metric_', '').replace('w_update_', 'WUpd:').replace('_', ' ').title()[:25] for col in window_corr_matrix.columns]\n",
        "            sns.heatmap(window_corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0,\n",
        "                        linewidths=.5, cbar_kws={\"shrink\": .8},\n",
        "                        xticklabels=window_heatmap_labels, yticklabels=window_heatmap_labels)\n",
        "            plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
        "            plt.yticks(rotation=0, fontsize=8)\n",
        "            # epoch numbers are 1-based for display\n",
        "            start_epoch_disp = loaded_history_for_heatmap['epoch'][start_epoch_idx]\n",
        "            end_epoch_disp = loaded_history_for_heatmap['epoch'][end_epoch_idx-1] # -1 because iloc is exclusive for end\n",
        "            plt.title(f\"Correlation Heatmap (Epochs {start_epoch_disp}-{end_epoch_disp}, Test: {TEST_MODE})\", fontsize=14)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            window_heatmap_filename = f\"correlation_heatmap_window_{i+1}_epochs_{start_epoch_disp}-{end_epoch_disp}_{_EXPERIMENT_TAG_NAME}{'_testmode' if TEST_MODE else ''}.png\"\n",
        "            window_heatmap_save_path = VISUALIZATION_DIR / window_heatmap_filename\n",
        "            try:\n",
        "                plt.savefig(window_heatmap_save_path, bbox_inches='tight', dpi=150)\n",
        "                print(f\"  window {i+1} heatmap saved to: {window_heatmap_save_path.name}\")\n",
        "            except Exception as e_save_hm_win:\n",
        "                print(f\"  error saving window {i+1} heatmap: {e_save_hm_win}\")\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(\"not enough epochs to generate meaningful time-windowed correlation heatmaps.\")\n",
        "else:\n",
        "    if correlation_df is None or correlation_df.empty:\n",
        "        print(\"dataframe for correlation was not created or is empty. skipping heatmap generation.\")\n",
        "    elif correlation_df.shape[1] < 2:\n",
        "        print(f\"not enough columns ({correlation_df.shape[1]}) in dataframe for correlation. skipping heatmap.\")\n",
        "\n",
        "\n",
        "print(f\"\\n--- cell 7 correlation heatmap generator complete (test_mode: {TEST_MODE}) ---\")"
      ],
      "metadata": {
        "id": "Ybl652eId9Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 8: ultimate evolving pca heatmap & cluster story video generator\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # use non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "try: from tqdm.notebook import tqdm\n",
        "except ImportError: tqdm = lambda x, **kwargs: x # simple fallback\n",
        "import gc # for garbage collection during intensive plotting\n",
        "\n",
        "print(f\"--- ultimate pca evolution video generator (test_mode: {TEST_MODE}) ---\")\n",
        "\n",
        "# --- 0. SCRIPT CONFIGURATION & TUNABLE PARAMETERS ---\n",
        "VIDEO_ENABLE_PCA_EVOLUTION = True\n",
        "VIDEO_PCA_FILENAME_PREFIX = \"UltraLofi_PCA_EvolutionStory\"\n",
        "VIDEO_PCA_WIDTH = 720\n",
        "VIDEO_PCA_HEIGHT = 1280 # portrait\n",
        "VIDEO_PCA_FPS = 10.0\n",
        "VIDEO_PCA_PAUSE_DURATION_SECONDS = 2.5\n",
        "\n",
        "LAYOUT_PCA_EPOCH_IMG_RATIO = 0.38\n",
        "LAYOUT_PCA_LC_CHART_RATIO = 0.28\n",
        "LAYOUT_PCA_PCA_HEATMAP_RATIO = 0.34 # bottom section for evolving PCA heatmap\n",
        "\n",
        "# pca specific parameters\n",
        "PCA_WINDOW_SIZE = 15 if not TEST_MODE else 7 # num past epochs for PCA. shorter for test mode.\n",
        "PCA_NUM_COMPONENTS_TO_DISPLAY = 3 # top N PCs for heatmap\n",
        "# select metrics for pca. fewer can be clearer. must exist in history.\n",
        "# example: focusing on key image quality and stability metrics + one weight update\n",
        "PCA_METRICS_TO_INCLUDE_KEYS = [\n",
        "    'metric_shannon_entropy_global',\n",
        "    'metric_local_entropy_variance',\n",
        "    'metric_IoU_with_gt_mean',\n",
        "    'metric_edge_sharpness_sobel',\n",
        "    'metric_output_stability_ssim',\n",
        "    # include one representative weight update metric if available and desired\n",
        "    # ensure this key exactly matches one generated in cell 3's history\n",
        "    f\"metric_w_update_{ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][0].replace('.', '_') if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] else 'conv_in_weight'}\"\n",
        "]\n",
        "PCA_HEATMAP_DPI = 75 # lower dpi for faster rendering in video loop\n",
        "PCA_HEATMAP_BG_COLOR = 'white'\n",
        "PCA_HEATMAP_GRID_COLOR = 'lightgray'\n",
        "PCA_HEATMAP_ANNOT_FMT = \".2f\"\n",
        "PCA_HEATMAP_CMAP = \"vlag\" # diverging: red=positive, blue=negative correlation with PC\n",
        "PCA_HEATMAP_CENTER = 0\n",
        "PCA_HEATMAP_LINEWIDTH = 0.5\n",
        "PCA_HEATMAP_CBAR_SHRINK = 0.8\n",
        "PCA_HEATMAP_TITLE = \"PCA Loadings (Metric Contributions to PCs)\"\n",
        "PCA_HEATMAP_X_LABELS = [f\"PC{i+1}\" for i in range(PCA_NUM_COMPONENTS_TO_DISPLAY)]\n",
        "\n",
        "# inflection event detection (re-using structure from your previous \"ultimate video\")\n",
        "# configure events based on original metrics, not PCA components directly for simplicity of pausing\n",
        "EVENT_DETECTION_CONFIG_PCA_VID = {\n",
        "    'metric_IoU_with_gt_mean':      {'goal': 'maximize', 'threshold_factor': 0.90, 'smoothing': 7, 'stability_win': 5, 'tolerance': 0.03, 'event_label': \"High IoU\"},\n",
        "    'metric_output_stability_ssim': {'goal': 'maximize', 'threshold_factor': 0.95, 'smoothing': 10, 'stability_win': 7, 'tolerance': 0.02, 'event_label': \"Stable Output\"},\n",
        "     f\"metric_w_update_{ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][0].replace('.', '_') if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] else 'conv_in_weight'}\":\n",
        "                                    {'goal': 'minimize', 'threshold_factor': 5, 'smoothing': 10, 'stability_win': 5, 'tolerance': 0.1, 'event_label': f\"{ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][0].split('.')[0] if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] else 'Layer1'} W-Upd Minimized\"},\n",
        "}\n",
        "# use a subset of METRICS_TO_PLOT_CONFIG from previous video cell, just for event text reference\n",
        "METRICS_CONFIG_FOR_EVENT_LABELS_PCA_VID = { # only need labels for events defined above\n",
        "    'metric_IoU_with_gt_mean': {'label': \"Mean IoU w/ GT\"},\n",
        "    'metric_output_stability_ssim': {'label': \"Stability (SSIM)\"},\n",
        "    f\"metric_w_update_{ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][0].replace('.', '_') if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] else 'conv_in_weight'}\":\n",
        "                                   {'label': f\"W-Upd: {ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'][0].split('.')[0] if ENHANCED_METRIC_CONFIG['tracked_layers_for_weights'] else 'Layer1'}\"}\n",
        "}\n",
        "\n",
        "\n",
        "# pause text & lc chart line (same as before)\n",
        "PAUSE_TEXT_FONT_PCA = cv2.FONT_HERSHEY_SIMPLEX; PAUSE_TEXT_SCALE_PCA = 0.7; PAUSE_TEXT_COLOR_PCA = (255,255,255)\n",
        "PAUSE_TEXT_THICKNESS_PCA = 2; PAUSE_TEXT_BG_COLOR_PCA = (0,0,0); PAUSE_TEXT_BG_ALPHA_PCA = 0.7\n",
        "PAUSE_TEXT_Y_POS_RATIO_PCA = 0.05; PAUSE_TEXT_PADDING_PCA = 10\n",
        "LC_CHART_LINE_COLOR_PCA = (0,0,255); LC_CHART_LINE_THICKNESS_PCA = 2\n",
        "LC_PLOT_CALIB_PCA = {'x_plot_start_rel': 60, 'plot_width_rel': VIDEO_PCA_WIDTH - 120,\n",
        "                     'y_line_start_rel': 40, 'line_height_rel': int(VIDEO_PCA_HEIGHT * LAYOUT_PCA_LC_CHART_RATIO) - 70}\n",
        "\n",
        "\n",
        "# --- 1. Initialization and Path Setup ---\n",
        "NUM_PCA_PAUSE_FRAMES = int(VIDEO_PCA_FPS * VIDEO_PCA_PAUSE_DURATION_SECONDS)\n",
        "_VIDEO_TIMESTAMP_PCA = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "_VIDEO_FILENAME_PCA = f\"{VIDEO_PCA_FILENAME_PREFIX}_{_EXPERIMENT_TAG_NAME}{'_testmode' if TEST_MODE else ''}_{_VIDEO_TIMESTAMP_PCA}.mp4\"\n",
        "_VIDEO_FULL_PATH_PCA = VIDEO_OUTPUT_DIR / _VIDEO_FILENAME_PCA # VIDEO_OUTPUT_DIR from cell 0 (on Drive)\n",
        "\n",
        "print(f\"  PCA Evolution Video Output Path: {_VIDEO_FULL_PATH_PCA}\")\n",
        "_H_EPOCH_PCA = int(VIDEO_PCA_HEIGHT * LAYOUT_PCA_EPOCH_IMG_RATIO)\n",
        "_H_LC_CHART_PCA = int(VIDEO_PCA_HEIGHT * LAYOUT_PCA_LC_CHART_RATIO)\n",
        "_H_PCA_HEATMAP_PCA = VIDEO_PCA_HEIGHT - _H_EPOCH_PCA - _H_LC_CHART_PCA\n",
        "print(f\"  Layout (H_px): EpochImg:{_H_EPOCH_PCA}, LCChart:{_H_LC_CHART_PCA}, PCAHeatmap:{_H_PCA_HEATMAP_PCA}\")\n",
        "\n",
        "# --- 2. Helper Functions ---\n",
        "def get_pca_loadings_with_alignment(data_window_standardized_df, n_components, prev_loadings_matrix=None):\n",
        "    if data_window_standardized_df.shape[0] < n_components or data_window_standardized_df.shape[0] < PCA_WINDOW_SIZE / 2 : # need enough samples\n",
        "        return None, None\n",
        "\n",
        "    pca = PCA(n_components=n_components, svd_solver='full') # ensure svd_solver for small feature sets\n",
        "    try:\n",
        "        pca.fit(data_window_standardized_df)\n",
        "    except ValueError: # e.g. if data has NaNs or Infs not caught earlier, or all zeros\n",
        "        return None, None\n",
        "\n",
        "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_) # scaled loadings\n",
        "    loadings_df = pd.DataFrame(loadings, index=data_window_standardized_df.columns, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
        "\n",
        "    # sign alignment (simple: largest absolute value in each PC vector is positive)\n",
        "    # more robust: align with previous frame's loadings if available\n",
        "    if prev_loadings_matrix is not None and prev_loadings_matrix.shape == loadings_df.shape:\n",
        "        for col in loadings_df.columns:\n",
        "            if np.sign(loadings_df[col].iloc[np.argmax(np.abs(loadings_df[col]))]) != \\\n",
        "               np.sign(prev_loadings_matrix[col].iloc[np.argmax(np.abs(prev_loadings_matrix[col]))]):\n",
        "                if (prev_loadings_matrix[col] @ loadings_df[col]) < 0: # if dot product is negative, flip\n",
        "                     loadings_df[col] *= -1\n",
        "    else: # initial frame or shape mismatch, use simple max abs value alignment\n",
        "        for col in loadings_df.columns:\n",
        "            if loadings_df[col].iloc[np.argmax(np.abs(loadings_df[col]))] < 0:\n",
        "                loadings_df[col] *= -1\n",
        "\n",
        "    return loadings_df, pca.explained_variance_ratio_\n",
        "\n",
        "# re-use event detection from previous video cell if available, or define here:\n",
        "def detect_metric_trigger_events_video_pca(history_dict, full_event_config, selected_metrics_config):\n",
        "    if not history_dict or not history_dict.get('epoch'): return []\n",
        "    num_epochs = len(history_dict['epoch'])\n",
        "    detected_events = []\n",
        "    for metric_key_hist, config in full_event_config.items():\n",
        "        if metric_key_hist not in selected_metrics_config: continue\n",
        "        metric_values = history_dict.get(metric_key_hist)\n",
        "        if not metric_values or len(metric_values) != num_epochs: continue\n",
        "        series = pd.Series(metric_values).fillna(method='bfill').fillna(method='ffill')\n",
        "        if series.isna().any() or len(series) < config['smoothing'] or len(series) < config['stability_win']: continue\n",
        "        smoothed = series.rolling(window=config['smoothing'],center=True,min_periods=1).mean().fillna(method='bfill').fillna(method='ffill')\n",
        "        if smoothed.isna().any():\n",
        "            first_valid_idx = smoothed.first_valid_index()\n",
        "            if first_valid_idx is not None and first_valid_idx > 0: smoothed.iloc[:first_valid_idx] = smoothed.iloc[first_valid_idx]\n",
        "        trigger_idx = -1\n",
        "        if config['goal'] == 'maximize':\n",
        "            target = smoothed.max() * config.get('threshold_factor',0.95)\n",
        "            for i in range(num_epochs - config['stability_win']):\n",
        "                if smoothed.iloc[i] >= target:\n",
        "                    if (smoothed.iloc[i : i + config['stability_win']] >= target * (1-config['tolerance'])).all(): trigger_idx=i; break\n",
        "        elif config['goal'] == 'minimize':\n",
        "            override = config.get('target_val_override')\n",
        "            target_abs = override if override is not None else (smoothed.min() * config.get('threshold_factor',1.05))\n",
        "            tol_abs = abs(target_abs * config['tolerance']) if target_abs != 0 else config['tolerance']\n",
        "            for i in range(num_epochs - config['stability_win']):\n",
        "                if abs(smoothed.iloc[i] - target_abs) <= tol_abs:\n",
        "                    if (abs(smoothed.iloc[i : i + config['stability_win']] - target_abs) <= tol_abs).all(): trigger_idx=i; break\n",
        "        elif config['goal'] == 'stabilize':\n",
        "            stabilization_thresh = smoothed.std() * config['tolerance'] if smoothed.std() > 1e-6 else config['tolerance'] * 0.01\n",
        "            for i in range(config['smoothing'], num_epochs - config['stability_win']):\n",
        "                if smoothed.iloc[i : i + config['stability_win']].std() < stabilization_thresh : trigger_idx=i; break\n",
        "        if trigger_idx != -1:\n",
        "            detected_events.append({'metric_key': metric_key_hist, 'display_name': config.get('event_label', selected_metrics_config[metric_key_hist]['label']),\n",
        "                                    'trigger_epoch_idx': trigger_idx, 'trigger_epoch_num': history_dict['epoch'][trigger_idx]})\n",
        "    return sorted(detected_events, key=lambda e: e['trigger_epoch_idx'])\n",
        "\n",
        "# --- 3. Load Data, Pre-process for PCA, Detect Pause Events ---\n",
        "CAN_RUN_PCA_VIDEO = True if VIDEO_ENABLE_PCA_EVOLUTION else False\n",
        "_HISTORY_DATA_PCA = None\n",
        "if CAN_RUN_PCA_VIDEO:\n",
        "    if 'training_history' in globals() and training_history.get('epoch'): _HISTORY_DATA_PCA = training_history\n",
        "    elif TRAINING_HISTORY_FILE.exists():\n",
        "        try:\n",
        "            with open(TRAINING_HISTORY_FILE, 'r') as f: _HISTORY_DATA_PCA = json.load(f)\n",
        "            print(f\"loaded history from: {TRAINING_HISTORY_FILE}\")\n",
        "        except Exception as e: print(f\"error loading history: {e}\"); CAN_RUN_PCA_VIDEO = False\n",
        "    else: print(f\"error: training history not found.\"); CAN_RUN_PCA_VIDEO = False\n",
        "\n",
        "_PCA_METRICS_DF_SCALED = None\n",
        "if CAN_RUN_PCA_VIDEO and _HISTORY_DATA_PCA:\n",
        "    pca_data_dict = {}\n",
        "    num_epochs_total = len(_HISTORY_DATA_PCA['epoch'])\n",
        "    valid_pca_metric_keys = []\n",
        "    for key in PCA_METRICS_TO_INCLUDE_KEYS: # key is like 'metric_shannon_entropy_global'\n",
        "        if key in _HISTORY_DATA_PCA and isinstance(_HISTORY_DATA_PCA[key], list) and len(_HISTORY_DATA_PCA[key]) == num_epochs_total:\n",
        "            pca_data_dict[key.replace('metric_', '')] = _HISTORY_DATA_PCA[key] # use shorter names for df columns\n",
        "            valid_pca_metric_keys.append(key.replace('metric_', ''))\n",
        "        else: print(f\"warning: metric '{key}' for PCA not found or length mismatch in history. skipping for pca.\")\n",
        "\n",
        "    if len(pca_data_dict) < PCA_NUM_COMPONENTS_TO_DISPLAY or len(pca_data_dict) < 2: # need at least N_PC features and at least 2\n",
        "        print(f\"error: not enough valid metrics ({len(pca_data_dict)}) for PCA (need >= {max(2,PCA_NUM_COMPONENTS_TO_DISPLAY)}). cannot generate pca video.\")\n",
        "        CAN_RUN_PCA_VIDEO = False\n",
        "    else:\n",
        "        _pca_metrics_df_raw = pd.DataFrame(pca_data_dict)\n",
        "        # handle NaNs from weight updates (first epoch typically NaN) by forward fill then backward fill\n",
        "        _pca_metrics_df_filled = _pca_metrics_df_raw.fillna(method='ffill').fillna(method='bfill')\n",
        "        # if still NaNs (e.g. all NaNs in a col), fill with 0 or mean before scaling\n",
        "        if _pca_metrics_df_filled.isnull().any().any():\n",
        "             _pca_metrics_df_filled.fillna(0, inplace=True) # simple fill with 0 for remaining NaNs\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        _PCA_METRICS_DF_SCALED = pd.DataFrame(scaler.fit_transform(_pca_metrics_df_filled), columns=_pca_metrics_df_filled.columns)\n",
        "        print(f\"data prepared for PCA with {_PCA_METRICS_DF_SCALED.shape[1]} features.\")\n",
        "        # Store the actual column names used in PCA for heatmap y-labels\n",
        "        PCA_METRIC_YLABELS_ACTUAL = [name.replace('_', ' ').title()[:20] for name in _PCA_METRICS_DF_SCALED.columns]\n",
        "\n",
        "\n",
        "_TRIGGER_EVENTS_LIST_PCA = []\n",
        "if CAN_RUN_PCA_VIDEO and _HISTORY_DATA_PCA:\n",
        "    _TRIGGER_EVENTS_LIST_PCA = detect_metric_trigger_events_video_pca(_HISTORY_DATA_PCA, EVENT_DETECTION_CONFIG_PCA_VID, METRICS_CONFIG_FOR_EVENT_LABELS_PCA_VID)\n",
        "    if not _TRIGGER_EVENTS_LIST_PCA: print(\"no metric trigger events for pause detected.\")\n",
        "    else: print(f\"detected {len(_TRIGGER_EVENTS_LIST_PCA)} metric trigger events for pause.\")\n",
        "\n",
        "\n",
        "# --- 4. Load Static LC Chart & Epoch Images ---\n",
        "_LC_CHART_FINAL_PCA, _EPOCH_IMAGE_PATHS_PCA, _NUM_FRAMES_PCA = None, [], 0\n",
        "if CAN_RUN_PCA_VIDEO:\n",
        "    lc_path_pca = VISUALIZATION_DIR / _FIGURE_SAVE_NAME_MAIN_PLOT\n",
        "    if lc_path_pca.exists():\n",
        "        lc_orig_pca = cv2.imread(str(lc_path_pca)); _LC_CHART_FINAL_PCA = cv2.resize(lc_orig_pca, (VIDEO_PCA_WIDTH, _H_LC_CHART_PCA), cv2.INTER_AREA) if lc_orig_pca is not None else None\n",
        "    if _LC_CHART_FINAL_PCA is None: print(f\"error: LC chart image load/resize failed.\"); CAN_RUN_PCA_VIDEO = False\n",
        "\n",
        "    img_pattern_pca = f\"epoch_*_samples.png\"\n",
        "    def get_epoch_idx_pca(p): match = re.search(r\"epoch_(\\d+)_samples\",p.name); return int(match.group(1))-1 if match else -1\n",
        "    raw_paths_pca = [p for p in VISUALIZATION_DIR.glob(img_pattern_pca) if get_epoch_idx_pca(p) != -1]\n",
        "    _EPOCH_IMAGE_PATHS_PCA = sorted(raw_paths_pca, key=get_epoch_idx_pca)\n",
        "    _NUM_FRAMES_PCA = len(_EPOCH_IMAGE_PATHS_PCA)\n",
        "    if _NUM_FRAMES_PCA == 0: print(f\"error: no epoch images found.\"); CAN_RUN_PCA_VIDEO = False\n",
        "    elif _HISTORY_DATA_PCA and len(_HISTORY_DATA_PCA.get('epoch',[])) != _NUM_FRAMES_PCA:\n",
        "        print(f\"warning: epoch img count ({_NUM_FRAMES_PCA}) != history epochs ({len(_HISTORY_DATA_PCA.get('epoch',[]))}). Adjusting PCA data length if necessary.\")\n",
        "        if _PCA_METRICS_DF_SCALED is not None and len(_PCA_METRICS_DF_SCALED) > _NUM_FRAMES_PCA:\n",
        "            _PCA_METRICS_DF_SCALED = _PCA_METRICS_DF_SCALED.iloc[:_NUM_FRAMES_PCA]\n",
        "            print(f\"  PCA data truncated to {_NUM_FRAMES_PCA} rows to match image count.\")\n",
        "\n",
        "\n",
        "# --- 5. Initialize VideoWriter ---\n",
        "_VIDEO_WRITER_PCA = None\n",
        "if CAN_RUN_PCA_VIDEO and _NUM_FRAMES_PCA > 0:\n",
        "    _VIDEO_WRITER_PCA = cv2.VideoWriter(str(_VIDEO_FULL_PATH_PCA), cv2.VideoWriter_fourcc(*'mp4v'), VIDEO_PCA_FPS, (VIDEO_PCA_WIDTH, VIDEO_PCA_HEIGHT))\n",
        "    if not _VIDEO_WRITER_PCA.isOpened(): print(f\"error: could not open VideoWriter.\"); CAN_RUN_PCA_VIDEO = False\n",
        "\n",
        "# --- 6. Main Video Generation Loop ---\n",
        "if CAN_RUN_PCA_VIDEO and _NUM_FRAMES_PCA > 0 and _VIDEO_WRITER_PCA and _VIDEO_WRITER_PCA.isOpened() and _PCA_METRICS_DF_SCALED is not None:\n",
        "    print(f\"\\nstarting PCA evolution video generation for {_NUM_FRAMES_PCA} frames...\")\n",
        "    _processed_pause_events_pca = set()\n",
        "    fig_w_pca, fig_h_pca = VIDEO_PCA_WIDTH / PCA_HEATMAP_DPI, _H_PCA_HEATMAP_PCA / PCA_HEATMAP_DPI\n",
        "    _previous_loadings_for_alignment = None # for aligning PC signs\n",
        "\n",
        "    for frame_idx in tqdm(range(_NUM_FRAMES_PCA), desc=\"Generating PCA Video\"):\n",
        "        # a. epoch sample image\n",
        "        epoch_img_pca = cv2.imread(str(_EPOCH_IMAGE_PATHS_PCA[frame_idx]))\n",
        "        epoch_final_img_pca = cv2.resize(epoch_img_pca, (VIDEO_PCA_WIDTH, _H_EPOCH_PCA), cv2.INTER_AREA) if epoch_img_pca is not None else np.zeros((_H_EPOCH_PCA, VIDEO_PCA_WIDTH, 3),dtype=np.uint8)\n",
        "\n",
        "        # b. evolving pca heatmap\n",
        "        pca_heatmap_img = np.full((_H_PCA_HEATMAP_PCA, VIDEO_PCA_WIDTH, 3), 240, dtype=np.uint8) # default light gray bg\n",
        "\n",
        "        # ensure frame_idx is within bounds of _PCA_METRICS_DF_SCALED\n",
        "        if frame_idx < len(_PCA_METRICS_DF_SCALED) :\n",
        "            current_window_data = _PCA_METRICS_DF_SCALED.iloc[max(0, frame_idx - PCA_WINDOW_SIZE + 1) : frame_idx + 1]\n",
        "\n",
        "            if not current_window_data.empty and current_window_data.shape[0] >= max(2,PCA_NUM_COMPONENTS_TO_DISPLAY) : # need enough samples for PCA\n",
        "                loadings_df, explained_variance = get_pca_loadings_with_alignment(current_window_data, PCA_NUM_COMPONENTS_TO_DISPLAY, _previous_loadings_for_alignment)\n",
        "                if loadings_df is not None:\n",
        "                    _previous_loadings_for_alignment = loadings_df.copy() # store for next frame's alignment\n",
        "                    fig_pca, ax_pca = plt.subplots(figsize=(fig_w_pca, fig_h_pca), dpi=PCA_HEATMAP_DPI)\n",
        "                    fig_pca.patch.set_facecolor(PCA_HEATMAP_BG_COLOR); ax_pca.set_facecolor(PCA_HEATMAP_BG_COLOR)\n",
        "                    sns.heatmap(loadings_df, ax=ax_pca, annot=True, fmt=PCA_HEATMAP_ANNOT_FMT, cmap=PCA_HEATMAP_CMAP, center=PCA_HEATMAP_CENTER,\n",
        "                                linewidths=PCA_HEATMAP_LINEWIDTH, cbar_kws={\"shrink\": PCA_HEATMAP_CBAR_SHRINK}, yticklabels=PCA_METRIC_YLABELS_ACTUAL) # use actual feature names for y-ticks\n",
        "                    pc_x_labels_with_variance = [f\"{PCA_HEATMAP_X_LABELS[i]}\\n({explained_variance[i]*100:.1f}%)\" for i in range(len(explained_variance))]\n",
        "                    ax_pca.set_xticklabels(pc_x_labels_with_variance, rotation=0, fontsize=7)\n",
        "                    ax_pca.tick_params(axis='y', rotation=0, labelsize=7)\n",
        "                    ax_pca.set_title(PCA_HEATMAP_TITLE, fontsize=9)\n",
        "                    plt.tight_layout(pad=0.5)\n",
        "                    fig_pca.canvas.draw()\n",
        "                    pca_heatmap_img_rendered = cv2.cvtColor(np.array(fig_pca.canvas.buffer_rgba()), cv2.COLOR_RGBA2BGR)\n",
        "                    if pca_heatmap_img_rendered.shape[0] != _H_PCA_HEATMAP_PCA or pca_heatmap_img_rendered.shape[1] != VIDEO_PCA_WIDTH:\n",
        "                        pca_heatmap_img = cv2.resize(pca_heatmap_img_rendered, (VIDEO_PCA_WIDTH, _H_PCA_HEATMAP_PCA), interpolation=cv2.INTER_AREA)\n",
        "                    else: pca_heatmap_img = pca_heatmap_img_rendered\n",
        "                    plt.close(fig_pca); # gc.collect() called later\n",
        "                else: # PCA failed for this window\n",
        "                    cv2.putText(pca_heatmap_img, \"PCA Priming...\", (50, _H_PCA_HEATMAP_PCA//2), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
        "            else: # not enough data in window yet\n",
        "                 cv2.putText(pca_heatmap_img, f\"PCA: {frame_idx+1}/{PCA_WINDOW_SIZE} epochs\", (30, _H_PCA_HEATMAP_PCA//2), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (100,100,100), 2)\n",
        "        else: # frame_idx beyond available PCA data (shouldn't happen if data aligned to _NUM_FRAMES_PCA)\n",
        "            cv2.putText(pca_heatmap_img, \"PCA Data Unavailable\", (50, _H_PCA_HEATMAP_PCA//2), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
        "\n",
        "\n",
        "        # c. composite frame\n",
        "        base_frame_pca = np.zeros((VIDEO_PCA_HEIGHT, VIDEO_PCA_WIDTH, 3), dtype=np.uint8)\n",
        "        y_off_pca = 0\n",
        "        base_frame_pca[y_off_pca : y_off_pca + _H_EPOCH_PCA, :] = epoch_final_img_pca; y_off_pca += _H_EPOCH_PCA\n",
        "        if _LC_CHART_FINAL_PCA is not None: base_frame_pca[y_off_pca : y_off_pca + _H_LC_CHART_PCA, :] = _LC_CHART_FINAL_PCA\n",
        "        y_off_lc_start_pca = y_off_pca; y_off_pca += _H_LC_CHART_PCA\n",
        "        base_frame_pca[y_off_pca : y_off_pca + _H_PCA_HEATMAP_PCA, :] = pca_heatmap_img\n",
        "\n",
        "        # lc chart progress line\n",
        "        if _LC_CHART_FINAL_PCA is not None:\n",
        "            lc_prog_ratio_pca = frame_idx / (_NUM_FRAMES_PCA - 1) if _NUM_FRAMES_PCA > 1 else 0.0\n",
        "            lx_lc = LC_PLOT_CALIB_PCA['x_plot_start_rel'] + int(lc_prog_ratio_pca * LC_PLOT_CALIB_PCA['plot_width_rel'])\n",
        "            lx_lc = max(LC_PLOT_CALIB_PCA['x_plot_start_rel'], min(lx_lc, LC_PLOT_CALIB_PCA['x_plot_start_rel'] + LC_PLOT_CALIB_PCA['plot_width_rel']))\n",
        "            ly1, ly2 = y_off_lc_start_pca + LC_PLOT_CALIB_PCA['y_line_start_rel'], y_off_lc_start_pca + LC_PLOT_CALIB_PCA['y_line_start_rel'] + LC_PLOT_CALIB_PCA['line_height_rel']\n",
        "            cv2.line(base_frame_pca, (lx_lc, ly1), (lx_lc, ly2), LC_CHART_LINE_COLOR_PCA, LC_CHART_LINE_THICKNESS_PCA)\n",
        "\n",
        "        epoch_num_pca_disp = get_epoch_idx_pca(_EPOCH_IMAGE_PATHS_PCA[frame_idx]) + 1\n",
        "        cv2.putText(base_frame_pca, f\"Epoch: {epoch_num_pca_disp}\", (15, _H_EPOCH_PCA-15), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (220,220,220),1,cv2.LINE_AA)\n",
        "\n",
        "        # d. pause events\n",
        "        pause_pca_frame, active_event_names_pca = False, []\n",
        "        for evt_idx, evt_data in enumerate(_TRIGGER_EVENTS_LIST_PCA):\n",
        "            if evt_data['trigger_epoch_idx'] == frame_idx and evt_idx not in _processed_pause_events_pca:\n",
        "                pause_pca_frame = True; active_event_names_pca.append(evt_data['display_name']); _processed_pause_events_pca.add(evt_idx)\n",
        "\n",
        "        if pause_pca_frame:\n",
        "            pause_ann_pca = base_frame_pca.copy()\n",
        "            pause_lbl_pca = f\"EVENT: {', '.join(active_event_names_pca)} (Ep {epoch_num_pca_disp})\"\n",
        "            (ptxt_w,ptxt_h),p_bline = cv2.getTextSize(pause_lbl_pca,PAUSE_TEXT_FONT_PCA,PAUSE_TEXT_SCALE_PCA,PAUSE_TEXT_THICKNESS_PCA)\n",
        "            pb_h = ptxt_h+p_bline+2*PAUSE_TEXT_PADDING_PCA\n",
        "            pb_y = int(VIDEO_PCA_HEIGHT*PAUSE_TEXT_Y_POS_RATIO_PCA) - int(pb_h/2)\n",
        "            pb_x = int((VIDEO_PCA_WIDTH-(ptxt_w+2*PAUSE_TEXT_PADDING_PCA))/2)\n",
        "            ptxt_x, ptxt_y = pb_x+PAUSE_TEXT_PADDING_PCA, pb_y+PAUSE_TEXT_PADDING_PCA+ptxt_h\n",
        "\n",
        "            pb_overlay = pause_ann_pca.copy()\n",
        "            cv2.rectangle(pb_overlay,(pb_x,pb_y),(pb_x+ptxt_w+2*PAUSE_TEXT_PADDING_PCA,pb_y+pb_h),PAUSE_TEXT_BG_COLOR_PCA,-1)\n",
        "            cv2.addWeighted(pb_overlay,PAUSE_TEXT_BG_ALPHA_PCA,pause_ann_pca,1-PAUSE_TEXT_BG_ALPHA_PCA,0,pause_ann_pca)\n",
        "            cv2.putText(pause_ann_pca,pause_lbl_pca,(ptxt_x,ptxt_y),PAUSE_TEXT_FONT_PCA,PAUSE_TEXT_SCALE_PCA,PAUSE_TEXT_COLOR_PCA,PAUSE_TEXT_THICKNESS_PCA,cv2.LINE_AA)\n",
        "            for _ in range(NUM_PCA_PAUSE_FRAMES): _VIDEO_WRITER_PCA.write(pause_ann_pca)\n",
        "        else:\n",
        "            _VIDEO_WRITER_PCA.write(base_frame_pca)\n",
        "\n",
        "        if frame_idx % 50 == 0: gc.collect() # periodic garbage collection\n",
        "\n",
        "    print(\"pca evolution video generation loop complete.\")\n",
        "\n",
        "# --- 7. Finalize ---\n",
        "if _VIDEO_WRITER_PCA is not None and _VIDEO_WRITER_PCA.isOpened():\n",
        "    _VIDEO_WRITER_PCA.release(); print(f\"pca evolution video saved successfully to: {_VIDEO_FULL_PATH_PCA.resolve()}\")\n",
        "elif CAN_RUN_PCA_VIDEO and _NUM_FRAMES_PCA > 0: print(\"video writer not open/init. video not saved.\")\n",
        "else: print(\"pca video generation skipped due to errors or no frames.\")\n",
        "\n",
        "print(f\"\\n--- cell 8 pca evolution video generator complete (test_mode: {TEST_MODE}) ---\")"
      ],
      "metadata": {
        "id": "qgIR2uVGVkPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1c6XZz6-4KWn_zJrj6xXpQ0CETJWJVb5b",
      "authorship_tag": "ABX9TyOyoj1BEsJy+DeKmnmVESZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "084fb525a670469a8e649ee1eafb9fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d3e9d316e214ca395014d5c19e1896c",
              "IPY_MODEL_3f8356e494ee49869906de79f39e2c8e",
              "IPY_MODEL_4bfb09441718433e8c0e0bf5f5ab975a"
            ],
            "layout": "IPY_MODEL_5c3cd339e0ed48b782aa8229612a07ee"
          }
        },
        "2d3e9d316e214ca395014d5c19e1896c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed19682e67d047878ee60c68e686c3b0",
            "placeholder": "​",
            "style": "IPY_MODEL_5b02532e3f8940adb3f1c9e77f693a99",
            "value": "generating classes: 100%"
          }
        },
        "3f8356e494ee49869906de79f39e2c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d2f944560240fcac134cb700a9a78c",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c308a1fae554286a35f3bafa36bd879",
            "value": 7
          }
        },
        "4bfb09441718433e8c0e0bf5f5ab975a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf88c0e6db2f45d7bb447326e4af3d50",
            "placeholder": "​",
            "style": "IPY_MODEL_86587f8f30aa405aa2073d366eba017e",
            "value": " 7/7 [00:00&lt;00:00, 148.96it/s]"
          }
        },
        "5c3cd339e0ed48b782aa8229612a07ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed19682e67d047878ee60c68e686c3b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b02532e3f8940adb3f1c9e77f693a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85d2f944560240fcac134cb700a9a78c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c308a1fae554286a35f3bafa36bd879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf88c0e6db2f45d7bb447326e4af3d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86587f8f30aa405aa2073d366eba017e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c77ffae60c794425b6866181cfad5bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52349ea4dd9542b98178b99f91558d9d",
              "IPY_MODEL_ad57104d7e954b27bccf4f15934aacdc",
              "IPY_MODEL_d66bdee52f2942dc8bb1756ddb57dbe8"
            ],
            "layout": "IPY_MODEL_a0b3fe77f4b74c6799545a64a8c064ff"
          }
        },
        "52349ea4dd9542b98178b99f91558d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daa512b7aab74d44a161cb3099ca6be7",
            "placeholder": "​",
            "style": "IPY_MODEL_37516cd89e8744cda9d9d74bccb44dd8",
            "value": "epoch 1/10 [train]: 100%"
          }
        },
        "ad57104d7e954b27bccf4f15934aacdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f8065df85b4db08571b8e492602672",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5abc2f316a3c44a3a5ef28adb8b4b9af",
            "value": 4
          }
        },
        "d66bdee52f2942dc8bb1756ddb57dbe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f75d8eaf16bb4fa0b1385e7a2f1c742b",
            "placeholder": "​",
            "style": "IPY_MODEL_db08d5d62d2f45f888a331c03d03841f",
            "value": " 4/4 [00:04&lt;00:00,  1.11s/it, loss=0.52, lr=0.0005]"
          }
        },
        "a0b3fe77f4b74c6799545a64a8c064ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "daa512b7aab74d44a161cb3099ca6be7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37516cd89e8744cda9d9d74bccb44dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19f8065df85b4db08571b8e492602672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5abc2f316a3c44a3a5ef28adb8b4b9af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f75d8eaf16bb4fa0b1385e7a2f1c742b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db08d5d62d2f45f888a331c03d03841f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46e96862b7a24eba83dff7e041be7c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43aa06377b83403080db17db1215de9b",
              "IPY_MODEL_f3808c4cc70a4b01b513b9a6338e7dd0",
              "IPY_MODEL_6f3d0a8def68401f96c159ccb2825a4d"
            ],
            "layout": "IPY_MODEL_0a83fdc9638b4014b41669cdf35266e5"
          }
        },
        "43aa06377b83403080db17db1215de9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45a85c13511d4d0cb7604440d6f564bd",
            "placeholder": "​",
            "style": "IPY_MODEL_59e4c30c3d264a1e9e706f499eb3fc65",
            "value": "epoch 1/10 [val]: 100%"
          }
        },
        "f3808c4cc70a4b01b513b9a6338e7dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d42f7c840a345ca909836b69df216c6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5cad83a33984d5d9fba73368a02b485",
            "value": 2
          }
        },
        "6f3d0a8def68401f96c159ccb2825a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5373f4990a3f4eafb55c25f272a8b13a",
            "placeholder": "​",
            "style": "IPY_MODEL_3ed317ebebbc40b9960086e4ba7d6917",
            "value": " 2/2 [00:01&lt;00:00,  1.20it/s]"
          }
        },
        "0a83fdc9638b4014b41669cdf35266e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "45a85c13511d4d0cb7604440d6f564bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59e4c30c3d264a1e9e706f499eb3fc65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d42f7c840a345ca909836b69df216c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5cad83a33984d5d9fba73368a02b485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5373f4990a3f4eafb55c25f272a8b13a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ed317ebebbc40b9960086e4ba7d6917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49dbfe060f3f4a93b505d82dc9837b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a66b3bca6774786aa07b5b86ef8e937",
              "IPY_MODEL_bb724707a82b47ff9fe74703bb72cc30",
              "IPY_MODEL_f42d0e47f3ae4c8dbb8d436e477697c6"
            ],
            "layout": "IPY_MODEL_b3ffc869431145ca9268b7a63bebe6a1"
          }
        },
        "8a66b3bca6774786aa07b5b86ef8e937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4fbe1814f6e4fb388a73f7a7512be4c",
            "placeholder": "​",
            "style": "IPY_MODEL_8789ae1369ef4e1da51c59280824e129",
            "value": "epoch 2/10 [train]: 100%"
          }
        },
        "bb724707a82b47ff9fe74703bb72cc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dcb13fab38e42f1b1c0341002db7c47",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffd014d8183b4146ab165fa666db5e79",
            "value": 4
          }
        },
        "f42d0e47f3ae4c8dbb8d436e477697c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ff5621e6814cb4a8e92e40d62fe4a0",
            "placeholder": "​",
            "style": "IPY_MODEL_56fbe536e82f4f24807ebfb56e7311e2",
            "value": " 4/4 [00:09&lt;00:00,  2.05s/it, loss=0.141, lr=0.000485]"
          }
        },
        "b3ffc869431145ca9268b7a63bebe6a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "e4fbe1814f6e4fb388a73f7a7512be4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8789ae1369ef4e1da51c59280824e129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dcb13fab38e42f1b1c0341002db7c47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd014d8183b4146ab165fa666db5e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00ff5621e6814cb4a8e92e40d62fe4a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56fbe536e82f4f24807ebfb56e7311e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81ac840bec3c4b4995a29ab360d003ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6eabec552de4d109094bde0cc9ae6ef",
              "IPY_MODEL_e5e245f42be44328be791a15ea4c1aca",
              "IPY_MODEL_9aaeec6f3a80471088afa6f1181e18b4"
            ],
            "layout": "IPY_MODEL_28c5253e734c4fb1b0349499203bf19e"
          }
        },
        "b6eabec552de4d109094bde0cc9ae6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe131076c7bd45fc9742301591d44134",
            "placeholder": "​",
            "style": "IPY_MODEL_c543ed4b82ad44a388a2cf923664b6c6",
            "value": "epoch 2/10 [val]: 100%"
          }
        },
        "e5e245f42be44328be791a15ea4c1aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a0580e9e914382a0132d05adfeffd5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55025b238c0945b98edbbd561f03bdb2",
            "value": 2
          }
        },
        "9aaeec6f3a80471088afa6f1181e18b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3854c561b5e46018c0b887b0aa004ff",
            "placeholder": "​",
            "style": "IPY_MODEL_bb3ac54aa3314001bf929fe52fa41b41",
            "value": " 2/2 [00:00&lt;00:00,  2.28it/s]"
          }
        },
        "28c5253e734c4fb1b0349499203bf19e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "fe131076c7bd45fc9742301591d44134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c543ed4b82ad44a388a2cf923664b6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8a0580e9e914382a0132d05adfeffd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55025b238c0945b98edbbd561f03bdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3854c561b5e46018c0b887b0aa004ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb3ac54aa3314001bf929fe52fa41b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}