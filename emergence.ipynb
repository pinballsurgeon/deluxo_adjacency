{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinballsurgeon/sweeping_analysis/blob/main/emergence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9dJdg4LTLDY"
      },
      "source": [
        "## installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G8PF6dZ36hP",
        "outputId": "e2c865e2-d463-4ff5-8550-60fc08925e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.7/dist-packages (4.8.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from pytrends) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pytrends) (4.2.6)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from pytrends) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->pytrends) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->pytrends) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->pytrends) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->pytrends) (2021.10.8)\n",
            "Requirement already satisfied: joypy in /usr/local/lib/python3.7/dist-packages (0.2.6)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from joypy) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from joypy) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from joypy) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joypy) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->joypy) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->joypy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.20.0->joypy) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joypy) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joypy) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joypy) (1.3.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.7/dist-packages (from openai) (1.2.0.49)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.63.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytrends\n",
        "!pip install joypy\n",
        "\n",
        "# install openai\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWG_yZP1TJGo"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "GaUejYHj0rAZ"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas \n",
        "import re\n",
        "import string\n",
        "\n",
        "import time \n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "from pytrends.request import TrendReq\n",
        "from pytrends         import dailydata\n",
        "import pytrends\n",
        "from operator         import index\n",
        "\n",
        "import joypy\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn \n",
        "\n",
        "import openai\n",
        "\n",
        "import networkx as nx\n",
        "from networkx.algorithms.community.modularity_max import greedy_modularity_communities\n",
        "\n",
        "# supply openai api key via file \n",
        "openai.api_key = open('openai_key').read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpJhiDNngony"
      },
      "source": [
        "## seed config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "6fBrXn4KLZXI"
      },
      "outputs": [],
      "source": [
        "# connection strenth minimum boost, base on standard deviation\n",
        "std_factor = .5\n",
        "pop_threshold = .3\n",
        "\n",
        "# batch size\n",
        "batch_size = 30\n",
        "\n",
        "# request object\n",
        "#req_obj = ['diseases', 'symptoms', 'behaviors', 'objects', 'emotions', 'home remedies', 'products', 'complications']\n",
        "\n",
        "req_obj = ['concepts', 'styles', 'cities', 'emotions']\n",
        "\n",
        "# seed topics\n",
        "\n",
        "# topics = ['dance', 'travel', 'music', 'sports', 'cooking', 'nightlife', 'boat', 'car', 'coffee', 'steak', 'turkey']\n",
        "\n",
        "topics = ['Heart', 'Stomach', 'Feet', 'Knees', 'Legs', 'Elbow', 'Head', 'Neck', 'Back', 'Nose', 'Mouth', 'Shoulder', 'Hip', 'Thigh', 'Liver', 'Colon', 'Intestines']\n",
        "\n",
        "#topics = ['Asthma', 'Bronchitis', 'Sinusitis', 'Emergency', 'Walgreens', 'Cvs', 'Eye', 'Knee', 'Skull', 'Ear', 'Nose', 'Rash', 'Arthritis'\n",
        "#        , 'Tinnitus', 'Burn', 'Allergy', 'Ambulance', 'Accident', 'Poison', 'Cough', 'Watery eye', 'Ringworm', 'Antibiotic', 'Doctor', 'Clinic'\n",
        "#        , 'Hypertension', 'Pharmacy', 'Primary care', 'Bruise', 'Sprain', 'Strain', 'Exczema', 'Cellulitis', 'Abscesses', 'Strep throat'\n",
        "#        , 'Pain', 'Throbbing', 'Irritability', 'Sad', 'Happy', 'Mad' , 'Cancer', 'Surgery', 'Heart', 'Lungs', 'Throat', 'Mouth', 'Toothache'\n",
        "#        , 'Ear infection', 'Diabetes', 'Migranes', 'Pink eye', 'Bladder infection', 'Attention deficit', 'Depression', 'Anxiety']\n",
        "\n",
        "#topics = ['headache', 'knee pain', 'lump', 'blurry vision']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## helper functions\n",
        "  1. get_trend_frame(topics, years): returns a dataframe of topic trends\n",
        "  2. stack_nodes_vertically(df_features):\n",
        "  3. build_node_labels(df_flat):"
      ],
      "metadata": {
        "id": "hpZqpTQgKIQz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "bRHT-HAW4iLc"
      },
      "outputs": [],
      "source": [
        "### 1. for a given list of topics, return dataframe of interests from pytrend\n",
        "def get_trend_frame(topics, years):\n",
        "\n",
        "  # batch management\n",
        "  batch_counter = 0\n",
        "  batch_idx = 0\n",
        "\n",
        "  # initialize return frame\n",
        "  df_total = pandas.DataFrame()\n",
        "\n",
        "  # for each provided \n",
        "  for topic in topics:\n",
        "\n",
        "    # prevent saturation of api endpoint\n",
        "    if batch_counter > batch_size:\n",
        "      print('')\n",
        "      batch_idx+=1\n",
        "      print('completed retrieval of batch %d - %d rows of %d' % (batch_idx, round(batch_idx * batch_size), len(topics) ))\n",
        "      batch_counter=0\n",
        "      time.sleep(30)\n",
        "\n",
        "\n",
        "    # pytrend request\n",
        "    pytrend = TrendReq()\n",
        "    pytrend.build_payload(kw_list=[topic]\n",
        "                         ,geo=\"US-TX\"\n",
        "                         ,timeframe=( 'today %s-y' % (years)))\n",
        "    \n",
        "    # load trend request\n",
        "    df = pytrend.interest_over_time()\n",
        "\n",
        "    batch_counter+=1\n",
        "\n",
        "    try:\n",
        "\n",
        "      # add column for new topic\n",
        "      df_total[topic] = df[topic]\n",
        "\n",
        "    except:\n",
        "      print('failed to add topic - %s ' % (topic))\n",
        "\n",
        "  # drop nans\n",
        "  df_total = df_total.dropna()\n",
        "\n",
        "  # return standard frame\n",
        "  return df_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "Nuy8_7_9jk6K"
      },
      "outputs": [],
      "source": [
        "### 2. for each topic and interval of time, stack a vertical dataframe of value counts\n",
        "def stack_nodes_vertically(df_features):\n",
        "\n",
        "  # initialize buffers resources\n",
        "  df_stk=pandas.DataFrame()\n",
        "  globalst=[]\n",
        "\n",
        "  # for each feature\n",
        "  for col in df_features:                      # topic\n",
        "\n",
        "      # for each time interval\n",
        "      for val in df_features.index.weekofyear: # weeknum\n",
        "\n",
        "          try:\n",
        "\n",
        "            # time sensitive\n",
        "            fs_val = df_features[col][val]       # full spectrum\n",
        "                \n",
        "            if fs_val > pop_threshold:\n",
        "\n",
        "              # iterate through each instance of max bin occurence, determining number of weeks written\n",
        "              for x in range(0,int(abs(fs_val) * 100)):\n",
        "                \n",
        "                  # build buffer list\n",
        "                  lst = [val, val, col]\n",
        "                  globalst.append(lst)\n",
        "\n",
        "            else:\n",
        "                  # build buffer list\n",
        "                  lst = [val, None, None]\n",
        "                  globalst.append(lst)\n",
        "\n",
        "          except Exception as e: # work on python 3.x\n",
        "              print(col)              \n",
        "              print(val)\n",
        "              print(fs_val)\n",
        "              print('Failed to upload to ftp: '+ str(e))\n",
        "\n",
        "  # materialize vertically flattened\n",
        "  df_flat = pandas.DataFrame(globalst\n",
        "                            ,columns=['WeekNum'\n",
        "                            ,'WeekNum_Rct'\n",
        "                            ,'Flower_Tot'])\n",
        "  \n",
        "  return df_flat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "h_lbF2IB4icN"
      },
      "outputs": [],
      "source": [
        "### 3. prefix ridgemap/joyplot labels with weeknum average\n",
        "def build_node_labels(df_flat):\n",
        "  \n",
        "  # get all distinct nodes\n",
        "  df_stats = df_flat.groupby('Flower_Tot').agg(['mean', 'count']).reset_index()\n",
        "\n",
        "  # rename columns to node labels\n",
        "  df_stats.columns = [' '.join(col).strip() for col in df_stats.columns.values]\n",
        "\n",
        "  # sort by average time interval\n",
        "  df_stats = df_stats.sort_values(by='WeekNum mean',ascending=True)\n",
        "\n",
        "  # include average into node label\n",
        "  df_stats['Name'] = ( ( df_stats['WeekNum mean'] * 100) + 1000).astype('int').astype('str') + ' '+ df_stats['Flower_Tot']\n",
        "\n",
        "  # update flattened node space with averaged names\n",
        "  for index, row in df_stats.iterrows():\n",
        "      df_flat['Flower_Tot'] = df_flat['Flower_Tot'].replace(row['Flower_Tot'], row['Name'])\n",
        "\n",
        "  return df_flat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 4. display ridgeplot of time frame, for \n",
        "def get_ridgeplot(df_flat):\n",
        "\n",
        "  # configure and build ridgeplot\n",
        "  fig, axes = joypy.joyplot(df_flat\n",
        "                            , column=['WeekNum', 'WeekNum_Rct']\n",
        "                            , by='Flower_Tot'\n",
        "                            , overlap=1.2\n",
        "                            , fill=True\n",
        "                            , figsize=(22,22)\n",
        "                            , x_range = [0,52]\n",
        "                            , colormap=cm.cool                         \n",
        "                            , ylim='own'\n",
        "                          )\n",
        "\n",
        "  # write ridgeplot\n",
        "  plt.savefig('frank_jj.png')\n",
        "\n",
        "  # plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "SnZfekidR963"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 5. send openai question, receive response\n",
        "def get_openai_response(question):\n",
        "\n",
        "    # format open au request\n",
        "    response = openai.Completion.create(\n",
        "                      engine=\"text-davinci-001\",\n",
        "                      prompt=question,\n",
        "                      temperature=.01,\n",
        "                      max_tokens=250,\n",
        "                      top_p=1,\n",
        "                      frequency_penalty=50,\n",
        "                      presence_penalty=0 )\n",
        "\n",
        "    # parse and process open ai response\n",
        "    response_choices = response[\"choices\"]\n",
        "\n",
        "    # replace blanks\n",
        "    response = response_choices[0][\"text\"].strip('\\n')\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "l1Q4o58dQg9l"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fVOt05VmdEjA"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FuUG92OfdE0k"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def swap_tokens(chk_topics, new_topics):\n",
        "\n",
        "  buffer_lst = []\n",
        "\n",
        "# for each graphed topics\n",
        "  for chk_topic in chk_topics:\n",
        "\n",
        "    buffer_lst.append(chk_topic)\n",
        "\n",
        "\n",
        "  for new_topic in new_topics:\n",
        "\n",
        "    # token validation checks\n",
        "\n",
        "      new_topic_split = new_topic.split()\n",
        "    \n",
        "    # 1. check that not preceeded by short word\n",
        "      if len(new_topic_split[0]) < 3:\n",
        "\n",
        "        try:\n",
        "          new_topic = new_topic_split[1]\n",
        "        except:\n",
        "          pass\n",
        "        \n",
        "\n",
        "      buffer_lst.append(new_topic)\n",
        "\n",
        "\n",
        "  return buffer_lst\n"
      ],
      "metadata": {
        "id": "Iko3rJCLdFG-"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swap_tokens(['test','hat','visits'],['s visit','run', 'jump'])"
      ],
      "metadata": {
        "id": "igEaX81ifwCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e1b529-9ed2-442a-ab43-4e7175a616ee"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'hat', 'visits', 'visit', 'run', 'jump']"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'s visit'.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKrlgpY_ayiK",
        "outputId": "117d9870-1335-495e-9b4d-9ecfa2b5da96"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['s', 'visit']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairwise_similarity(df_features, topics, std_factor):\n",
        "\n",
        "  # pair wise correlation / similarity\n",
        "  c = df_features.corr().abs()\n",
        "\n",
        "  # flatten and sort\n",
        "  so = c.unstack().sort_values(kind=\"quicksort\")\n",
        "\n",
        "  # remove low connections\n",
        "  min_connection_strength = so.mean() + ( std_factor * so.std() )\n",
        "  min_connection_cutoff = len(so) - len(so[so > min_connection_strength])\n",
        "\n",
        "  # turn to frame\n",
        "  df_slice = so[min_connection_cutoff:-len(topics)].to_frame()\n",
        "\n",
        "  df_slice.reset_index(inplace=True)\n",
        "\n",
        "  df_slice.columns = ['source','target','weight']\n",
        "\n",
        "  return df_slice"
      ],
      "metadata": {
        "id": "cLyLhzvffwTI"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_graph_network(df_slice):\n",
        "\n",
        "  # build graph \n",
        "  G = nx.from_pandas_edgelist(df_slice\n",
        "                            ,'source'          # parent\n",
        "                            ,'target'          # child\n",
        "                            , ['weight'])      # relationship\n",
        "\n",
        "\n",
        "  # define plot area\n",
        "  plt.figure(3,figsize=(22,22)) \n",
        "\n",
        "\n",
        "  # visualize graph \n",
        "  nx.draw_spring(G                        \n",
        "      ,with_labels=True\n",
        "      ,edge_color='gray'\n",
        "      ,node_color='white'\n",
        "      ,font_size=22)\n",
        "  \n",
        "  return G"
      ],
      "metadata": {
        "id": "AICJ2mO_h5dd"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_openai_generated_topics(topic, obj):\n",
        "  \n",
        "  new_tokens_list = []\n",
        "\n",
        "  responses = []\n",
        "\n",
        "  # retrieve \n",
        "  retrieval = get_openai_response('from the perspetive of a Texas resident, list one word %s related to %s' % (obj , topic))\n",
        "  responses.append(retrieval)\n",
        "\n",
        "  new_tokens = re.split(\"; |, |\\*|\\n\",responses[0].lower())\n",
        "  #new_tokens = re.split(\"; |, |\\*|\\n\",retrieval.lower())\n",
        "  #new_tokens_list.append(' '.join(new_tokens))\n",
        "\n",
        "\n",
        "  return new_tokens"
      ],
      "metadata": {
        "id": "Ski7o9Ezh5so"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_related_topics(df):\n",
        "\n",
        "  lst_buffer = []\n",
        "  graph_topics = list(df['source'].unique())\n",
        "\n",
        "  # batch management\n",
        "  batch_counter = 0\n",
        "  batch_idx = 0\n",
        "\n",
        "  for graph_topic in graph_topics:\n",
        "\n",
        "    for obj in req_obj:\n",
        "\n",
        "      # prevent saturation of api endpoint\n",
        "      if batch_counter > batch_size:\n",
        "        print('')\n",
        "        batch_idx+=1\n",
        "        print('completed openai retrieval of batch %d - %d rows of %d' % (batch_idx, round(batch_idx * batch_size), len(graph_topics) * len(req_obj) ))\n",
        "        batch_counter=0\n",
        "        time.sleep(30)\n",
        "\n",
        "      topic_list = get_openai_generated_topics(graph_topic, obj)\n",
        "\n",
        "      for topic in topic_list:\n",
        "\n",
        "        topic = topic.translate(string.punctuation)\n",
        "        topic = re.sub(r'[^\\w\\s]','',topic)\n",
        "\n",
        "        if len(topic) < 10 and len(topic) > 4:\n",
        "\n",
        "          lst_buffer.append(topic)\n",
        "      \n",
        "      batch_counter+=1\n",
        "    \n",
        "  return lst_buffer"
      ],
      "metadata": {
        "id": "ce29Glkzh6C0"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gather, pre-process and stage graph data structure"
      ],
      "metadata": {
        "id": "esEyoi87K8fd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utiv9ScViQTb"
      },
      "outputs": [],
      "source": [
        "# request features\n",
        "df_features = get_trend_frame(topics, '5')[topics]\n",
        "\n",
        "# normalize feature space\n",
        "df_features=(df_features-df_features.mean())/df_features.std()\n",
        "\n",
        "# flatten feature space vertically\n",
        "df_flat = stack_nodes_vertically(df_features)\n",
        "\n",
        "# build node labels\n",
        "df_flat = build_node_labels(df_flat)\n",
        "\n",
        "# display ridgeplot of events\n",
        "get_ridgeplot(df_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwI-_CfpTFjk"
      },
      "outputs": [],
      "source": [
        "df_features\n",
        "#df_slice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5gnECCqZZdUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PL-uoYE3Zdpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9K_zNAkMrXv"
      },
      "outputs": [],
      "source": [
        "# time-based pairwise similiarity weights\n",
        "df_slice = get_pairwise_similarity(df_features, topics, std_factor)\n",
        "\n",
        "# reduce to realistic relationship weights\n",
        "#df_slice = df_slice[(df_slice['weight']<0.85)]\n",
        "#df_slice = df_slice[df_slice['source'] != df_slice['target'] ]\n",
        "\n",
        "# display flattened histogram\n",
        "df_slice.hist()\n",
        "\n",
        "# build graph\n",
        "G = build_graph_network(df_slice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLhuMw06acSo"
      },
      "outputs": [],
      "source": [
        "# circler plot\n",
        "circ_pos = nx.circular_layout(G) \n",
        "\n",
        "# define plot area\n",
        "plt.figure(3,figsize=(22,22)) \n",
        "\n",
        "#Use the networkx draw function to easily visualise the graph\n",
        "nx.draw(G,circ_pos,with_labels=True,font_size=22)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwVqHIVRaduM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC9iB8QXCyWY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZIQvoHMOkl4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAKCkgXrCy2L"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWHPfa5XF8cc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lulhg1UoH_J8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdxuGicoF8w3"
      },
      "outputs": [],
      "source": [
        "# community detection\n",
        "\n",
        "# community detection\n",
        "c = list(greedy_modularity_communities(G))\n",
        "\n",
        "# how many communities we detected\n",
        "community_count=len(c)\n",
        "print('Communities found - %s ' % community_count); print()\n",
        "\n",
        "for community in range(community_count):\n",
        "  \n",
        "  print('Community %d - %s' % (community, sorted(c[community])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U6gHwivN2IPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK75ZXw7Cy_8"
      },
      "outputs": [],
      "source": [
        "### graph properties\n",
        "\n",
        "# list nodes\n",
        "G.nodes.items()\n",
        "\n",
        "# number of edges in graph\n",
        "G.number_of_edges()\n",
        "\n",
        "# number of nodes in graph\n",
        "G.number_of_nodes()\n",
        "\n",
        "G.get_edge_data(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRilHrwycAEt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpIi6Xt2cA0E"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl2gqeBfcBDZ"
      },
      "outputs": [],
      "source": [
        "# existing community topics\n",
        "chk_topics = list(df_slice['source'].unique())\n",
        "\n",
        "# purposed community topics\n",
        "new_topics = get_related_topics(df_slice)\n",
        "print(new_topics)\n",
        "\n",
        "# new topic list\n",
        "gen_topics = list(set(swap_tokens(chk_topics, new_topics)))\n",
        "print(gen_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sq55saRcSkd"
      },
      "outputs": [],
      "source": [
        "#print(df_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0Rn5Mkicdur"
      },
      "outputs": [],
      "source": [
        "# request features\n",
        "df_features = get_trend_frame(gen_topics, '5')[gen_topics]\n",
        "\n",
        "# normalize feature space\n",
        "df_features=(df_features-df_features.mean())/df_features.std()\n",
        "\n",
        "# flatten feature space vertically\n",
        "df_flat = stack_nodes_vertically(df_features)\n",
        "\n",
        "# build node labels\n",
        "df_flat = build_node_labels(df_flat)\n",
        "\n",
        "# display ridgeplot of events\n",
        "get_ridgeplot(df_flat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# time-based pairwise similiarity weights\n",
        "df_slice = get_pairwise_similarity(df_features, gen_topics, std_factor)\n",
        "\n",
        "# reduce to relevant relationships\n",
        "df_slice = df_slice[(df_slice['weight']<0.85)]\n",
        "df_slice = df_slice[df_slice['source'] != df_slice['target'] ]\n",
        "\n",
        "# display flattened histogram\n",
        "df_slice.hist()\n",
        "\n",
        "# build graph\n",
        "G = build_graph_network(df_slice)"
      ],
      "metadata": {
        "id": "Z3uXA2MYZPC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# community detection\n",
        "\n",
        "# community detection\n",
        "c = list(greedy_modularity_communities(G))\n",
        "\n",
        "# how many communities we detected\n",
        "community_count=len(c)\n",
        "print('Communities found - %s ' % community_count); print()\n",
        "\n",
        "for community in range(community_count):\n",
        "  \n",
        "  print('Community %d - %s' % (community, sorted(c[community])))"
      ],
      "metadata": {
        "id": "OQQWXlMhZPWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hQiUbkliZPmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-tTNwurEa4iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# existing community topics\n",
        "chk_topics = list(df_slice['source'].unique())\n",
        "\n",
        "# purposed community topics\n",
        "new_topics = get_related_topics(df_slice)\n",
        "print(new_topics)\n",
        "\n",
        "# new topic list\n",
        "gen_topics = list(set(swap_tokens(chk_topics, new_topics)))\n",
        "print(gen_topics)"
      ],
      "metadata": {
        "id": "DlsBkDV2a4wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# request features\n",
        "df_features = get_trend_frame(gen_topics, '5')\n",
        "\n",
        "# normalize feature space\n",
        "df_features=(df_features-df_features.mean())/df_features.std()\n",
        "\n",
        "# flatten feature space vertically\n",
        "df_flat = stack_nodes_vertically(df_features)\n",
        "\n",
        "# build node labels\n",
        "df_flat = build_node_labels(df_flat)\n",
        "\n",
        "# display ridgeplot of events\n",
        "get_ridgeplot(df_flat)"
      ],
      "metadata": {
        "id": "6l_-fsyO6Hjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# time-based pairwise similiarity weights\n",
        "df_slice = get_pairwise_similarity(df_features, gen_topics, std_factor*2.6)\n",
        "\n",
        "# reduce to relevant relationships\n",
        "df_slice = df_slice[(df_slice['weight']<0.85)]\n",
        "df_slice = df_slice[df_slice['source'] != df_slice['target'] ]\n",
        "\n",
        "# display flattened histogram\n",
        "df_slice.hist()\n",
        "\n",
        "# build graph\n",
        "G = build_graph_network(df_slice)"
      ],
      "metadata": {
        "id": "HTguNjWF5-dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# community detection\n",
        "\n",
        "# community detection\n",
        "c = list(greedy_modularity_communities(G))\n",
        "\n",
        "# how many communities we detected\n",
        "community_count=len(c)\n",
        "print('Communities found - %s ' % community_count); print()\n",
        "\n",
        "for community in range(community_count):\n",
        "  \n",
        "  print('Community %d - %s' % (community, sorted(c[community])))"
      ],
      "metadata": {
        "id": "l6WBixEF5-95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bw0B-Rue5_Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xdQpkumC5_h1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "j9dJdg4LTLDY",
        "zWG_yZP1TJGo"
      ],
      "name": "emergence.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNtyV888DdVQ5wJdZVTPdrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}