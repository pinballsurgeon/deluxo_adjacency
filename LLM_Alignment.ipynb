{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LLM Alignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlR0F1BVfnAkK2y6F40Olc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinballsurgeon/deluxo_adjacency/blob/main/LLM_Alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Enable CUDA GPUs in runtime settings "
      ],
      "metadata": {
        "id": "uYYUC1TjeXqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install"
      ],
      "metadata": {
        "id": "g5u-EKIhWuSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install transformers\n",
        "!pip install transformers -q\n",
        "\n",
        "# install openai\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU85Rtm5Wp4U",
        "outputId": "807cdf61-24f8-410d-b00e-886b980e21fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.7/dist-packages (0.20.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.7/dist-packages (from openai) (1.2.0.62)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import"
      ],
      "metadata": {
        "id": "FHs_YYbzWqf-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "RZLIOaXhWYA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea18297-10ef-42b6-aa5f-6d7d61ede366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "# standard dan-dard\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import numpy\n",
        "import pandas\n",
        "import string\n",
        "import seaborn\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow_hub\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins import projector\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        " \n",
        "# universal encoder\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "encoder_model = tensorflow_hub.load(module_url)\n",
        "\n",
        "# set up a logs directory, so Tensorboard knows where to look for files.\n",
        "log_dir='/logs/embedding_projector/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# LM transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
        "\n",
        "# gpt3 \n",
        "import openai\n",
        "\n",
        "# supply openai api key via file \n",
        "openai.api_key = open('openai_key').read()\n",
        "\n",
        "# load tensorboar dextension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download"
      ],
      "metadata": {
        "id": "933E4XA_Xw5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b3\", use_cache=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b3\")"
      ],
      "metadata": {
        "id": "dIBl-gISXxjG"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template"
      ],
      "metadata": {
        "id": "3nC2EEkXeS2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "request_sentence = 'What is %s'\n",
        "regex = re.compile('[^a-zA-Z ]')"
      ],
      "metadata": {
        "id": "jT4GPNtJeT7U"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions"
      ],
      "metadata": {
        "id": "ZYRlEHOAZ_1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### OPENAI - Ask Davinci - build openai completion request, define parameters\n",
        "def get_openai_response(question):\n",
        "\n",
        "    # format open au request\n",
        "    response = openai.Completion.create(\n",
        "                      engine=\"text-davinci-001\",\n",
        "                      prompt=question,\n",
        "                      temperature=.5,\n",
        "                      max_tokens=250,\n",
        "                      top_p=1,\n",
        "                      frequency_penalty=50,\n",
        "                      presence_penalty=0 )\n",
        "\n",
        "    # parse and process open ai response\n",
        "    response_choices = response[\"choices\"]\n",
        "\n",
        "    # replace blanks\n",
        "    response = response_choices[0][\"text\"].strip()\n",
        "    \n",
        "    # replace non-alphabet chars\n",
        "    response = regex.sub('', response)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "nx9SUBMSZ9FW"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### OPENAI - Form Davinci request\n",
        "def prompt_openai(topic, request_template):\n",
        "  \n",
        "  # build sentence and retrieve \n",
        "  retrieval = get_openai_response(request_template % topic)\n",
        "  \n",
        "  # return response\n",
        "  return retrieval"
      ],
      "metadata": {
        "id": "ILlyBtILazVO"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### BLOOM - Generate Bloom response model\n",
        "def get_bloom_response(question):\n",
        "\n",
        "  response = model.generate(**question\n",
        "                          , num_beams = 2\n",
        "                          , num_beam_groups = 2\n",
        "                          , top_k=1\n",
        "                          , temperature=0.9\n",
        "                          , repetition_penalty = 2.0\n",
        "                          , diversity_penalty=2.0\n",
        "                          , max_new_tokens = 20)\n",
        "  \n",
        "  return response\n"
      ],
      "metadata": {
        "id": "5oKkxl60kiTz"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### BLOOM - Form request\n",
        "def prompt_bloom(topic, request_template):\n",
        "\n",
        "  # initiate tokenizer\n",
        "  input_tokens = tokenizer(request_template % topic, return_tensors=\"pt\").to(0)\n",
        "\n",
        "  # build sentence and retrieve \n",
        "  retrieval = get_bloom_response(input_tokens)\n",
        "\n",
        "  # decode retrieval\n",
        "  retrieval = tokenizer.decode(retrieval[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\",\"\\n\\n\\n\"])\n",
        "\n",
        "  # remove prompt from response\n",
        "  retrieval = retrieval.replace(request_template % topic, '')\n",
        "\n",
        "  # replace blanks\n",
        "  retrieval = retrieval.strip()\n",
        "  \n",
        "  # replace non-alphabet chars\n",
        "  retrieval = regex.sub('', retrieval)\n",
        "\n",
        "  return retrieval\n"
      ],
      "metadata": {
        "id": "I3JeOGWcj5IF"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_bloom(topic, request_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "boh3ejnmmAg8",
        "outputId": "4098133a-edd9-4c33-df16-492710192f61"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-a62fe6043163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_bloom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-fa146c2cddf2>\u001b[0m in \u001b[0;36mprompt_bloom\u001b[0;34m(topic, request_template)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# build sentence and retrieve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mretrieval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bloom_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# decode retrieval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-d947b546cc56>\u001b[0m in \u001b[0;36mget_bloom_response\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_bloom_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   response = model.generate(**question\n\u001b[0m\u001b[1;32m      5\u001b[0m                           \u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                           \u001b[0;34m,\u001b[0m \u001b[0mnum_beam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'generate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pandas.DataFrame(columns=['Topic','Openai_response'])\n",
        "topic = 'Science'\n",
        "\n",
        "openai_response = prompt_openai(topic, request_sentence)\n",
        "\n",
        "\n",
        "df.loc[len(df.index)] = [topic, openai_response]"
      ],
      "metadata": {
        "id": "RBwA0bNxazof"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Openai_response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JILF-Zchh5X3",
        "outputId": "05744536-7bc1-4a41-a5a2-5637eaf46927"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    Science is a systematic way of acquiring knowl...\n",
            "Name: Openai_response, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f'what is today'"
      ],
      "metadata": {
        "id": "IZx76rBVaz2_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(0)"
      ],
      "metadata": {
        "id": "VjLCAwq_guFE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = model.generate(**input_ids, num_beams = 2, num_beam_groups = 2, top_k=1, temperature=0.9, repetition_penalty = 2.0, diversity_penalty=2.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2DzCDC5guW5",
        "outputId": "5ca5646d-7faf-4f5f-ea59-0fff092346e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1207: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
            "  \"Passing `max_length` to BeamSearchScorer is deprecated and has no effect. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\",\"\\n\\n\\n\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcvjmrqugunp",
        "outputId": "7e7af3c4-7a3c-4e24-f213-7993116fd3e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is today known as the “New York Times”, and it was founded in 1851 by William\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst = ['Science', 'Math', 'Sailboat']"
      ],
      "metadata": {
        "id": "hwTM7vBu2QAS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_bloom())"
      ],
      "metadata": {
        "id": "PJ5IaYD22QRS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embed sentences\n",
        "message_embeddings_ = model(df['text'])\n",
        "\n",
        "# array product (build our 2d universe)\n",
        "corr = numpy.inner(message_embeddings_, message_embeddings_)\n",
        "\n",
        "# seaborn config\n",
        "seaborn.set(font_scale=1.2, rc={'figure.figsize':(20,15)})\n",
        "\n",
        "# seaborn plot\n",
        "g = seaborn.heatmap(\n",
        "    corr,\n",
        "    xticklabels=df['titles'].str.slice(0,25), # vect paper titles, sliced for nice\n",
        "    yticklabels=df['titles'].str.slice(0,25), # vect paper titles\n",
        "    vmin=0,\n",
        "    vmax=1,\n",
        "    cmap=\"YlOrRd\")\n",
        "\n",
        "# rotate and label\n",
        "g.set_xticklabels(df['titles'].str.slice(0,25), rotation=90)\n",
        "g.set_title(\"Similarity\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "dvRTUm5E2Qew",
        "outputId": "5da3e48c-cb99-4332-bd3e-768f2e82cdee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-41c1867a4265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# embed sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmessage_embeddings_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# array product (build our 2d universe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_embeddings_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_embeddings_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7Ji6aKYE2Qqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}